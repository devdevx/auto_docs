{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MkDocs","text":""},{"location":"#documentation","title":"Documentation","text":"<p>Official doc. Publish</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>    mkdocs.yml    # The configuration file.\n    docs/\n        index.md  # The documentation homepage.\n        ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"#docker-commands","title":"Docker commands","text":"<p>Init project</p> <pre><code>docker run --rm -it -v \"%cd%\":/docs squidfunk/mkdocs-material new .\n</code></pre> <p>Live reload</p> <pre><code>docker run --rm -it -p 8000:8000 -v \"%cd%\":/docs squidfunk/mkdocs-material\n</code></pre>"},{"location":"#publish-in-github","title":"Publish in github","text":"<p>Github only allows pages in public repos for free tier When deploying from branch use <code>gh-pages</code> branch</p>"},{"location":"IDEs/vsc/","title":"Visual Studio Code","text":"<p>Search for a file</p> <pre><code>Ctrl + p\n</code></pre>"},{"location":"IDEs/vsc/#plugins","title":"Plugins","text":"<ul> <li>Version Lens</li> </ul>"},{"location":"ML/ML/","title":"Machine Learning","text":""},{"location":"ML/ML/#links","title":"Links","text":"<p>Tutorial: 175 problems solved Full course</p>"},{"location":"ML/ML/#explore","title":"Explore","text":"<p>https://github.com/maziarraissi/Applied-Deep-Learning</p> <p>https://www.kaggle.com/c/titanic</p> <p>https://github.com/microsoft/ML-For-Beginners</p> <p>https://www.freecodecamp.org/news/free-machine-learning-course-10-hourse/</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#todo","title":"TODO","text":"<p>Sample architecture in multiple clouds</p>"},{"location":"architecture/multitenant/","title":"Multitenant","text":"<p>TODO</p> <p>https://oncodedesign.com/data-isolation-and-sharing-in-multitenant-system-part1/</p> <p>https://oncodedesign.com/data-isolation-and-sharing-in-multitenant-system-part2/</p> <p>https://oncodedesign.com/data-isolation-and-sharing-in-multitenant-system-part3/</p>"},{"location":"architecture/patterns/","title":"Patterns","text":""},{"location":"architecture/patterns/#links","title":"Links","text":"<ul> <li>Outbox Message example Spring Boot</li> <li>Saga example</li> <li>Cloud patterns</li> </ul>"},{"location":"cloud/aws/aws/","title":"AWS","text":""},{"location":"cloud/aws/aws/#sam","title":"SAM","text":"<p>Commands</p> <pre><code>sam init\nsam local start-api\nsam local generate event &lt;service&gt; &lt;type&gt; &gt; events/event.json\nsam local invoke \"&lt;function&gt;\" -e events/event.json\nsam build\nsam deploy\nsam deploy --guided\nsam delete\n</code></pre>"},{"location":"cloud/aws/cert-ia-prac/","title":"AWS Certified IA Practitioner","text":""},{"location":"cloud/aws/cert-ia-prac/#security","title":"Security","text":"<ul> <li>Use IAM to implement role-based access controls to restrict data access to authorized personnel only.</li> <li>Use cryptographic hashing techniques to verify the authenticity of the data.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#characteristics-of-a-generative-ai","title":"Characteristics of a Generative AI","text":"<ul> <li>Personalization is a critical capability for generative AI. It involves tailoring content or recommendations to individual users based on their preferences or characteristics, which enhances user experiences and engagement.</li> <li>Scalability can handle increasing workloads efficiently. It's crucial for large-scale applications.</li> <li>Data efficiency relates to how well a model can learn from limited data.</li> <li>Simplicity is essential for user adoption and system maintenance, it only indirectly impacts personalized recommendations. The real complexity lies in understanding user preferences, not in the simplicity of the AI model itself.</li> <li>Adaptability ensures versatility across different scenarios.</li> <li>Responsiveness matters for chatbots and virtual assistants, real-time responsiveness.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#challenges-of-generative-ai","title":"Challenges of Generative AI","text":"<ul> <li>Toxicity refers to the risk of generating content that is offensive, disturbing, or inappropriate. This challenge is compounded by the subjective nature of defining what constitutes toxic content, which can vary based on context and culture. Furthermore, detecting and mitigating subtle or indirect offensive language adds to the complexity of managing toxicity in generative AI outputs.</li> <li>Hallucinations are assertions or claims made by generative AI models that sound plausible but are factually incorrect. Due to the nature of LLMs, which generate text based on patterns rather than factual verification, they may produce convincing yet erroneous information. For instance, LLMs can generate fictitious scientific citations or non-existent references that appear legitimate but are inaccurate.</li> <li>Intellectual Property concerns arise when generative AI models produce content that mimics or reproduces elements of their training data. This can lead to legal issues if the generated content closely resembles copyrighted material or reproduces it verbatim. For example, an AI-generated image in the style of a famous artist could raise objections if it closely mimics the artist's original work.</li> <li>Disruption of the Nature of Work refers to how generative AI can change job roles or automate tasks rather than issues of ownership and attribution.</li> <li>Plagiarism and Cheating</li> <li>Knowledge cutoff: Generative AI models are trained on datasets available up to a certain point in time, and they do not have access to real-time data or updates. This limitation can result in the models providing outdated information, which is especially problematic in dynamic environments where up-to-date knowledge is crucial. The knowledge cutoff can limit the effectiveness of the AI in providing accurate and relevant responses to customers.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#generative-ai-models","title":"Generative AI models","text":"<ul> <li>Large language models (LLMs) are extensive deep learning models pre-trained on massive datasets. They utilize a transformer architecture, which includes neural networks composed of an encoder and a decoder with self-attention mechanisms. These components work together to derive meaning from text sequences and comprehend the relationships between words and phrases.</li> <li>Stable Diffusion is a generative AI model that creates distinctive photorealistic images based on text and image prompts.</li> <li>Multimodal models are AI systems that can process and generate content across multiple modalities, such as text, images, and audio. These models are designed to understand and integrate information from diverse data types, enabling more comprehensive and contextually rich outputs.</li> <li>Researchers introduced the term \u201cfoundation model\u201d to describe machine learning models that are trained on a diverse range of generalized and unlabeled data. These models are capable of performing a wide array of tasks, including language comprehension, text and image generation, and natural language conversation. Can be fine-tuned for a wide range of tasks.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#prompt-engineering-techniques","title":"Prompt engineering techniques","text":"<ul> <li>Zero-shot prompting is a technique where a user presents a task to a generative model without providing any examples or explicit training for that specific task. Instead, it relies solely on the general knowledge acquired during pre-training to perform the task based on the prompt.</li> <li>Few-shot prompting is a technique that involves providing a language model with contextual examples to guide its understanding and expected output for a specific task. This is technique is particularly helpful for models with limited training data or when adapting the model to new domains or tasks.</li> <li>Chain-of-thought (CoT) prompting is a technique that divides intricate reasoning tasks into smaller intermediary steps. This technique can help the model reason through the task in a structured manner and improve its ability to handle complex tasks.</li> <li>Self-refine prompting is a technique where a model iteratively solves a problem, critiques its own solution, and then revises the solution based on the critique. This cycle continues until a predetermined stopping condition, such as running out of tokens or time, is met.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#foundational-models-fine-tuning","title":"Foundational models fine-tuning","text":"<ul> <li>Instruction-based fine-tuning is a process where a pre-trained foundation model is further trained with specific instructions to perform particular tasks. This approach helps the model understand and follow detailed guidance, generating more accurate and relevant responses. By fine-tuning the model with task-specific instructions, the model becomes capable of handling complex tasks, such as multi-turn conversations or generating personalized recommendations based on user input. This method uses the model's existing knowledge while refining its ability to execute specific tasks, making it ideal for scenarios that require detailed, task-oriented responses.</li> <li>Domain adaptation fine-tuning uses a pre-trained model to improve its performance only in a specific domain. This method is more about making the model knowledgeable in a specific domain.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#core-dimension-of-responsible-ai","title":"Core dimension of responsible AI","text":"<ul> <li>Transparency: Refers to the ability to understand and inspect the inner workings, decision-making processes, and outputs of an AI system. It involves making the AI system's behavior, decisions, and underlying logic visible and comprehensible to relevant stakeholders, such as developers, regulators, and end-users.</li> <li>Explainability: Providing clear explanations and interpretability of how AI systems make decisions, enabling accountability and trust.</li> <li>Veracity and Robustness: Ensuring AI systems operate reliably and consistently and are resilient to potential failures or adversarial attacks.</li> <li>Fairness: Ensuring AI models are unbiased and do not discriminate against individuals or groups based on protected characteristics.</li> <li>Privacy and Security: Safeguarding the privacy and security of the data used by AI systems and handling personal information responsibly.</li> <li>Governance: Establishing clear governance frameworks, policies, and processes for the responsible development and deployment of AI systems.</li> <li>Safety: Identifying and mitigating potential risks and unintended consequences associated with AI systems.</li> <li>Controllability: Maintaining appropriate human control and oversight over AI systems, particularly in high-stakes decision-making scenarios.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#generative-ai-models-security-vulnerabilities","title":"Generative AI models security vulnerabilities","text":"<ul> <li>Prompt Injection involves manipulating the AI model's input prompts to cause it to produce unintended or harmful outputs. It is crucial to address this vulnerability to prevent malicious users from exploiting the model's behavior.</li> <li>Training data poisoning involves attackers manipulating the training data to corrupt the AI model's learning process. This vulnerability can significantly impact the model's accuracy and reliability, making it a critical concern.</li> <li>Model theft involves unauthorized access or copying of the model's parameters or architecture. This can lead to misuse of the model or unauthorized replication, which poses a severe risk to the model's intellectual property and security.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#machine-learning-techniques","title":"Machine-learning techniques","text":"<ul> <li>Regression is a supervised learning technique used for predicting continuous values. It involves determining the relationship between a dependent variable and one or more independent variables. By analyzing the patterns in historical data, regression models can predict future outcomes, making it ideal for tasks like forecasting stock prices, real estate values, or portfolio performance.</li> <li>Linear regression refers to supervised learning models that use one or more inputs to predict a value on a continuous scale. It is used to predict housing prices. After training a model using a set of historical sales training data that includes those characteristics, you could forecast the price of a property based on its location, age, and number of rooms.</li> <li>Probability density is generally used to estimate the likelihood or possibility of a random variable falling within a particular range of values, not for predicting future values based on historical data.</li> <li>Dimensionality reduction is is primarily used to reduce the number of features in a dataset while retaining as much information as possible. It is not used for prediction tasks directly.</li> <li>Anomaly detection is is only used to identify outliers or unusual patterns in data.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#machine-learning-approaches","title":"Machine learning approaches","text":"<ul> <li>Supervised Learning is a machine learning approach where a model is trained on a labeled dataset. This means the dataset contains input-output pairs where the desired output is known. The key advantage of Supervised Learning is that it allows the model to learn the relationship between input features like product categories and prices and output label return status, which enables accurate forecasting of future events.</li> <li>Few-shot learning is only used when the data available for training is limited. It enables models to generalize from just a few examples.</li> <li>Transfer learning is generally used when you have a pre-trained model that can be fine-tuned on a specific task.</li> <li>Unsupervised learning is only used for analyzing and clustering data without labeled outputs. It is often applied in scenarios where the goal is to discover hidden patterns or groupings within the data, such as customer segmentation or anomaly detection.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#unsupervised-learning","title":"Unsupervised learning","text":"<ul> <li>Clustering: grouping similar data points together.</li> <li>Association rule learning: discovering associations between different features.</li> <li>Probability density estimation: estimating the probability distribution of the data.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#foundational-models-capabilities","title":"Foundational models capabilities","text":"<ul> <li>Visual comprehension: It has the capability to identify objects, scenes, and other elements within images.</li> <li>Language processing: It can answer natural language questions and even write short scripts or articles in response to prompts.</li> <li>Speech to text: It is designed for tasks like transcription and video captioning in various languages.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#metrics-for-evaluation-of-model-results","title":"Metrics for evaluation of model results","text":"<ul> <li>Recall-Oriented Understudy for Gisting Evaluation (ROUGE-N) is the most appropriate metric for evaluating text summaries because it is specifically designed for this purpose. It measures how much of the important information from the reference summary is captured in the generated summary, which is essential for determining the effectiveness of a summarization model.</li> <li>BLEU (Bilingual Evaluation Understudy) is more commonly used for evaluating machine translation. It measures precision by comparing the n-grams of the generated text with the reference.</li> <li>F1 score is a general metric for evaluating classification models, balancing precision and recall.</li> <li>Cross-Entropy Loss is a metric used during the training phase of a model to measure the difference between the predicted and actual outputs. It is a tool to optimize model performance during training.</li> <li>MAPE (Mean Absolute Percentage Error) calculates the average of the absolute differences between actual and projected values, divides it by actual values, and returns a percentage. A lower MAPE score indicates greater model performance because the predictions are more accurate and closer to the actual values.</li> <li>MAE (Mean Absolute Error) is the average difference between expected and actual values for all observations. It is a widely used statistic in numerical prediction tasks to evaluate a model's prediction error. MAE computes the average absolute distance between predicted and actual values, making it simple to interpret. MAE is calculated by combining the absolute errors and dividing by the total number of observations. MAE values range from 0 to infinity, with lower values suggesting a better fit of the model to the dataset.</li> <li>Mean squared error (MSE) is primarily used in regression tasks to measure the difference between predicted and actual numerical values. It is not designed to evaluate text similarity.</li> <li>Accuracy is a commonly used metric to evaluate the performance of classification models. ( ( TP + TN ) / TOTAL )</li> <li>Recall measures the proportion of actual positive instances (true positives) correctly identified by the model.</li> <li>InferenceLatency only measures the time taken for a model to generate predictions.</li> <li>Perplexity is a metric used to evaluate the performance of language models, typically measures how well a language model predicts the next word in a sequence.</li> <li>BERTScore is a tool that compares how similar generated text is to a reference by understanding the context of words leveraging BERT (Bidirectional Encoder Representations from Transformers) embeddings. This makes it better at judging the quality of text, especially for tasks like evaluating chatbots. It helps determine how closely a chatbot's responses match what a human might say or respond.</li> <li>Metric for Evaluation of Translation with Explicit ORdering (METEOR) evaluates text similarity based on precision and recall metrics.</li> <li>Area Under the ROC Curve (AUC): A performance metric for classification models, representing the model's ability to distinguish between classes across various thresholds. accuracy = (TP + TN) / TOTAL (for balanced classes) precision = TP / (TP + FP) (to minimize false positive, like spam,  how often the positive predictions are correct?) recall = TP / (TP + FN) (to minimize false negatives, like health problems detection, can an ML model find all instances of the positive class?) false positive rate = FP / (TN + FP) true negative rate = TN / (TN + FP) F1 balances precision y recall = (precision * recall) / (precision + recall) * 2</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#visualizations-to-evaluate-model-performance","title":"Visualizations to evaluate model performance","text":"<ul> <li>Confusion matrix: It has entries for all possible combinations of correct and incorrect predictions, and shows how often each one was made by our model.</li> <li>Box plot: Typically used to display the distribution of numerical data and highlight measures like the median and interquartile range. </li> <li>Correlation matrix: Shows the linear relationships between numerical variables, which can aid in feature selection.</li> <li>Precision-recall curve: Appropriate for binary classification tasks, particularly when there is a class imbalance.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#rlhf-reinforcement-learning-from-human-feedback","title":"RLHF (Reinforcement Learning from Human Feedback)","text":"<p>Is a specific technique used in training AI systems to appear more human, alongside other techniques such as supervised and unsupervised learning. First, the model's responses are compared to the responses of a human. Then, a human assesses the quality of different responses from the machine, scoring which responses sound more human. The score can be based on innately human qualities, such as friendliness, the right degree of contextualization, and mood. One of the key steps in the RLHF approach is creating a reward model. The reward model is trained on human feedback to learn to predict the quality or appropriateness of the language model's outputs. Another key step in the RLHF approach is fine-tuning the language model using the reward model and reinforcement learning techniques.</p>"},{"location":"cloud/aws/cert-ia-prac/#ml-algorithms","title":"ML Algorithms","text":"<ul> <li>Forecasting algorithms are crucial in predictive analytics, especially for predicting future values based on historical patterns in time series data. AWS offers strong forecasting capabilities through Amazon Forecast, a fully managed service that utilizes machine learning to provide accurate time-series predictions. Amazon Forecast automatically manages the complexities of selecting and fine-tuning the appropriate algorithms based on the input data, making it an ideal solution for businesses seeking to enhance their financial planning and resource allocation.</li> <li>Linear Learner algorithm is typically used for tasks involving finding a linear relationship between input features and target variables, typically for regression tasks. While it can be used for time series data, it may not effectively capture the temporal dependencies and patterns.</li> <li>Object2Vec algorithm is primarily used for creating embeddings for objects or entities so that similar objects are closer in the vector space. This is useful for tasks like recommendations or similarity-based queries.</li> <li>Clustering algorithm is only used for grouping similar data points into clusters.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#aws-cloud-adoption-framework-caf-key-capabilities-needed-for-effective-ai-integration","title":"AWS Cloud Adoption Framework (CAF) key capabilities needed for effective AI integration","text":"<ul> <li>Business: Ensures AI investments drive digital transformation and business outcomes, positioning AI strategically.</li> <li>People: Connects AI technology with business, focusing on talent, language, and culture for competitive advantage.</li> <li>Governance: Manages AI initiatives to maximize benefits and minimize risks, emphasizing responsible AI use.</li> <li>Platform: Builds scalable cloud infrastructure for AI services and development, highlighting AI-specific needs.</li> <li>Security: Ensures data and cloud workload security. Addressing AI-specific security challenges.</li> <li>Operations: Manages AI workloads to meet business needs, ensuring reliability and consistent value creation.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#fitting","title":"Fitting","text":"<ul> <li>Low bias and high variance suggests that the model is too sensitive to the unique properties of the training data. As a result, it fails to generalize successfully to new data\u2014a classic indicator of overfitting.</li> <li>Low bias and low variance typically means the model fits the training data well and generalizes well to the test data.</li> <li>Increased bias and less variance suggests underfitting, where the model fails to capture the complexity of the data, leading to poor performance on both training and test data.</li> <li>Increased bias and increased variability represents underfitting (due to high bias) and overfitting (due to high variance), leading to poor overall performance.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#concepts","title":"Concepts","text":"<ul> <li>Data augmentation is a technique for expanding a dataset by generating new samples from existing data through various transformations. This technique addresses challenges in acquiring diverse real-world data by artificially increasing the dataset's size and variety, with recent advances in generative AI enhancing its efficiency and quality.</li> <li>Bidirectional Encoder Representations from Transformers (BERT), a bidirectional model, examines the context of an entire sequence before making predictions. It was trained on a plain text corpus and Wikipedia, utilizing 3.3 billion tokens (words) and 340 million parameters. BERT is capable of answering questions, predicting sentences, and translating texts.</li> <li>Exploratory Data Analysis (EDA) is the process of analyzing and understanding the characteristics of the data before building an ML model. It involves tasks such as visualizing data distributions, calculating summary statistics, identifying missing values, and detecting outliers. EDA aims to gain insights into the data and identify potential issues or patterns that may impact the model's performance.</li> <li>Amazon EC2 Trn1 Instances are specifically designed for high-performance machine learning (ML) model training and utilize AWS Trainium chips. Trainium is a custom-built silicon by AWS optimized for deep learning workloads, making it cost-efficient and scalable for training large and complex models. These instances support major ML frameworks such as PyTorch, TensorFlow, and Apache MXNet, making them versatile for ML practitioners. The Trn1 instances offer high network throughput and fast interconnects, which improve the efficiency of distributed training workloads and reduce overall training time.</li> <li>Low bias: Indicates that the model is not making erroneous assumptions about the training data.</li> <li>High variance: Indicates that the model is paying attention to noise in the training data and is overfitting.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#aws-services","title":"AWS Services","text":"<ul> <li>AWS AI Service Cards: provide important information on the ethical considerations, transparency, and intended usage of AWS AI services. They aimed at helping users comprehend the ethical use of AI services, ensuring that the models are fair, transparent, and accountable. These cards provide information on possible hazards and the best ways to use AI services responsibly.</li> <li>Amazon Comprehend: is a natural language processing (NLP) service that uses machine learning to find insights and relationships in text.</li> <li>Amazon Polly: is a service that turns text into lifelike speech.</li> <li>Amazon Textract: is a machine learning service designed to automatically extract text (including handwriting) and structured data from scanned documents.</li> <li>Amazon Transcribe: is an automatic speech recognition service.</li> <li>Amazon Lex: is for building conversational interfaces using voice and text. It is used to create chatbots that can interact with users through natural language. Additionally, it focuses on dialogue management and natural language understanding.</li> <li>Amazon Q Business: is a generative-AI powered assistant designed primarily for enterprises to answer questions, provide summaries, generate content, and automate tasks using their own data sources.</li> <li>Amazon Personalize: is a service that only provides personalized recommendations and user experiences by analyzing user behavior and preferences. It is primarily used to create recommendations for applications such as e-commerce or content platforms.</li> <li>Amazon Rekognition: is primarily used for image and video analysis, including face recognition, object detection, and content moderation.</li> <li>Amazon Kendra: is an enterprise search service that enables users to search a wide range of document formats and data sources. While Kendra can be integrated into a search system to help locate relevant content, it does not perform image analysis or text extraction from images or scanned PDFs.</li> <li>Amazon SageMaker: allows you to train your own ML algorithms.</li> <li>Amazon Augmented AI (Amazon A2I): is a service that makes it easy to build human review workflows for machine learning predictions. It allows developers to incorporate human review into their machine learning applications to improve model accuracy and ensure compliance with regulatory or business requirements. With Amazon A2I, developers can create human review workflows, manage the workforce, and integrate human review into their applications.</li> <li>Amazon OpenSearch Service: makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. For vector databases, you can read about k-Nearest Neighbor (k-NN) search in OpenSearch Service.</li> <li>Amazon Aurora PostgreSQL-Compatible Edition and Amazon Relational Database Service (Amazon RDS) for PostgreSQL: support the pgvector extension to store embeddings from machine learning (ML) models in your database and to perform efficient similarity searches.</li> <li>Amazon Neptune ML: is a capability of Neptune that uses Graph Neural Networks (GNNs), an ML technique purpose-built for graphs, to make easy, fast, and more accurate predictions using graph data.</li> <li>Amazon MemoryDB: supports storing millions of vectors, with single-digit millisecond query and update response times, and tens of thousands queries per second (QPS) at greater than 99% recall.</li> <li>Amazon DocumentDB (with MongoDB compatibility): supports vector search, a new capability that enables you to store, index, and search millions of vectors with millisecond response times. With vector search for Amazon DocumentDB, you can simply set up, operate, and scale databases for your ML applications.</li> <li>AWS CloudTrail: is a service provided by AWS that enables you to audit, govern, and ensure compliance within your AWS account. Events are recorded in CloudTrail to capture actions performed by users, roles, or AWS services. These events encompass actions carried out in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs. CloudTrail is automatically active in your AWS account upon its creation. Any activity in your AWS account is then logged as a CloudTrail event.</li> <li>Amazon Macie: uses machine learning to automatically discover sensitive data within your S3 buckets. It scans objects and identifies patterns that match common types of sensitive information, such as personally identifiable information (PII), credit card numbers, and intellectual property.</li> <li>Amazon GuardDuty: analyzes event logs (e.g., CloudTrail, VPC Flow Logs) to detect suspicious activities, unauthorized access, and potential security threats. It focuses on identifying malicious behavior.</li> <li>Amazon Inspector: assesses the security posture of EC2 instances, Lambda functions and ECR images. It identifies vulnerabilities, security issues, and potential misconfiguration.</li> <li>Amazon Kinesis: is primarily used for ingesting, processing, and analyzing large volumes of real-time data (e.g., logs, clickstreams, sensor data).</li> <li>VPC Gateway Endpoint is a highly available and scalable AWS service that allows you to privately connect your Amazon Virtual Private Cloud (VPC) to supported AWS services, including Amazon S3 and Amazon SageMaker, without the need for an internet gateway, NAT device, or VPN connection.</li> <li>AWS PrivateLink: allows you to connect your VPC directly to AWS services without exposing your data to the public internet. This is important for businesses that must meet strict compliance standards, as it keeps your network traffic completely private and within the AWS network without relying on public-facing gateways or external connections.</li> <li>AWS Artifact: is a self-service portal that provides access to on-demand AWS compliance documentation. This service offers a comprehensive repository of AWS's security and compliance reports, including certifications, attestations, and agreements. These documents are essential for customers in highly regulated industries, such as healthcare, finance, and biotechnology, as they prove AWS's adherence to industry standards and regulatory requirements. AWS Artifact helps organizations ensure that their data hosted on AWS complies with frameworks like HIPAA, GDPR, and PCI DSS.</li> <li>AWS Config: is used for assessing, auditing, and evaluating the configurations of your AWS resources.</li> <li>AWS Trusted Advisor: is primarily designed to provide real-time best practices guidance to help you optimize your AWS environment.</li> <li>Amazon Q Developer: provides AI-powered solutions to help developers increase productivity by automating important coding processes such as generating code samples, tracking references, and ensuring compliance with open-source licensing. By automating these repetitive operations, developers can concentrate on more complicated and inventive elements of software development, lowering the chance of licensing breaches.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#sagemaker-features","title":"SageMaker features","text":"<ul> <li>Amazon SageMaker JumpStart: provides an easy way to start with pre-built models and solutions, allowing the company to accelerate its machine-learning efforts without needing extensive expertise. JumpStart offers access to a wide variety of pre-trained models and end-to-end solutions that can be quickly deployed and customized, making it ideal for a company with limited data science resources.</li> <li>Amazon SageMaker Data Wrangler: simplifies the process of preparing and transforming data for machine learning. It offers a visual interface for data wrangling, enabling users to clean, transform, and visualize data without writing complex code. This makes it a perfect match for the company's requirement of a low-code solution.</li> <li>Amazon SageMaker Canvas: is a no-code tool that enables users to build machine-learning models by simply interacting with data. It allows business analysts and non-technical users to generate accurate predictions without needing to write a single line of code, addressing the company's need for a no-code solution to improve churn prediction.</li> <li>Amazon SageMaker Clarify: is designed to detect bias in machine learning models and explain model predictions.</li> <li>Amazon SageMaker Model Monitor: is just intended to monitor machine learning models in production to detect data drift and ensure model quality over time.</li> <li>Amazon SageMaker Studio Lab: is a free service primarily aimed at learning and experimenting with machine learning using Jupyter notebooks. While useful for educational purposes and early-stage experimentation, it is not designed to offer a low-code or no-code solution suitable for production-level tasks.</li> <li>Amazon SageMaker Feature Store: is primarily a centralized repository for storing and managing features used in machine learning models. It allows users to share and reuse features across different models.</li> <li>Amazon SageMaker Model Cards: provide a structured framework for documenting essential details about your machine-learning models at every stage of their development. These cards contain information about the model's purpose, training data, evaluation metrics, ethical considerations, and deployment environment. Creating detailed documentation for each model helps organizations adhere to industry standards for transparency and compliance. Model Cards are handy during audits, ensuring all relevant information is documented and easily accessible and facilitating regulatory reviews and internal assessments.</li> <li>Amazon SageMaker Ground Truth: is mainly used for creating high-quality labeled datasets through human review workflows.</li> <li>Amazon SageMaker Debugger: is just a tool for monitoring and debugging machine learning models during training.</li> <li>Amazon SageMaker Training: is a fully managed machine learning (ML) service provided by SageMaker. It helps you effectively train a wide range of ML models at scale. The main components of SageMaker jobs are the containerization of ML workloads, and the management of AWS compute resources. The SageMaker Training platform handles the heavy lifting involved in setting up and managing infrastructure for ML training workloads. With SageMaker Training, you can focus on developing, training, and fine-tuning your model.</li> <li>Amazon SageMaker Autopilot: is an automated machine learning solution that can train and deploy models.</li> <li>Amazon SageMaker Model Registry: is designed specifically to manage machine learning models.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#sagemaker-clarify","title":"SageMaker Clarify","text":"<p>Amazon SageMaker Clarify, which includes using Partial Dependence Plots (PDPs), helps visualize the impact of different features on a model's predictions. This allows stakeholders to better understand each feature's role in the model's decision-making process, enhancing overall model transparency. Amazon SageMaker Clarify generates partial dependence plots (PDPs) to display the marginal effect of features on a machine learning model's predicted outcome. These plots help explain the target response with specific input features. Clarify also extends explainability to both computer vision (CV) and natural language processing (NLP) by utilizing the same Shapley values (SHAP) algorithm that is used for explaining tabular data models.</p>"},{"location":"cloud/aws/cert-ia-prac/#sagemaker-inference-options","title":"SageMaker inference options","text":"<ul> <li>Real-time Inference: Use for low-latency workloads with predictable traffic patterns that need consistent latency characteristics and are always available.</li> <li>Serverless Inference: Ideal for synchronous workloads with spiky traffic patterns that can tolerate latency variations.</li> <li>Batch Inference: Choose for processing large sets of data offline without requiring a persistent endpoint.</li> <li>Asynchronous Inference: Typically used for asynchronous processing where latency is less critical. It helps in cost control by scaling down instances when not in use, which is beneficial for cost-sensitive scenarios.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#sagemaker-built-in-options","title":"SageMaker built-in options","text":"<ul> <li>Model Parallelism is a feature designed to help train large deep-learning models that cannot fit into the memory of a single GPU. This feature automatically partitions the model across multiple GPUs, efficiently training very large models. By distributing the model layers across different GPUs, SageMaker Model Parallelism enables the more effective utilization of memory and computational resources. This distribution helps reduce the training time and allows for handling models that are otherwise too large to be trained on a single GPU.</li> <li>Managed Spot Training is designed to reduce the cost of training jobs by utilizing spare EC2 capacity at a lower price compared to on-demand instances. Thus, Managed Spot Training focuses on cost-efficiency rather than the distribution of large models across multiple GPUs, making it irrelevant to the problem of training very large models.</li> <li>Incremental Training enables a model to be further trained with additional data, leveraging previously learned weights. This method is beneficial for updating a model with new data without retraining from the beginning.</li> <li>Pipe Mode option optimizes the input data pipeline by streaming data directly from Amazon S3 to the training instances. This helps to efficiently handle large datasets by reducing data loading times and improving the overall throughput of the training process.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#amazon-sagemaker-pipelines-step","title":"Amazon SageMaker Pipelines step","text":"<ul> <li>CreateModel: This step creates a deployable SageMaker model from the trained artifacts, preparing it for inference in a production environment.</li> <li>Training: It handles training the machine learning model using the prepared data, generating the model artifacts needed for deployment.</li> <li>QualityCheck: This step evaluates the model to ensure it meets performance and quality standards before being deployed.</li> <li>Processing: It preprocesses and transforms raw data into a suitable format for model training.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#train-a-model-in-sagemaker-training-from-s3-data","title":"Train a model in SageMaker Training from S3 data","text":"<ul> <li>Upload the dataset to Amazon S3.</li> <li>Create a training job in Amazon SageMaker.</li> <li>Configure the training job to use the dataset from Amazon S3.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#amazon-bedrock-features","title":"Amazon Bedrock features","text":"<ul> <li>Model Evaluation on Amazon Bedrock: enables you to assess, compare, and choose the most suitable foundational models for your specific needs. Amazon Bedrock provides the option of automatic evaluation and human evaluation. You can utilize automatic evaluation with predefined algorithms for metrics like accuracy, robustness, and toxicity. Moreover, for subjective or custom metrics such as friendliness, style, and alignment to brand voice, you can establish a human evaluation workflow with just a few clicks. Human evaluation workflows can make use of your own employees or an AWS-managed team as reviewers. Model evaluation offers preselected curated datasets or allows you to bring your own datasets.</li> <li>Amazon Bedrock Guardrails: allow you to set up protections for your AI applications, aligning with your specific use cases and responsible AI guidelines. You can customize multiple guardrails for various scenarios and apply them to different foundation models (FM), ensuring a uniform user experience and consistent safety and privacy measures across your AI applications. These guardrails can be used with both user inputs and model responses based on text.</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#customize-a-foundational-model-using-amazon-bedrock","title":"Customize a foundational model using Amazon Bedrock","text":"<ul> <li>Prepare the training dataset</li> <li>Create a fine-tuning or pre-training job</li> <li>Purchase Provisioned Throughput</li> </ul>"},{"location":"cloud/aws/cert-ia-prac/#amazon-rekognition-features","title":"Amazon Rekognition features","text":"<ul> <li>Content moderation is the process of monitoring and filtering user-generated content to ensure it complies with the platform's policies and guidelines. It involves detecting and removing explicit, offensive, or inappropriate content.</li> </ul>"},{"location":"cloud/aws/cert-saa/","title":"AWS Certified Solutions Architect Associate","text":""},{"location":"cloud/aws/cert-saa/#compute-services","title":"Compute services","text":""},{"location":"cloud/aws/cert-saa/#ec2","title":"EC2","text":"<ul> <li>Termination protection is turned off by default</li> <li>The default action for the root EBS volume is to be deleted when the instance is terminated</li> <li>The other volumes are not deleted by default when the instance is terminated</li> <li>You can encrypt your root volume and the others</li> <li>You can create an encrypted root volume</li> <li>Snapshots of encrypted volumes are encrypted automatically</li> <li>You can only share unencrypted snapshots</li> <li>To encrypt a non encrypted volume:<ul> <li>Create snapshot</li> <li>Copy to encrypted</li> <li>Create AMI from that snapshot</li> </ul> </li> <li>Changes in security groups take effect immediately</li> <li>Security group is stateful, same rule applies for inbound and outbound</li> <li>With security groups you can not blacklist or create deny rules</li> <li>You can attach more than one security group to EC2 instance (up to five)</li> <li>You can attach one security group to multiple instances</li> <li>By default, in a security group all inbound traffic is blocked and all outbound allowed</li> <li>You can't delete the default group, but you can change the group's rules</li> <li>By default, a new EC2 instance uses an IPv4 addressing protocol</li> <li>With EC2 Hibernate, the instance boots much faster (preserver RAM in EBS, so no need to reload the OS). Useful for long-running processes or services that take time to initialize</li> <li>To use hibernation the root volume must be encrypted</li> <li>To use hibernation RAM must be less than 150 GB</li> <li>Instances can't be hibernated more than 60 days</li> <li>Hibernate is available for On-Demand and Reserved Instances</li> <li>You can define boostrap scripts</li> <li>You can get information about an instance (such as public ip) from <code>IP/latest/meta-data/</code> (default info) or <code>IP/latest/user-data/</code> (custom info)</li> <li>Hypervisors: Xen and Nitro</li> <li>In order to achieve the 64,000 IOPS for a provisioned IOPS SSD, you must provision a Nitro-based EC2 instance. The maximum IOPS and throughput are guaranteed only on Instances built on the Nitro System provisioned with more than 32,000 IOPS. Other instances guarantee up to 32,000 IOPS only</li> <li>Hot attach: Attach the ENI to an instance when it is running</li> <li>Warm attach: Attach the ENI to an instance when it is stopped</li> <li>Cold attach: Attach the ENI to an instance when is being launched</li> <li>When stopped and restarted the underlying host of the instance is possibly changed</li> <li>There is no cost if the instance is running and it has only one associated EIP</li> </ul>"},{"location":"cloud/aws/cert-saa/#pricing","title":"Pricing","text":"<ul> <li>On demand<ul> <li>Pay as you go</li> <li>On-Demand Capacity Reservations: provide compute capacity that is always available on the specified recurring schedule</li> </ul> </li> <li>Reserved<ul> <li>Significand discount and contracts of 1 or 3 years</li> <li>Convertible Reserved Instances allow you to exchange for another convertible reserved instance of a different instance family</li> <li>When reserve expires if not terminated then billed as on-demand</li> </ul> </li> <li>Spot<ul> <li>Bid for low price and compute when available</li> <li>If you terminate then you pay for the partial hour, not the same for AWS termination</li> </ul> </li> <li>Dedicated host</li> </ul>"},{"location":"cloud/aws/cert-saa/#spot-instances-and-spot-fleet","title":"Spot instances and Spot fleet","text":"<ul> <li>Spot instances can save up to 90% of the cost on-demand</li> <li>Useful for any type of computing where you don't need persistent storage</li> <li>You can block Spot instances from terminating using Spot block</li> <li>A Spot Fleet is a collection of Spot Instances and, optionally, On-Demand instances</li> <li>When Amazon EC2 is going to interrupt your Spot Instance, it emits an event two minutes prior to the actual interruption</li> <li>In rare situations, Spot blocks may be interrupted due to Amazon EC2 capacity needs</li> <li>It is possible that your Spot Instance is terminated before the warning can be made available</li> </ul>"},{"location":"cloud/aws/cert-saa/#placement-group","title":"Placement Group","text":"<ul> <li>The name for the placement group must be unique in your AWS account</li> <li>Only certain types of instances can be placed in placement groups</li> <li>AWS recommends homogeneous instances</li> <li>You can't merge placement groups</li> <li>You can move an existing instance to a placement group (It needs to be stopped and only by CLI or SDK, not available by console yet)</li> </ul>"},{"location":"cloud/aws/cert-saa/#types","title":"Types","text":"<ul> <li>Clustered Placement Group<ul> <li>All in the same AZ (can not span multiple AZ)</li> <li>For low network latency and high network throughput</li> <li>To solve <code>insufficient capacity error</code> you can stop and restart all the instances (may migrate them to hardware that has capacity for all the requested instances)</li> </ul> </li> <li>Spread Placement Group<ul> <li>Each placed on distinct underlying hardware</li> <li>Can be in multiple AZ</li> <li>For individual critical instances</li> </ul> </li> <li>Partitioned<ul> <li>Each partition has its own set of racks</li> <li>For multiple instances</li> </ul> </li> </ul>"},{"location":"cloud/aws/cert-saa/#ecs-elastic-container-service","title":"ECS (Elastic Container Service)","text":"<ul> <li>Managed container orchestration service</li> <li>Not supports resource-based policies</li> </ul>"},{"location":"cloud/aws/cert-saa/#components","title":"Components","text":"<ul> <li>Cluster: logical collection of ECS resources (EC2 instances or Fargate instances)</li> <li>Task definition: defines your application</li> <li>Container definition: inside a task definition it defines the individual containers and controls the CPU and memory allocation and the port mapping</li> <li>Task: single running copy of any containers defined by task definition</li> <li>Service: allows task definitions to be scaled by adding tasks, defines max and min values</li> <li>Registry: storage for container images</li> <li>Supports ELB</li> <li>Task Roles applies policy per task</li> </ul>"},{"location":"cloud/aws/cert-saa/#eks-elastic-kubernetes-service","title":"EKS (Elastic Kubernetes Service)","text":"<ul> <li>K8s</li> <li>Same toolset on-premise and cloud</li> <li>Containers are grouped in pods</li> <li>Supports EC2 and Fargate</li> <li>Use EKS if you already use K8s and want to migrate to AWS</li> </ul>"},{"location":"cloud/aws/cert-saa/#fargate","title":"Fargate","text":"<ul> <li>Serverless container engine</li> <li>Eliminates need to provision and manage servers</li> <li>Specify and pay for resources per application</li> <li>Works with ECS and EKS</li> <li>Each workload run in its own kernel</li> <li>Isolation and security</li> <li>Choose EC2 instances instead if you have compliance requirements or require broader customizations or access to GPUs</li> </ul>"},{"location":"cloud/aws/cert-saa/#ecr","title":"ECR","text":"<ul> <li>Managed Docker container registry</li> <li>Store, manage and deploy images</li> <li>Integrated with ECS and EKS</li> <li>Works with on-premise deployments</li> <li>HA</li> <li>Integrated with IAM</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-batch","title":"AWS Batch","text":"<ul> <li>For orchestration and automation</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-parallelcluster","title":"AWS ParallelCluster","text":"<ul> <li>For orchestration and automation</li> </ul>"},{"location":"cloud/aws/cert-saa/#sam","title":"SAM","text":"<ul> <li>Helps define and deploy your serverless functions</li> </ul>"},{"location":"cloud/aws/cert-saa/#lambda","title":"Lambda","text":"<ul> <li>Can be used as event-driven compute or as a compute service to run code in response to HTTP requests using API Gateway</li> <li>Scales out automatically</li> <li>Is serverless</li> <li>Supports Node, Java, Python, C#, Go, PowerShell, Ruby</li> <li>Priced on the number of requests and duration rounded to 1ms</li> <li>Lambda functions can trigger other lambda functions</li> <li>Architectures can be extremely complicated, AWS X-ray allows you to debug what is happening</li> <li>Lambda can do things globally</li> <li>The default timeout is 3 seconds and the maximum execution duration per request in AWS Lambda is 900 seconds, which is equivalent to 15 minutes</li> </ul>"},{"location":"cloud/aws/cert-saa/#triggers","title":"Triggers","text":"<ul> <li>API Gateway (sync)</li> <li>Alexa Skill (sync)</li> <li>Cognito (sync)</li> <li>IoT Rule</li> <li>SNS</li> <li>Kinesis</li> <li>SQS</li> <li>S3</li> <li>DynamoDB</li> <li>EventBridge</li> <li>CloudWatch</li> <li>CloudFront (sync)</li> <li>ALB (sync)</li> <li>Lex (sync)</li> <li>Kinesis Data Firehose (sync)</li> </ul>"},{"location":"cloud/aws/cert-saa/#elastic-beanstalk","title":"Elastic Beanstalk","text":"<ul> <li>You can deploy and manage applications without worrying about the infrastructure</li> <li>You simply upload the application, it handles automatically the details of capacity provisioning, load balancing, scaling and application health monitoring</li> </ul>"},{"location":"cloud/aws/cert-saa/#elastic-transcoder","title":"Elastic transcoder","text":"<ul> <li>Media transcoder in the cloud</li> <li>Converts media files to different formats</li> <li>Provides transcoding presets for popular output formats</li> <li>Pay based on the minutes that you transcode and the resolution at which you transcode</li> </ul>"},{"location":"cloud/aws/cert-saa/#storage-services","title":"Storage services","text":""},{"location":"cloud/aws/cert-saa/#ebs","title":"EBS","text":"<ul> <li>Volume in same AZ than the EC2</li> <li>Snapshot exists in S3</li> <li>Snapshots are incremental</li> <li>Is recommended to stop the instance to create a snapshot from the root volume</li> <li>You can change EBS volume size and type on the fly</li> <li>To move a volume to other AZ you need to create a snapshot, then create image (AMI) from snapshot and finally launch in the desired AZ</li> <li>To move a volume to another region, snapshot it, create AMI, copy AMI to other region and the use copied AMI to launch EC2 instance</li> <li>Is not configured as an NFS file system</li> <li>Use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation of EBS snapshots</li> <li>Lower latency than S3</li> <li>You can enable the EBS Encryption By Default feature for the AWS Region</li> </ul>"},{"location":"cloud/aws/cert-saa/#volume-types","title":"Volume types","text":""},{"location":"cloud/aws/cert-saa/#ssd","title":"SSD","text":"<ul> <li>For small, random I/O operations</li> <li>Can be used as bootable volume</li> <li>Best for transactional workloads, IOPS performance, database performance</li> <li>Focused in IOPS</li> <li>Types:<ul> <li>General purpose SSD (gp2)</li> <li>Provisioned IOPS SSD (io1)<ul> <li>For critical business applications that require sustained IOPS performance</li> <li>For databases</li> <li>For each 10 GiB volume can be provisioned with up to 500 IOPS, up to limit</li> </ul> </li> </ul> </li> </ul>"},{"location":"cloud/aws/cert-saa/#hdd","title":"HDD","text":"<ul> <li>For large, sequential I/O operations</li> <li>Can not be used as bootable volume</li> <li>Best for large streaming workloads, big data</li> <li>Focused in throughput (MiB/s)</li> <li>Types:<ul> <li>Throughput optimized HDD (st1)</li> <li>For frequently accessed data</li> <li>Cold HDD (sc1)</li> <li>For infrequently accessed data </li> </ul> </li> </ul>"},{"location":"cloud/aws/cert-saa/#raid","title":"RAID","text":"<ul> <li>RAID 0: enables you to improve your storage volumes' performance by distributing the I/O across the volumes in a stripe</li> <li>RAID 1: used for data mirroring</li> </ul>"},{"location":"cloud/aws/cert-saa/#ami-types","title":"AMI types","text":"<ul> <li>Instance Store Volumes are sometimes called Ephemeral Storage</li> <li>Instance store volumes can not be stopped. If the underlying host fails you will lose the data</li> <li>EBS can be stopped without any lose</li> <li>You can reboot both without any lose</li> <li>By default, boot root volumes will be deleted on termination but with EBS you can tell AWS to keep it</li> </ul>"},{"location":"cloud/aws/cert-saa/#efs-elastic-file-system","title":"EFS (Elastic File System)","text":"<ul> <li>File storage service for EC2 that can be shared</li> <li>Supports Networking File System v4 (NFSv4) protocol</li> <li>POSIX-compliant shared file system</li> <li>You only pay for the storage you use (no pre-provisioning required)</li> <li>Can scale to petabytes</li> <li>Can support thousands of concurrent NFS connections</li> <li>Data is stored across multiple AZ's within a region</li> <li>Read after write consistency</li> <li>Provide low-latency file operations, but EBS has better low-latency</li> </ul>"},{"location":"cloud/aws/cert-saa/#amazon-fsx-for-windows","title":"Amazon FSx for Windows","text":"<p>When you need centralised storage for Windows-based applications such as Sharepoint, MicrosoftSQL Server, Workspace, IIS Web Server or any other native Microsoft Application. Runs on Windows Server Message Block (SMB)-based file services.</p>"},{"location":"cloud/aws/cert-saa/#amazon-fsx-for-lustre","title":"Amazon FSx for Lustre","text":"<p>When you need high-speed, high-capacity distributed storage. This will be for applications that do High Performance Compute (HPC), financial modelling etc. Can store data directly on S3.</p>"},{"location":"cloud/aws/cert-saa/#s3","title":"S3","text":"<ul> <li>Object-based storage</li> <li>File size from 0 bytes to 5 TB</li> <li>Stored in Buckets</li> <li>Universal namespace</li> <li>200 status response when create file and the MD5 checksum</li> <li>Key, value, metadata and version ID, and subresources (ACL and Torrent)</li> <li>built for 99.99% availability, guarantee 99.9%</li> <li>11 9s for durability</li> <li>Can configure MFA Delete</li> <li>Versioning can not be disabled, only suspended</li> <li>Each version needs to be public individually</li> <li>Deleting deleted object with version restores it</li> <li>Lifecycle management</li> <li>S3 Select allows to query data with SQL (csv in zip for example)</li> <li>Glacier Select</li> <li>Cross region replication requires versioning to be enabled in both buckets</li> <li>Cross region replication only starts when is activated and with new versions, old objects aren't replicated</li> <li>Cross region replication now copies public permission on the file</li> <li>Cross region replication not replicated delete markers or deleting individual versions</li> <li>S3 Transfer acceleration uses CloudFront Edge network to accelerate S3 uploads</li> <li>Path style url will be eventually deprecated</li> <li>The \"resource owner\" refers to the AWS account that creates Amazon S3 buckets and objects</li> <li>Objects must be stored at least 30 days in the current storage class before you can transition them to STANDARD_IA or ONEZONE_IA</li> </ul>"},{"location":"cloud/aws/cert-saa/#consistency","title":"Consistency","text":"<ul> <li>R and W consistency for PUTS on new objects</li> <li>Eventual consistency for overwrite PUTS and DELETES (changes take time)</li> </ul>"},{"location":"cloud/aws/cert-saa/#storage-classes","title":"Storage classes","text":"<ul> <li>S3 standard: when require multiple AZ</li> <li>S3 IA: for fast infrequent access, charged retrieval fee</li> <li>S3 One zone IA: for fast infrequent access, when no require multiple AZ</li> <li>S3 Intelligent tier: automatic management of tier</li> <li>S3 Glacier: retrieval times configurable from minutes to hours</li> <li>S3 Glacier Deep Archive: lowest cost, retrieval time of 12 hours </li> </ul>"},{"location":"cloud/aws/cert-saa/#glacier-retrievals","title":"Glacier retrievals","text":"<ul> <li>Expedited retrievals typically allows access in less than 5 min, provisioned capacity ensures this</li> <li>Provisioned capacity allows for each unit provides 3 expedited retrievals can be performed every 5 min and at 150 MB/s</li> </ul>"},{"location":"cloud/aws/cert-saa/#performance","title":"Performance","text":"<ul> <li>You can get better performance by spreading you reads across different prefixes (paths)</li> <li>3.500 PUT/COPY/POST/DELETE</li> <li>5.500 GET/HEAD</li> <li>Keep in mind KMS limits</li> <li>5.500 / 10.000 / 30.000 depending on region, quota can not be increased</li> <li>Multipart upload, recommended over 100 MB, required over 5GB</li> <li>Byte-Range Fetches for download big files</li> </ul>"},{"location":"cloud/aws/cert-saa/#security-and-encryption","title":"Security and encryption","text":"<ul> <li>Can enable access logs (Server access logging)</li> <li>Encryption in transit (SSL/TLS)</li> <li>Encryption at rest (server side)<ul> <li>S3 Managed Keys (SSE-S3)</li> <li>AWS Key Managed Service (SSE-KMS)<ul> <li>SSE-KMS also provides you with an audit trail that shows when your CMK was used and by whom</li> </ul> </li> <li>Customer Provided Keys (SSE-C)</li> </ul> </li> <li>Client side encryption</li> </ul>"},{"location":"cloud/aws/cert-saa/#s3-object-locks","title":"S3 Object Locks","text":"<ul> <li>Use to store once read many (WORM)</li> <li>Can be applied to individual objects or buckets</li> <li>Two modes<ul> <li>Governance (special permissions)</li> <li>Compliance (nobody)</li> </ul> </li> </ul>"},{"location":"cloud/aws/cert-saa/#cross-account-access","title":"Cross account access","text":"<ul> <li>Bucket policies + IAM (on the bucket). Only programmatic access.</li> <li>Bucket ACLs + IAM (individual objects). Only programmatic access.</li> <li>Cross account IAM Roles. Programmatic and consoles access.</li> </ul>"},{"location":"cloud/aws/cert-saa/#charges","title":"Charges","text":"<ul> <li>Storage</li> <li>Requests</li> <li>Storage Management Pricing</li> <li>Data Transfer Pricing</li> <li>Transfer Acceleration</li> <li>Cross Region Replication Pricing</li> </ul>"},{"location":"cloud/aws/cert-saa/#host-a-website","title":"Host a website","text":"<ul> <li>The S3 bucket name must be the same as the domain name</li> <li>Requires a registered domain name</li> </ul>"},{"location":"cloud/aws/cert-saa/#storage-gateway","title":"Storage gateway","text":"<ul> <li>File gateway<ul> <li>For flat files</li> <li>Can store in S3 using the NFS and SMB protocols</li> <li>Stored directly to S3</li> <li>Can then mount an SMB shared mounted as local disk</li> </ul> </li> <li>Volume gateway<ul> <li>Stored volumes<ul> <li>Entire dataset stored on site and asynchronously backed to S3</li> <li>Internet Small Computer System Interface (iSCSI) devices</li> </ul> </li> <li>Cached volumes<ul> <li>Entire dataset stored in S3 and the most frequently accessed data is cached on site</li> <li>Internet Small Computer System Interface (iSCSI) devices</li> </ul> </li> </ul> </li> <li>Tape gateway<ul> <li>For tape backup that needs to be migrated to AWS S3 or AWS Glacier</li> </ul> </li> </ul>"},{"location":"cloud/aws/cert-saa/#database-services","title":"Database services","text":""},{"location":"cloud/aws/cert-saa/#aurora","title":"Aurora","text":"<ul> <li>MySQL and PostgreSQL compatible relational database</li> <li>10GB to 10GB up to 64TB</li> <li>Compute resources can scale up to 32vCPU and 244GB of Memory</li> <li>2 copies in each AZ with minimum of 3 AZ</li> <li>Can lose 2 copies without affect writes and 3 without reads</li> <li>Storage auto-healing</li> <li>Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case</li> <li>Has a built-in Cluster endpoint (aka writer)</li> <li>Has a built-in Reader endpoint</li> <li>Types of replicas:<ul> <li>Aurora (15) (Only with automated failover available)</li> <li>Mysql (5)</li> <li>PostgreSQL(1)</li> </ul> </li> <li>The read replication latency of less than 1 second is only possible if you would use Amazon Aurora replicas</li> </ul>"},{"location":"cloud/aws/cert-saa/#backups","title":"Backups","text":"<ul> <li>Automated enabled by default without any impact on performance</li> <li>Can take snapshots without any impact on performance</li> <li>Can share snapshots with other AWS accounts</li> </ul>"},{"location":"cloud/aws/cert-saa/#aurora-serverless","title":"Aurora Serverless","text":"<ul> <li>Provides a relatively simple, cost-effective option for infrequent, intermittent or unpredictable workloads</li> </ul>"},{"location":"cloud/aws/cert-saa/#dynamodb","title":"DynamoDB","text":"<ul> <li>NoSQL</li> <li>Stored on SSD</li> <li>Spaced across 3 geographically distinct data centers</li> <li>Eventual Consistent Reads (Default)</li> <li>Strongly Consistent reads (Reflects all writes, for millisecond read after write)</li> <li>The maximum item size (key + value) in DynamoDB is 400 KB</li> <li>Solve throttling by enabling auto-scaling</li> </ul>"},{"location":"cloud/aws/cert-saa/#dynamodb-streams","title":"DynamoDB Streams","text":"<ul> <li>Captures a time-ordered sequence of item-level modifications in any table</li> <li>Stores this information in a log for up to 24 hours</li> </ul>"},{"location":"cloud/aws/cert-saa/#dynamodb-accelerator-dax","title":"DynamoDB Accelerator (DAX)","text":"<ul> <li>Read and write caching (to reduce response times from milliseconds to microseconds)</li> </ul>"},{"location":"cloud/aws/cert-saa/#transactions","title":"Transactions","text":"<ul> <li>Up to 25 items or 4 MB</li> <li>Consume more reads / writes (prepare and commit)</li> </ul>"},{"location":"cloud/aws/cert-saa/#on-demand-capacity","title":"On-Demand Capacity","text":"<ul> <li>Not the default</li> <li>Pay-per-request</li> <li>No minimum capacity</li> <li>No charge for read/write (only storage and backups)</li> <li>Pay more per request than with provisioned</li> <li>Used for new product launches</li> </ul>"},{"location":"cloud/aws/cert-saa/#on-demand-backup-and-restore","title":"On-Demand Backup and restore","text":"<ul> <li>Full backups at any time</li> <li>Zero impact on performance or availability</li> <li>Consistent in seconds and retained until deleted</li> <li>Operates in same region as the source table</li> </ul>"},{"location":"cloud/aws/cert-saa/#point-in-time-recovery-pitr","title":"Point in Time Recovery (PITR)","text":"<ul> <li>Restore to any point in the last 35 days</li> <li>Incremental backups</li> <li>Not enabled by default</li> <li>Latest restorable: five minutes in the past</li> </ul>"},{"location":"cloud/aws/cert-saa/#global-tables","title":"Global Tables","text":"<ul> <li>Require streams (24h changes history)</li> <li>Requires to add a region the table</li> <li>Almost instantaneously</li> </ul>"},{"location":"cloud/aws/cert-saa/#security","title":"Security","text":"<ul> <li>Encryption at rest with KMS</li> <li>Site-to-site VPN</li> <li>Direct connect (DX)</li> <li>IAM policies and roles</li> <li>Fine-grained access</li> <li>Can monitor with CloudWatch and CloudTrail</li> <li>VPC endpoints</li> </ul>"},{"location":"cloud/aws/cert-saa/#elasticache","title":"ElastiCache","text":"<ul> <li>Supports Memcached and Redis</li> <li>Redis is Multi-AZ</li> <li>Memcached is multi-thread</li> <li>You can do backups and restore Redis</li> <li>Using Redis AUTH command can improve data security by requiring the user to enter a password before they are granted permission to execute Redis commands on a password-protected Redis server </li> </ul>"},{"location":"cloud/aws/cert-saa/#rds","title":"RDS","text":"<ul> <li>Multi-AZ: for disaster recovery, automatically provisions and maintains a synchronous \u201cstandby\u201d replica in a different AZ (can have read replicas in multiple AZ)</li> <li>Multi-AZ requires one to two minutes to complete failover</li> <li>Read replicas: for performance (require automatic backups, limit of 5 for database instance, can chain but increase latency)</li> <li>For OLTP</li> <li>Run on virtual machines and you can't log in</li> <li>Patching is AWS responsibility</li> <li>Not serverless</li> <li>Automated backups enabled by default</li> <li>Replicas can be promoted to master</li> <li>You can enable storage autoscaling to never run out of disk space or manually increase the allocated storage</li> <li>When encrypted then all is encrypted (backups, read replicas and snapshots)</li> <li>Max size of 16TB</li> <li>You can invoke an AWS Lambda function from an Amazon Aurora MySQL-Compatible Edition DB cluster with a native function or a stored procedure</li> <li>Use Enhanced Monitoring to closely monitor how the different processes or threads on a DB instance use the CPU</li> <li>Enable IAM DB Authentication to use an authentication token (15 min lifetime)</li> <li>Amazon RDS Root CA certificate can be used to increase security</li> </ul>"},{"location":"cloud/aws/cert-saa/#redshift","title":"Redshift","text":"<ul> <li>Data warehouse for OLAP</li> <li>Can be single node (160 GB) or multi node (leader node and compute nodes, up to 128 for each leader node)</li> <li>Advanced compression automatically</li> <li>Massive parallel processing (MPP): distributes loads in all nodes</li> <li>Backups enabled by default with 1 day retention, configurable up to 35</li> <li>Always attempts to maintain 3 at least 3 copies (original, replica one in compute nodes and backup in S3)</li> <li>Can asynchronously replicate to S3 in another region for disaster recovery</li> <li>Only compute nodes are charged, not leader</li> <li>Backups and data transfer are charged</li> <li>Only available in 1 AZ</li> <li>Automated snapshots are not enough for a region outrage</li> <li>Can enable Cross-Region Snapshots</li> <li>Redshift Spectrum allows you to directly run SQL queries against exabytes of unstructured data in Amazon S3</li> </ul>"},{"location":"cloud/aws/cert-saa/#migration-services","title":"Migration services","text":""},{"location":"cloud/aws/cert-saa/#database-migration-service-dms","title":"Database Migration Service (DMS)","text":"<ul> <li>Helps to migrate databases from and to AWS and also between on-premises through AWS.</li> <li>Supports homogeneous and heterogeneous migrations</li> <li>SCT (Schema Conversion Tool) is required for heterogeneous migrations</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-datasync","title":"AWS DataSync","text":"<ul> <li>Used to move large amount of data from on-premise to AWS (on-premises storage will no longer be used)</li> <li>Used with NFS and SMB compatible file systems</li> <li>Replication can be done hourly, daily or weekly</li> <li>Install the DataSync agent to start the replication</li> <li>Can be used to replicate EFS to EFS</li> <li>Can move directly to S3 Glacier Deep Archive</li> </ul>"},{"location":"cloud/aws/cert-saa/#snowball-edge","title":"Snowball Edge","text":"<ul> <li>Used to transfer massive amounts of data</li> <li>Is a transported device</li> <li>Can import and export to S3</li> </ul>"},{"location":"cloud/aws/cert-saa/#snowmobile","title":"Snowmobile","text":"<ul> <li>Up to 100PB</li> </ul>"},{"location":"cloud/aws/cert-saa/#networking-and-content-delivery","title":"Networking and content delivery","text":""},{"location":"cloud/aws/cert-saa/#api-gateway","title":"API Gateway","text":"<ul> <li>Low cost (pay by calls and data transferred) and scales automatically</li> <li>Can throttle to prevent attacks</li> <li>Can log results to CloudWatch</li> <li>You can enable caching</li> <li>Can configure Same Origin Policy and CORS (helps to prevent XSS)</li> <li>CORS is enforced by the client browser</li> </ul>"},{"location":"cloud/aws/cert-saa/#cloudfront","title":"CloudFront","text":"<ul> <li>Edge Location: not just read only</li> <li>Lambda@Edge lets you run code closer to users of your application, which improves performance and reduces latency</li> <li>Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users</li> <li>Object cached for the life of TTL</li> <li>You can clear cache objects (invalidations), but you will be charged</li> <li>Origin: S3/EC2/ELB/Route53</li> <li>Origin Access Identity (OAI): for security</li> <li>the Cache-Control max-age directive is set to zero if the origin server is being hit for each request instead of the AWS Edge locations</li> <li>Distribution: Name given the CDN which consist of a collection of Edge Locations</li> <li>Web Distribution: for websites, supports SNI Custom SSL</li> <li>RTMP: for media streaming</li> </ul>"},{"location":"cloud/aws/cert-saa/#signed-urls-and-cookies","title":"Signed URLS and cookies","text":"<ul> <li>1 file = 1 URL</li> <li>1 cookie = multiple files</li> <li>If your origin is EC2 the use CloudFront</li> </ul>"},{"location":"cloud/aws/cert-saa/#route-53","title":"Route 53","text":"<ul> <li>Always use Alias record over CNAME</li> <li>AAAA record is IPv6 and A is IPv4</li> <li>You can buy domain names in AWS and can take up to 3 days</li> <li>To point a subdomain to an ELB you need an Alias or CNAME record</li> </ul>"},{"location":"cloud/aws/cert-saa/#routing-policies","title":"Routing policies","text":"<ul> <li>Simple routing: One record with multiple IP addresses, returns values in random order</li> <li>Weighted routing: Splits traffic based on weights</li> <li>Latency-based routing: Routes to the lowes latency region available</li> <li>Failover routing: When the primary route is not available then uses the secondary one</li> <li>Geolocation routing: Based on the geographic location (continents or countries) of the users</li> <li>Geo-proximity routing (traffic flow only): Can create extremely complicated flows</li> <li>Multivalue answer routing: Like simple routing but with health check on each IP</li> </ul>"},{"location":"cloud/aws/cert-saa/#routing-failover-strategies","title":"Routing failover strategies","text":"<ul> <li>Active-Active: All receive traffic (not failover policy)</li> <li>Active-Passive: Only one part receive traffic (failover policy)</li> </ul>"},{"location":"cloud/aws/cert-saa/#health-checks","title":"Health checks","text":"<ul> <li>You can set checks on individual record sets</li> <li>If a check fail, the record is removed until it passes again</li> <li>You can set SNS notifications to alert you</li> </ul>"},{"location":"cloud/aws/cert-saa/#elb","title":"ELB","text":"<ul> <li>Not have predefined ipv4 address, you resolve them using DNS name</li> <li>If the application stops responding an 504 is returned</li> <li>Target groups to aggregate targets and use as they in application load balancers</li> <li>Sticky sessions are used to link call to final EC2 destination</li> <li>Cross zone load balancing allows crossing AZ to keep traffic balanced</li> <li>Distribute traffic only within their respective regions</li> <li>Path patterns allows sending traffic to other destination base on path rules</li> <li>Can enable access logs that capture detailed information about requests sent to your load balancer</li> </ul>"},{"location":"cloud/aws/cert-saa/#types_1","title":"Types","text":"<ul> <li>Application load balancers</li> <li>For HTTP and HTTPs traffic (Layer 7)</li> <li>Can support path-based and host-based routing</li> <li>Can not assign Elastic IP</li> <li>Cross-zone load balancing always enabled</li> <li>Network load balancers</li> <li>For TCP traffic (Layer 4)</li> <li>Ultra low latency and capable to handling millions of requests per second</li> <li>Can assign Elastic IP</li> <li>Cross-zone load balancing disabled by default</li> <li>Classic load balancers</li> <li>Legacy</li> <li>Basic load balancing at cost-effective level</li> <li>Cross-zone load balancing disabled by default in CLI but enabled in console</li> </ul>"},{"location":"cloud/aws/cert-saa/#vpcs","title":"VPCs","text":"<p> - Default VPC creates a default Route Table, Network Access Control List (NACL) and default Security Group - Default VPC not creates subnets or default internet gateway - AZ are randomized - Amazon always reserve 5 IP addresses within your subnets (first 4 and last) - Only one internet gateway per VPC - One subnet only in one availability zone - Security groups can't span VPCs - Egress-only internet gateways does not utilize IPv4 (only IPv6) - Works with IPv4 and optionally with IPv6 (enabling dual-stack mode) - Can enable or disable DNS resolution and hostnames</p>"},{"location":"cloud/aws/cert-saa/#nat-instance","title":"NAT Instance","text":"<ul> <li>Is a EC2 instance</li> <li>Disable Source/Destination Check on the instance</li> <li>Must be in a public subnet</li> <li>Must be a route out of the private subnet to the NAT</li> <li>The network bandwidth of the NAT instance depends on the bandwidth of the instance type</li> <li>You can create a HA but is complicated</li> <li>Behind a security group</li> </ul>"},{"location":"cloud/aws/cert-saa/#nat-gateway","title":"NAT Gateway","text":"<ul> <li>Must be in a public subnet</li> <li>Redundant inside AZ</li> <li>Preferred by enterprise</li> <li>Starts at 5Gbps and scales to 54Gbps</li> <li>Not associated with security groups</li> <li>Automatically assigned public IP</li> <li>Need to update the route tables</li> <li>No need to disable the Source/Destination Check</li> <li>Create one in each AZ to get HA</li> </ul>"},{"location":"cloud/aws/cert-saa/#network-acl","title":"Network ACL","text":"<ul> <li>Configure inbound and outbound rules separately and each can allow or deny traffic (configure rule number by increments of 100)</li> <li>Rules applied by rule number (lower first)</li> <li>Only one NACL by subnet but can multiple subnets in the same NACL</li> <li>The VPC automatically comes with a default network ACL, and by default allows all inbound and outbound traffic</li> <li>You can add or remove rules from the default network ACL</li> <li>You can not edit or remove the default rule <code>* All Traffic Deny</code></li> <li>You must have an outbound rule to allow ephemeral ports (32768 - 65535) to answer clients (used as the client\u2019s source port for the traffic response)</li> </ul>"},{"location":"cloud/aws/cert-saa/#flow-logs","title":"Flow Logs","text":"<ul> <li>You cannot enable for VPCs that are peering with your VPCs unless is in your account</li> <li>You can tag</li> <li>You cannot change the configuration once it is created</li> <li>Not all traffic is monitored: AWS DNS, Windows licences, instance-metadata, DHCP and reserved IPs by the default VPC router</li> </ul>"},{"location":"cloud/aws/cert-saa/#vpc-endpoints","title":"VPC endpoints","text":"<ul> <li>Without requiring gateway, NAT, VPN nor Direct Connect</li> <li>Interface: Elastic network interface with a private IP that serves as an entry point for traffic destined to a supported service</li> <li>Gateway: For S3 and DynamoDB</li> </ul>"},{"location":"cloud/aws/cert-saa/#bastion-host","title":"Bastion host","text":"<ul> <li>A NAT is used to provide internet traffic to EC2 instances in private subnets</li> <li>A bastion is used to securely administer EC2 instances</li> <li>You can not use a NAT Gateway as a Bastion</li> <li>You can not use ALB because is layer 7, and you need to use layer 4</li> <li>For HA in pro you need 2 hosts in 2 AZ and use a NLB with static IP address</li> <li>For HA in dev you need an auto-scaling of 1 and a fixed EIP (you can use a user data script to provision the same)</li> </ul>"},{"location":"cloud/aws/cert-saa/#vpc-peering","title":"VPC Peering","text":"<ul> <li>You can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region</li> <li>VPC peering connection does not support edge to edge routing like:<ul> <li>A VPN connection or an AWS Direct Connect connection to a corporate network</li> <li>An Internet connection through an Internet gateway</li> <li>A gateway VPC endpoint to an AWS service; for example, an endpoint to Amazon S3</li> <li>(IPv6) A ClassicLink connection</li> </ul> </li> </ul>"},{"location":"cloud/aws/cert-saa/#direct-connect-dx","title":"Direct connect (DX)","text":"<ul> <li>Directly connects data center to AWS</li> <li>Useful for high throughput workloads</li> <li>Or if you need a stable reliable secure connection</li> </ul>"},{"location":"cloud/aws/cert-saa/#steps","title":"Steps","text":"<ul> <li>Create a virtual interface in the DC console. This is a PUBLIC virtual interface</li> <li>Go to VPC console and in the VPN connection create a Customer Gateway</li> <li>Create a virtual private gateway</li> <li>Attach it to the desired VPC</li> <li>Select VPN connections and create a new one</li> <li>Select the virtual private gateway and the customer gateway</li> <li>Once the VPN is available, set up the VPN on the customer gateway or firewall</li> </ul>"},{"location":"cloud/aws/cert-saa/#global-accelerator","title":"Global accelerator","text":"<ul> <li>Is a service in which you create accelerators to improve availability and performance of your application for local and global users</li> <li>You are assigned 2 static IPs (or can bring your own), each in a different network zone</li> <li>You can control traffic using traffic dials in the endpoint group</li> <li>This service is more suitable for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover</li> <li>Traffic can be routed to the closest edge location via an Anycast static IP address</li> <li>With AWS Global Accelerator, you can add or remove endpoints in the AWS Regions, run blue/green deployment, and A/B test without needing to update the IP addresses in your client applications</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-transit-gateway","title":"AWS Transit gateway","text":"<ul> <li>Allows you to have transitive peering between a thousand of VPCs and on-premises data centers (simplify topology)</li> <li>Works on hub-and-spoke model</li> <li>Works on regional basis but can have it across multiple regions</li> <li>You can use it across multiple accounts using RAM</li> <li>You can use route tables to limit how VPCs interact</li> <li>Works with Direct Connect and VPN connections</li> <li>Supports IP multicast</li> <li>Enables you to scale the IPsec VPN throughput with equal-cost multi-path (ECMP) routing support over multiple VPN tunnels</li> <li>Associate the VPCs to an Equal Cost Multipath Routing (ECMR)-enabled transit gateway and attach additional VPN tunnels to scale the throughput of the VPN connections</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-privatelink","title":"AWS PrivateLink","text":"<ul> <li>For peering VPCs to a lot of customers VPCs</li> <li>Doesn't require VPC peering</li> <li>Requires a Network Load Balancer in the service VPC and an ENI on the customer VPC</li> </ul>"},{"location":"cloud/aws/cert-saa/#vpn-cloudhub","title":"VPN CloudHub","text":"<ul> <li>Is only for VPNs and not for VPCs</li> <li>If you have multiple sites, each with its own VPN connection, you can use this to connect those sites together</li> <li>Hub-and-spoke model</li> <li>Low cost, easy to manage</li> <li>It operates over the public internet with all traffic encrypted</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-managed-vpn","title":"AWS Managed VPN","text":"<ul> <li>It lets you reuse existing VPN equipment and processes, and reuse existing internet connections</li> <li>It is an AWS-managed high availability VPN service</li> <li>It supports static routes or dynamic Border Gateway Protocol (BGP) peering and routing policies.</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-certificate-manager","title":"AWS Certificate Manager","text":"<ul> <li>To persist SSL/TLS certificates (IAM certificate store is another valid alternative)</li> </ul>"},{"location":"cloud/aws/cert-saa/#network-costs","title":"Network costs","text":"<ul> <li>Use private IPs over public ones to save costs (this utilizes the AWS backbone network)</li> <li>If you want to cut all the network costs, group all EC2 instances in the same AZ and use private IPs (single point of failure)</li> </ul>"},{"location":"cloud/aws/cert-saa/#eni-vs-ena-vs-efa","title":"ENI vs ENA vs EFA","text":"<p>ENI: For basic networking. Perhaps you need a separate management network to your production network or a separate logging network, and you need to do this at low cost. In this scenario use multiple ENIs for each network.</p> <p>Enhanced network adapter (ENA): When you need speed from 10Gbps to 100Gbps. Anywhere you need reliable, high throughput.</p> <p>Elastic fabric adapter (EFA): When you need to accelerate High Performing Computing (HPC) and machine learning applications or if you need to do an OS by-pass. If you see a scenario question mentioning HPC or ML and asking what network adapter you want, choose EFA.</p>"},{"location":"cloud/aws/cert-saa/#security-and-identity-services","title":"Security and identity services","text":""},{"location":"cloud/aws/cert-saa/#cognito","title":"Cognito","text":"<ul> <li>Provides web identity federation (users of your app can sign in using a well-known identity provider (IdP))</li> <li>Recommended for all mobile applications</li> <li>Brokers between the app and Google/Facebook to provide temporary credentials which map to an IAM role</li> <li>No need for application to embed or store AWS credentials locally</li> <li>Push synchronization using silent SNS</li> </ul>"},{"location":"cloud/aws/cert-saa/#user-pools","title":"User pools","text":"<ul> <li>For users</li> <li>Generates JWTs</li> </ul>"},{"location":"cloud/aws/cert-saa/#identity-pools","title":"Identity pools","text":"<ul> <li>Provide temporary AWS credentials to AWS services when the user exchanges the JWT</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-directory-service-ad-connector","title":"AWS Directory Service AD Connector","text":"<ul> <li>Provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services</li> <li>You can assign an IAM Role to the users or groups from your Active Directory once it is integrated with your VPC via the AWS Directory Service AD Connector</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-directory-service-simple-ad","title":"AWS Directory Service Simple AD","text":"<ul> <li>Just provides a subset of the features offered by AWS Managed Microsoft AD</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-inspector","title":"AWS Inspector","text":"<ul> <li>Automated security assessment service that helps improve the security and compliance of applications deployed on AWS</li> </ul>"},{"location":"cloud/aws/cert-saa/#macie","title":"Macie","text":"<ul> <li>Uses AI to analyze data in S3 and helps identify PII</li> <li>Can also be utilized to analyze CloudTrail logs for suspicious API activity</li> <li>Includes dashboards, reports and alerting</li> <li>Great for PCI-DSS compliance and preventing ID theft</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-firewall-manager","title":"AWS Firewall Manager","text":"<ul> <li>Centrally configure and manage firewall rules across an AWS Organization (WAF, AWS Shield Advanced, Security groups)</li> </ul>"},{"location":"cloud/aws/cert-saa/#iam","title":"IAM","text":"<ul> <li>You can customize the password policy</li> <li>Roles are more secure than storing access keys and are easier to manage</li> <li>Roles can be assigned to an EC2 instance after is created using both, the console and command line</li> <li>Roles are universal</li> <li>Roles can be assumed by users</li> <li>You can add a condition to the IAM policy which allows access to specific tags</li> </ul>"},{"location":"cloud/aws/cert-saa/#components_1","title":"Components","text":"<ul> <li>Users</li> <li>Groups: collection of users</li> <li>Policies: give permissions to users/groups/roles</li> <li>Role: assign them to AWS resources</li> </ul>"},{"location":"cloud/aws/cert-saa/#policies","title":"Policies","text":"<ul> <li>ARN: <code>arn:partition:service:region:account_id:(resource|resource_type/resource)</code></li> <li>A policy document is a list of statements, where each statement matches an AWS API request</li> <li>Each statement has an effect (allow/deny) to do some actions over a list or resources</li> <li>Explicit deny always overrides, and not explicit allow is an implicit deny</li> <li>Permission boundary limits policies to access specific services</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-sso","title":"AWS SSO","text":"<p>Centrally manage access using existing identities with account level permissions. </p>"},{"location":"cloud/aws/cert-saa/#kms","title":"KMS","text":"<ul> <li>Regional secure key management and encryption and decryption</li> <li>Manages customer master keys (CMKs)</li> <li>Ideal for S3 objects, database passwords and API keys stored in System Management Parameter Store</li> <li>Encrypt and decrypt data up to 4 KB size</li> <li>Pay per API call</li> <li>Audit capability using CloudTrail (logs in S3)</li> <li>FIPS 140-2 Level 2 (Level 3 is CloudHSM)</li> <li>AWS services integrated with KMS do not support asymmetric CMKs</li> <li>Asymmetric CMKs are used outside AWS by users who can't call KMS APIs</li> </ul>"},{"location":"cloud/aws/cert-saa/#types_2","title":"Types","text":"<ul> <li>AWS Managed CMK: can view, can not manage and dedicated to my account</li> <li>Customer Managed CMK: can view, can manage and dedicated to my account</li> <li>AWS Owner CMK: can not view, can manage and not dedicated to my account</li> </ul>"},{"location":"cloud/aws/cert-saa/#cloudhsm","title":"CloudHSM","text":"<ul> <li>Dedicated hardware module security</li> <li>FIPS 140-2 Level 3 (Level 2 is KMS)</li> <li>You manage your own keys</li> <li>No access to the AWS-managed component</li> <li>Runs within a dedicated VPC</li> <li>Single tenant, dedicated hardware, multi AZ cluster</li> <li>Industry-standard APIs (no AWS APIs)</li> <li>PKCS#11</li> <li>Java Cryptography Extensions (JCE)</li> <li>Microsoft CryptoNG (CNG)</li> <li>Keys irretrievable if lost</li> </ul>"},{"location":"cloud/aws/cert-saa/#organizations","title":"Organizations","text":"<ul> <li>Paying account should be used only for billing purposes</li> <li>Enable/disable services using SCP on OU or individual accounts</li> </ul>"},{"location":"cloud/aws/cert-saa/#consolidated-billing","title":"Consolidated Billing","text":"<ul> <li>One bill per AWS account</li> <li>Very easy to track charges and allocate costs</li> <li>Volume pricing discount</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-resource-access-manager-ram","title":"AWS Resource Access Manager (RAM)","text":"<ul> <li>Helps to share resources between accounts</li> </ul>"},{"location":"cloud/aws/cert-saa/#system-manager-parameter-store","title":"System Manager Parameter Store","text":"<ul> <li>Component of AWS System Manager (SSM)</li> <li>Secure serverless storage for configuration and secrets</li> <li>Values can be stored encrypted or plaintext</li> <li>Store parameters in hierarchies</li> <li>Track versions</li> <li>Can set TTL</li> </ul>"},{"location":"cloud/aws/cert-saa/#secrets-manager","title":"Secrets Manager","text":"<ul> <li>Similar to Parameter Store</li> <li>Charge per secret store and 10000 API calls</li> <li>Automatically rotate secrets</li> <li>Apply the new key/pass in RDS for you</li> <li>Generate random secrets</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-shield","title":"AWS Shield","text":"<ul> <li>Protects against DDoS attacks</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-shield-standard","title":"AWS Shield Standard","text":"<ul> <li>Automatically enabled for all customers at no cost</li> <li>Protects against common layer 3 and 4 attacks (SYN/UDP floods and reflection attacks)</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-shield-advanced","title":"AWS Shield Advanced","text":"<ul> <li>3000$ per month</li> <li>Enhanced protection for EC2, ELB, CloudFront, Global Accelerator, Route 53</li> <li>Support and access to the DDoS Response Team (DTR)</li> <li>DDoS cost protection</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-waf","title":"AWS WAF","text":"<ul> <li>Monitor ClodFront, ALB or API Gateway</li> <li>Can allow request except the ones you specify</li> <li>Can block all request except the ones you specify</li> <li>Count the request that match the properties you specified</li> </ul>"},{"location":"cloud/aws/cert-saa/#extra-protection-conditions","title":"Extra protection (conditions)","text":"<ul> <li>IP</li> <li>Country</li> <li>Values in headers</li> <li>Strings (exact or regex) in requests</li> <li>Length of requests</li> <li>Presence of SQL code</li> <li>Presence of script</li> <li>Cross-site scripting (XSS)</li> <li>Rate-based</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-network-firewall","title":"AWS Network Firewall","text":"<p>AWS Network Firewall is a stateful, managed, network firewall, and intrusion detection and prevention service for your  virtual private cloud (VPC). With Network Firewall, you can filter traffic at the perimeter of your VPC. This includes  traffic going to and coming from an internet gateway, NAT gateway, or over VPN or AWS Direct Connect. Network Firewall  uses Suricata \u2014 an open-source intrusion prevention system (IPS) for stateful inspection.</p>"},{"location":"cloud/aws/cert-saa/#management-tools","title":"Management tools","text":""},{"location":"cloud/aws/cert-saa/#cloudwatch","title":"CloudWatch","text":"<ul> <li>CloudWatch is a monitoring tool for performance</li> <li>On EC2 will monitor events every 5min by default</li> <li>You can have 1min monitoring intervals</li> <li>You can create CloudWatch alarms which trigger notifications</li> <li>You can create dashboards</li> <li>You can create alarms</li> <li>You can create events</li> <li>You can aggregate logs</li> <li>You can collect additional logs installing unified CloudWatch Logs agent</li> </ul>"},{"location":"cloud/aws/cert-saa/#host-level-metrics","title":"Host Level Metrics","text":"<ul> <li>CPU</li> <li>Network</li> <li>Disk</li> <li>Status Check</li> </ul>"},{"location":"cloud/aws/cert-saa/#auto-scaling","title":"Auto Scaling","text":"<ul> <li>There is a vCPU-based On-Demand Instance limit per Region</li> <li>To change AMI create a new launch configuration</li> </ul>"},{"location":"cloud/aws/cert-saa/#components_2","title":"Components","text":"<ul> <li>Groups</li> <li>Configuration templates</li> <li>Scaling Options</li> </ul>"},{"location":"cloud/aws/cert-saa/#attach-ec2-to-existing-auto-scaling-group","title":"Attach EC2 to existing Auto Scaling group","text":"<ul> <li>The instance is in the running state</li> <li>The AMI used to launch the instance must still exist</li> <li>The instance is not a member of another Auto Scaling group</li> <li>The instance is launched into one of the AZ defined in your Auto Scaling group</li> <li>If the Auto Scaling group has an attached load balancer, the instance and the load balancer must both be in EC2-Classic or the same VPC</li> <li>If the Auto Scaling group has an attached target group, the instance and the load balancer must both be in the same VPC</li> </ul>"},{"location":"cloud/aws/cert-saa/#termination-policy","title":"Termination policy","text":"<ol> <li>If there are instances in multiple AZ, choose the AZ with the most instances and at least one instance that is not protected from scale in</li> <li>If there is more than one AZ with this number of instances, choose the AZ with the instances that use the oldest launch configuration</li> <li>Determine which unprotected instances in the selected AZ use the oldest launch configuration. If there is one such instance, terminate it.</li> <li>If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) If there is one such instance, terminate it.</li> <li>If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random. </li> </ol>"},{"location":"cloud/aws/cert-saa/#scaling-options","title":"Scaling Options","text":""},{"location":"cloud/aws/cert-saa/#manual-scaling","title":"Manual scaling","text":"<ul> <li>Requires manual interaction to increase and decrease number of instances</li> </ul>"},{"location":"cloud/aws/cert-saa/#dynamic-scaling","title":"Dynamic scaling","text":"<ul> <li>Target tracking scaling: Increase or decrease the current capacity of the group based on a target value for a specific metric</li> <li>Step scaling: Increase or decrease the current capacity of the group based on a set of scaling adjustments, known as step adjustments, that vary based on the size of the alarm breach</li> <li>Simple scaling: Increase or decrease the current capacity of the group based on a single scaling adjustment</li> <li>Scheduled scaling: for predictable traffic patterns</li> <li>Suspend and resume scaling: to temporary pause scaling activities</li> </ul>"},{"location":"cloud/aws/cert-saa/#predictive-scaling","title":"Predictive scaling","text":"<ul> <li>For recurring load patterns</li> </ul>"},{"location":"cloud/aws/cert-saa/#scheduled-scaling","title":"Scheduled scaling","text":"<ul> <li>Scheduled</li> </ul>"},{"location":"cloud/aws/cert-saa/#cloudformation","title":"CloudFormation","text":"<ul> <li>Is a way of completely scripting your cloud environment</li> <li>Has preconfigured templates (Quick Start) and you can create new ones</li> <li>You can associate the <code>CreationPolicy</code> attribute with a resource to prevent its status from reaching create complete until AWS CloudFormation receives a specified number of success signals or the timeout period is exceeded</li> <li>To signal a resource, you can use the <code>cfn-signal</code> helper script or SignalResource API, AWS CloudFormation publishes valid signals to the stack events so that you track the number of signals sent</li> </ul>"},{"location":"cloud/aws/cert-saa/#cloudtrail","title":"CloudTrail","text":"<ul> <li>Records AWS Management Console actions and API calls (auditing)</li> <li>You can identify which users and accounts called AWS, the IPs and when the calls occurred</li> <li>By default generated logs are encrypted using S3 server-side encryption (SSE)</li> <li>Enable the CloudTrail Log File Validation feature on all trails to identify whether a log file has been tampered</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-config","title":"AWS Config","text":"<ul> <li>Service that enables you to assess, audit, and evaluate the configurations of your AWS resources</li> <li>Continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations</li> <li>Can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines</li> <li>This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting</li> </ul>"},{"location":"cloud/aws/cert-saa/#opsworks","title":"OpsWorks","text":"<ul> <li>Automate operations with Chef and Puppet</li> <li>Configuration management model based on concepts such as stacks and layers that supports multiple versions of Chef</li> </ul>"},{"location":"cloud/aws/cert-saa/#codedeploy","title":"CodeDeploy","text":"<ul> <li>Automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-trusted-advisor","title":"AWS Trusted Advisor","text":"<ul> <li>Can help you optimize your AWS environment by giving recommendations to reduce cost, increase performance, and improve security</li> </ul>"},{"location":"cloud/aws/cert-saa/#analytics-services","title":"Analytics services","text":""},{"location":"cloud/aws/cert-saa/#athena","title":"Athena","text":"<ul> <li>Interactive query service</li> <li>Allows you to query data located in S3 using standard SQL</li> <li>Serverless</li> <li>Commonly used to analyse log data stored in S3</li> <li>Not supports XML data format</li> <li>Supports JSON, Apache Parquet, Apache ORC</li> </ul>"},{"location":"cloud/aws/cert-saa/#emr-elastic-map-reduce","title":"EMR (Elastic map-reduce)","text":"<ul> <li>Reduce cost and is faster than in-promise</li> <li>Is a cluster</li> <li>Log data is stored in master node</li> <li>You can configure the cluster to periodically archive the logs in the S3 (5 min intervals) but you need to enable when create the cluster</li> <li>Can be used to export data from DynamoDB and import data into DynamoDB</li> </ul>"},{"location":"cloud/aws/cert-saa/#nodes","title":"Nodes","text":"<ul> <li>Master node: every cluster has one, tracks the status of the tasks</li> <li>Core node: run tasks and stores data in the HDFS (Hadoop Distributed File System)</li> <li>Task node: only run tasks and no store data in the HDFS, is optional</li> </ul>"},{"location":"cloud/aws/cert-saa/#kinesis","title":"Kinesis","text":"<ul> <li>Makes easy to load and analyse streaming data (real-time)</li> </ul>"},{"location":"cloud/aws/cert-saa/#kinesis-data-streams","title":"Kinesis Data Streams","text":"<ul> <li>Stores data from 24 h to 7 days</li> <li>Data is stored in individual shards</li> <li>Data can be consumed by lambda functions</li> <li>Shards use DynamoBD, if it does not have enough capacity to store the lease data then increase the write capacity assigned to the shard table</li> </ul>"},{"location":"cloud/aws/cert-saa/#kinesis-data-firehose","title":"Kinesis Data Firehose","text":"<ul> <li>Key components: delivery streams, records of data and destinations</li> <li>Without data persistence, data modified on the fly with lambda functions</li> <li>Results stored in S3, Redshift (through S3), DynamoDB or Elasticsearch</li> </ul>"},{"location":"cloud/aws/cert-saa/#kinesis-data-analytics","title":"Kinesis Data Analytics","text":"<ul> <li>To analyze data inside kinesis (streams and firehose)</li> </ul>"},{"location":"cloud/aws/cert-saa/#kinesis-video-streams","title":"Kinesis Video Streams","text":"<ul> <li>For video</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-glue","title":"AWS Glue","text":"<ul> <li>Is a fully managed ETL (extract, transform, and load) AWS service</li> <li>You can use its crawlers to automatically infer database and table schema from your data S3 and store the associated metadata in the AWS Glue Data Catalog</li> <li>Athena uses the AWS Glue Data Catalog to store and retrieve table metadata   </li> </ul>"},{"location":"cloud/aws/cert-saa/#billing-and-cost-management","title":"Billing and cost management","text":""},{"location":"cloud/aws/cert-saa/#aws-budgets","title":"AWS Budgets","text":"<ul> <li>It gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount</li> <li>we can be proactive about attending to cost overruns before they become a major budget issue at the end of the month or quarter</li> <li>Budget alerts can be sent via email and/or Amazon Simple Notification Service (SNS) topic</li> </ul>"},{"location":"cloud/aws/cert-saa/#application-services","title":"Application services","text":""},{"location":"cloud/aws/cert-saa/#sns","title":"SNS","text":"<ul> <li>Service to send notifications from the cloud</li> <li>You can have push notifications (Apple, Google, Fire OS, Windows, Android)</li> <li>You can have notifications by email or SMS, also SQS and HTTP endpoint</li> <li>Allows grouping multiple recipients using topics</li> <li>All messages published are stored redundantly across multiple AZ</li> <li>Pay-as-you-go</li> <li>You can use SNS message filtering to assign a filter policy to the topic subscription</li> </ul>"},{"location":"cloud/aws/cert-saa/#sqs","title":"SQS","text":"<ul> <li>You can decouple the components of an application</li> <li>Is a fail-safe queue</li> <li>Messages can contain up to 256 KB of text, if bigger then they are stored in S3</li> <li>Any component can retrieve the messages using the SQS API</li> <li>The queue acts as a buffer</li> <li>Pull-based</li> <li>Messages can be kept from 1 min to 14 days (default is 4 days)</li> <li>Visibility timeout is the time when the message is not visible and waits to be processed confirmed or returned to be visible</li> <li>Messages in the SQS queue will continue to exist even after being processed, until you delete that message</li> <li>Visibility timeout is 12 h</li> <li>Long polling waits until a message is available, or it time-outs (ReceiveMessageWaitTimeSeconds)</li> <li>The default (minimum) delay for a queue is 0 seconds, the maximum is 15 minutes</li> <li>To set delay seconds on individual messages use message timers to allow Amazon SQS to use the message timer's DelaySeconds value instead of the delay queue's DelaySeconds value</li> <li>Backlog per instance metric: number of messages in queue / fleet capacity</li> </ul>"},{"location":"cloud/aws/cert-saa/#standard-queues","title":"Standard queues","text":"<ul> <li>Nearly-unlimited number of transactions per second</li> <li>Guarantee that a message is delivered at least once</li> <li>Might deliver out of order, but provide best-effort ordering</li> </ul>"},{"location":"cloud/aws/cert-saa/#fifo-queues","title":"FIFO queues","text":"<ul> <li>Type FIFO</li> <li>Message delivered once and keeps order</li> <li>Support message groups that allow multiple ordered message groups with a single queue</li> <li>Limited to 300 transactions per second (TPS)</li> <li>Visibility timeout up to 12 h</li> </ul>"},{"location":"cloud/aws/cert-saa/#amazon-mq","title":"Amazon MQ","text":"<ul> <li>Supports industry-standard APIs and protocols so you can switch from any standards-based message broker to Amazon MQ without rewriting the messaging code in your applications</li> </ul>"},{"location":"cloud/aws/cert-saa/#swf-simple-workflow-service","title":"SWF (Simple Workflow Service)","text":"<ul> <li>Makes it easy to coordinate work across distributed application components</li> <li>Workflow execution can last up to 1 year</li> <li>Presents a task oriented API</li> <li>Keeps track of all the tasks and events in an application</li> <li>All workflows inside a domain can interact</li> </ul>"},{"location":"cloud/aws/cert-saa/#swf-actors","title":"SWF Actors","text":"<ul> <li>Workflow starter: An application that initiates the flow</li> <li>Decider: Control the flow</li> <li>Activity workers: Carry out the activity tasks</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-step-functions","title":"AWS Step Functions","text":"<ul> <li>Provides serverless orchestration for modern applications</li> </ul>"},{"location":"cloud/aws/cert-saa/#aws-appsync","title":"AWS AppSync","text":"<ul> <li>Develop GraphQL APIs with multiple sources like DynamoDB or lambda</li> </ul>"},{"location":"cloud/aws/cert-saa/#event-processing-patterns","title":"Event processing patterns","text":""},{"location":"cloud/aws/cert-saa/#pubsub-messaging","title":"Pub/Sub Messaging","text":"<ul> <li>SNS Topic</li> </ul>"},{"location":"cloud/aws/cert-saa/#dead-letter-queue-dlq","title":"Dead-Letter Queue (DLQ)","text":"<ul> <li>SNS</li> <li>SQS</li> <li>Lambda</li> </ul>"},{"location":"cloud/aws/cert-saa/#fanout-pattern","title":"Fanout pattern","text":"<ul> <li>Publisher send the message to SNS Topic and the all SQS subscribe to the topic</li> </ul>"},{"location":"cloud/aws/cert-saa/#s3-event-notifications","title":"S3 Event Notifications","text":"<ul> <li>You can filter by name of file and action</li> <li>Can be consumed by SNS Topic, SQS Queue or Lambda Function</li> </ul>"},{"location":"cloud/aws/cert-saa/#desktop-and-app-streaming","title":"Desktop and app streaming","text":""},{"location":"cloud/aws/cert-saa/#amazon-workspace","title":"Amazon Workspace","text":"<ul> <li>To create virtual desktops in your VPC</li> </ul>"},{"location":"cloud/aws/cert-saa/#extra-documentation","title":"Extra documentation","text":"<ul> <li>AWS Overview Withepaper</li> <li>AWS Well-Architected Framework Withepaper</li> <li>AWS Disaster Recovery Withepaper</li> <li>AWS Withepapers Repository</li> <li>Well-Architected Labs</li> </ul>"},{"location":"cloud/aws/cert-sap/","title":"AWS Certified Solutions Architect Professional","text":""},{"location":"cloud/aws/cert-sap/#security","title":"Security","text":""},{"location":"cloud/aws/cert-sap/#iam","title":"IAM","text":"<ul> <li>Managers users, groups (of users) and roles (can be assumed by someone of something) where you attach policies (permissions to resources).</li> <li>Groups cannot belong to groups.</li> <li>Users can belong to many groups.</li> <li>IdP can assume roles too.</li> <li>Access keys allows the user to use the cli.</li> <li>Most IAM policies are stored in AWS as JSON documents. They have several policy elements, including a Version, Effect, Action, and Resource (also Condition).</li> <li>IAM policies have a 2 kb size limit for users, 5 kb for groups, and a 10 kb for roles.</li> <li>IAM lets you create roles, and doing so allows you to define a set of permissions and then let authenticated users assume them. This feature increases your security posture by granting temporary access to the resources you define.</li> <li>You can use IAM to grant your employees and applications access to the AWS Management Console and to AWS service APIs using your existing identity systems.</li> <li>The access advisor section shows the latest time that a user, group, role or policy was used, this helps to detect unused elements.</li> </ul>"},{"location":"cloud/aws/cert-sap/#limits-on-policy-size","title":"Limits on policy size","text":"<ul> <li>User policy size cannot exceed 2,048 characters.</li> <li>Role policy size cannot exceed 10,240 characters.</li> <li>Group policy size cannot exceed 5,120 characters.</li> </ul>"},{"location":"cloud/aws/cert-sap/#policy-elements","title":"Policy elements","text":"<ul> <li>Effect: Allow or Deny (required)</li> <li>Action (required)</li> <li>NotAction: explicitly matches everything except the specified list of actions, recommended for deny statements</li> <li>Principal: you cannot use the Principal element in an IAM identity-based policy because, in that case, you are already attaching the policy directly to a principal (optional)</li> <li>NotPrincipal: recommended for deny statements</li> <li>Resource: ARN of the resource (can have wildcards) (required)</li> <li>NotResource: recommended for deny statements</li> <li>Condition: define complex conditions, multiple are interpreted as an AND operation (optional)</li> </ul>"},{"location":"cloud/aws/cert-sap/#identity-based-policies","title":"Identity-based policies","text":"<ul> <li>AWS Managed: They can be attached to multiple users, groups, and roles.</li> <li>Customer managed: Provides more precise control than AWS managed policies and can also be attached to multiple users, groups, and roles.</li> <li>Inline: Embedded directly into a single user, group, or role. not recommended to use.</li> </ul>"},{"location":"cloud/aws/cert-sap/#policy-types","title":"Policy types","text":""},{"location":"cloud/aws/cert-sap/#identity-based-grant","title":"Identity-based (Grant)","text":"<ul> <li>Also known as IAM policies, identity-based policies are managed and inline policies attached to IAM identities (users, groups to which users belong, or roles). Impacts IAM principal permissions.</li> </ul>"},{"location":"cloud/aws/cert-sap/#resource-based-grant","title":"Resource-based (Grant)","text":"<ul> <li>These are inline policies that are attached to AWS resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies.</li> <li>Resource-based policies grant permissions to the principal that is specified in the policy; hence, the principal policy element is required.</li> </ul>"},{"location":"cloud/aws/cert-sap/#permissions-boundaries-guardrails","title":"Permissions boundaries (Guardrails)","text":"<ul> <li>A permissions boundary sets the maximum permissions that an identity-based policy can grant to an IAM entity. The entity can perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. Resource-based policies that specify the user or role as the principal are not limited by the permissions boundary.</li> <li>Restricts permissions for the IAM entity attached to it.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-organizations-scp-guardrails","title":"AWS Organizations SCP (Guardrails)","text":"<ul> <li>AWS Organizations is a service for grouping and centrally managing AWS accounts.</li> <li>If you enable all features in an organization, then you can apply SCPs to any or all of your accounts.</li> <li>SCPs specify the maximum permissions for an account, or a group of accounts, called an organizational unit (OU).</li> <li>Restricts permissions for entities in an AWS account, including AWS account root users.</li> </ul>"},{"location":"cloud/aws/cert-sap/#acls-grant","title":"ACLs (Grant)","text":"<ul> <li>ACLs are supported by Amazon S3 buckets and objects.</li> <li>They are similar to resource-based policies although they are the only policy type that does not use the JSON policy document structure.</li> <li>ACLs are cross-account permissions policies that grant permissions to the specified principal.</li> <li>ACLs cannot grant permissions to entities within the same account.</li> </ul>"},{"location":"cloud/aws/cert-sap/#sessions-policies-guardrails","title":"Sessions policies (Guardrails)","text":"<ul> <li>A session policy is an inline permissions policy that users pass in the session when they assume the role.</li> <li>The permissions for a session are the intersection of the identity-based policies for the IAM entity (user or role) used to create the session and the session policies.</li> <li>Restricts permissions for assumed roles and federated users.</li> </ul>"},{"location":"cloud/aws/cert-sap/#evaluation-logic","title":"Evaluation logic","text":"<ul> <li>When an IAM entity (user or role) requests access to a resource in the same account, AWS evaluates all the permissions granted by the identity-based and resource-based policies. The resulting permission set is the total accumulated permissions from the two policy types. AWS evaluates all policies associated with a resource and builds a compilation of access rights into a key-value file called the request context.</li> <li>If an action is allowed by an identity-based policy, a resource-based policy, or both, AWS allows the action.</li> <li>An explicit deny in either of these policies overrides the allow.</li> </ul>"},{"location":"cloud/aws/cert-sap/#attributes-and-tagging","title":"Attributes and Tagging","text":"<ul> <li>Attribute-based access control (ABAC) is an authorization strategy that defines permissions based on attributes.</li> <li>In AWS, these attributes are called tags.</li> <li>Tags can be attached to IAM principals (users or roles) and to AWS resources.</li> <li>Using both RBAC and ABAC is the way to go. For example, if multiple users share the same job function but different cost centers, and you want to grant access only to resources belonging to each individual\u2019s cost center. With ABAC, only one role is required instead of multiple roles when using only RBAC.</li> <li>ABAC offers scalability, is more manageable and defines granular permissions.</li> </ul>"},{"location":"cloud/aws/cert-sap/#iam-condition-keys","title":"IAM Condition Keys","text":"<ul> <li><code>iam:AWSServiceName</code> : control access for a specific service role. Use case: roles need to be attached to only specific services.</li> <li>`iam:OrganizationsPolicyId : provides the IAM entity access to specific SCPs. Use case: users are also required to be authenticated via a service control policy.</li> <li><code>iam:PermissionsBoundary</code> : checks that the specified policy is attached as a permissions boundary on the IAM principal resource. Use case: guardrails need to be configured for a policy attached to an IAM group.</li> <li><code>iam:PolicyARN</code> : control how users can apply AWS managed and customer managed policies. Use case: users can attach specific customer managed policies to only certain IAM groups and roles.</li> <li><code>iam:ResourceTag</code> : checks that the tag attached to the identity resource, either a user or role, matches the specified key name and value provided. Use case: permissions should be assigned to only a certain IAM role.</li> </ul>"},{"location":"cloud/aws/cert-sap/#condition-keys-for-passing-roles-iampassrole","title":"Condition keys for passing roles (iam:passRole)","text":"<ul> <li><code>iam:PassedToService</code> : specifies the service principal of the service to which a role can be passed. Use case: service roles should be created and passed to only specific AWS services.</li> <li><code>iam:AssociatedResourceArn</code> : specifies the ARN of the resource to which this role will be associated at the destination service. Use case: users can pass a role that is associated with only a certain resource.</li> </ul>"},{"location":"cloud/aws/cert-sap/#global-condition-key","title":"Global Condition Key","text":"<ul> <li><code>aws:CalledVia</code> : List of services that are calling in the chain of execution (ex: User calls CF, CF calls DynamoDB and DynamoDB calls KMS)</li> <li><code>aws:CalledViaFirst</code></li> <li><code>aws:CalledViaLast</code></li> <li><code>aws:ViaAWSService</code> : boolean</li> <li><code>aws:CurrentTime</code> : YYYY-MM-DDThh:mm:ssZ</li> <li><code>aws:EpochTime</code> : unix timestamp</li> <li><code>aws:TokenIssueTime</code></li> <li><code>aws:MultiFactorAge</code> : age in seconds</li> <li><code>aws:MultiFactorAuthPresent</code> : boolean</li> <li><code>aws:SecureTransport</code> : if the request is sent using SSL</li> <li><code>aws:SourceAccount</code></li> <li><code>aws:SourceArn</code></li> <li><code>aws:SourceIp</code> : IP range or single value</li> <li><code>aws:SourceVpc</code></li> <li><code>aws:SourceVpce</code> : for vpc endpoint</li> <li><code>aws:VpcSourceIp</code> : for vpc endpoint with the IP range or single value</li> <li><code>aws:PrincipalAccount</code></li> <li><code>aws:PrincipalArn</code></li> <li><code>aws:PrincipalOrgId</code></li> <li><code>aws:PrincipalOrgPaths</code></li> <li><code>aws:PrincipalType</code> : to restrict access to only certain principal type (Account, User, AssumedRole, AWS service, FederatedUser)</li> <li><code>aws:PrincipalTag</code> : <code>aws.PrincipalTag/&lt;key&gt;</code> : <code>&lt;value&gt;</code></li> <li><code>aws:RequestTag</code> : <code>aws.RequestTag/&lt;key&gt;</code> : <code>&lt;value&gt;</code></li> <li><code>aws:ResourceTag</code> : <code>ec2.ResourceTag/&lt;key&gt;</code> : <code>&lt;value&gt;</code></li> <li><code>aws:TagsKeys</code> : to define which tag keys are allowed to be used when tagging a resource</li> <li><code>aws:Referer</code> : when calling API operations using a web browser</li> <li><code>aws:RequestedRegion</code> : limit access to regions</li> <li><code>aws:UserAgent</code></li> <li><code>aws:userid</code> : restrict access to specific IAM users or IAM roles</li> <li> <p><code>aws:username</code> : restrict access to specific IAM users</p> </li> <li> <p>If a condition key is missing from a request context, the policy can fail the evaluation. If you use condition keys that are available only in some circumstances, you can use the <code>IfExists</code> versions of the condition operators.</p> </li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-sts","title":"AWS STS","text":"<ul> <li>You do not have to rotate them or explicitly revoke them when they're no longer needed.</li> <li>You can manage your users in an external system outside AWS and grant them access to perform AWS tasks and access your AWS resources. IAM supports two types of identity federation: corporate identity federation and web identity federation.</li> <li>Using roles and cross-account access, you can define user identities in one account and use those identities to access AWS resources in other accounts that belong to your organization.</li> <li>If you run applications on Amazon EC2 instances and those applications need access to AWS resources, you can provide temporary security credentials to your instances when you launch them.</li> <li>When a user or application requires temporary security credentials to access AWS resources, they make the AssumeRole API request. These temporary credentials consist of an access key ID, a secret access key, and a security token. Each time a role is assumed and a set of temporary security credentials is generated, an IAM role session is created.</li> <li>By default, all users assuming the same role get the same permissions for their role session. To create distinctive role session permissions or to further restrict overall permissions, users or systems can set a session policy when assuming a role. A session policy is an inline permissions policy that users pass in the session when they assume the role. You can pass the policy yourself, or you can configure your broker to insert the policy when your identities federate in to AWS.</li> <li>Each IAM role session is uniquely identified by a role session name. AWS STS provides a condition key called sts:RoleSessionName that controls how IAM principals and applications name their role sessions when they assume an IAM role. Administrators can rely on the role session name to track user actions when viewing AWS CloudTrail logs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#assumerole-request-optional-parameters","title":"AssumeRole request optional parameters","text":"<ul> <li><code>DurationSeconds</code>: if not defined, by default is 1h, you can provide a value between 900s (15m) up to 12h.</li> <li>Policy: this parameter includes IAM policy that you want to use as an inline session policy. The resulting session's permissions are the intersection of the role's identity-based policy and the session policies.</li> <li><code>PolicyArns.member.N</code>: the ARNs of the IAM managed policies that you want to use as managed session policies. The policies must exist in the same account as the role. You can provide up to 10 managed policy ARNs.</li> <li><code>Tags.member.N</code>: the session tags that you want to pass with the role. Each session tag consists of a key name and an associated value.</li> <li><code>SerialNumber</code> and <code>TokenCode</code>: you can include MFA information when you call AssumeRole with these parameters. This is useful for cross-account scenarios to ensure that the user who assumes the role has been authenticated with an AWS MFA device. In that scenario, the trust policy of the role being assumed includes a condition that tests for MFA.</li> </ul>"},{"location":"cloud/aws/cert-sap/#assumerole-response-sections","title":"AssumeRole response sections","text":"<ul> <li><code>AssumeRoleUser</code>: contains the ARN and ID of the role.</li> <li><code>Credentials</code>: contains the access key, secret access key and session token.</li> <li><code>PackedPolicySize</code>: percentage that indicates the packed size of the session policies and tags combined, the request fails if is greater than 100, which means the policies and tags exceeded the allowed space.</li> </ul>"},{"location":"cloud/aws/cert-sap/#role-session-naming","title":"Role session naming","text":"<ul> <li>AWS Service - EC2: the instance ID.</li> <li>AWS Service - Lambda: function name.</li> <li>AWS Service - Cognito Identity Pool: Cognito identity credentials.</li> <li>SAML-Based: when you use the <code>AssumeRolewithSAML</code> API it uses the value provided by the identity provider.</li> <li>User defined: when assuming an IAM role with APIs such as AssumeRole or <code>AssumeRoleWithWebIdentity</code>, the role session name is a required input parameter.</li> </ul>"},{"location":"cloud/aws/cert-sap/#session-tagging","title":"Session Tagging","text":"<ul> <li>Session tags are attributes passed in an IAM role session when you assume a role or federate a user using the AWS CLI or AWS API.</li> <li>To be able to add session tags, you must have the <code>sts:TagSession</code> action allowed in your IAM policy.</li> <li>Session tags are principal tags that you specify while requesting a session.</li> <li>New session tags override existing assumed role or federated user tags with the same tag key, regardless of case.</li> <li>You cannot pass session tags using the AWS Management Console.</li> <li>You can use session tags to control access to resources or to control the tags that can be passed into a subsequent session.</li> <li>You can pass a maximum of 50 session tags.</li> <li>Session tags support role chaining. Role chaining occurs when you use a role to assume a second role through the AWS CLI or API. You can assume one role and then use the temporary credentials to assume another role and continue from session to session. By default, tags are not passed to subsequent role sessions. However, you can set session tags as transitive. This ensures that those session tags pass to subsequent sessions in a role chain.</li> <li>Role chaining is especially useful when you want to impose guardrails against yourself or an administrator in order to prevent something accidental.</li> <li>If your company uses a SAML-based IdP to manage corporate user identities, you can use SAML attributes for access control in AWS. Attributes can include cost center identifiers, user email addresses, department classifications, and project assignments. When you pass these attributes as session tags, you can then control access to AWS based on these session tags.</li> </ul>"},{"location":"cloud/aws/cert-sap/#saml-based-federation","title":"SAML-Based Federation","text":"<ul> <li>Before your application can call <code>AssumeRoleWithSAML</code>, you must configure your SAML IdP to issue the claims that AWS requires. Additionally, you must use IAM to create a SAML provider entity in your AWS account that represents your identity provider. You must also create an IAM role that specifies this SAML provider in its trust policy.</li> <li>User attributes can be passed as session tags using standards-based SAML.</li> </ul>"},{"location":"cloud/aws/cert-sap/#assumerolewithsaml-request-parameters","title":"AssumeRoleWithSAML request parameters","text":"<ul> <li><code>RoleArn</code></li> <li><code>PrincipalArn</code>: ARN of the configured SAML provider</li> <li><code>SamlAssertion</code>: b64 SAML authentication response</li> <li><code>DurationSeconds</code> (optional): from 15min to 12h, default is 1h. If the SAML <code>SessionNotOnOrAfter</code> attribute is shorter, it has preference over this.</li> <li><code>Policy</code> (optional): this parameter includes IAM policy that you want to use as an inline session policy. The resulting session's permissions are the intersection of the role's identity-based policy and the session policies.</li> <li><code>PolicyArns.member.N</code> (optional): the ARNs of the IAM managed policies that you want to use as managed session policies. The policies must exist in the same account as the role. You can provide up to 10 managed policy ARNs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#assumerolewithsaml-response-sections","title":"AssumeRoleWithSAML response sections","text":"<ul> <li><code>Issuer</code>: URL that uniquely identifies your SAML identity provider, SAML assertions sent to the service provider must match this value.</li> <li><code>AssumeRoleUser</code>: contains the ARN and ID of the role.</li> <li><code>Credentials</code>: contains the access key, secret access key and session token.</li> <li><code>Audience</code>: the service provider, is typically a URL.</li> <li><code>SubjectType</code>: provides information on the format of the name identifier of the Subject field. An identifier intended for a single session only is called a transient identifier.</li> <li><code>PackedPolicySize</code>: percentage that indicates the packed size of the session policies and tags combined, the request fails if is greater than 100, which means the policies and tags exceeded the allowed space.</li> <li><code>NameQualifier</code>: hash value based on the concatenation of the issuer response value, aws account id and the name of the saml provider in IAM. The combination of this value and the subject can be used to uniquely identify a federated user.</li> <li><code>Subject</code></li> </ul>"},{"location":"cloud/aws/cert-sap/#web-base-federation","title":"Web-Base Federation","text":"<ul> <li>You must have an identity token from a supported identity provider and create a role that the application can assume.</li> <li>The role that your application assumes must trust the identity provider that is associated with the identity token. In other words, the identity provider must be specified in the role's trust policy.</li> <li>Calling <code>AssumeRoleWithWebIdentity</code> does not require the use of AWS security credentials. Therefore, you can distribute an application (for example, on mobile devices) that requests temporary security credentials without including long-term AWS credentials in the application.</li> </ul>"},{"location":"cloud/aws/cert-sap/#assumerolewithwebidentity-request-parameters","title":"AssumeRoleWithWebIdentity request parameters","text":"<ul> <li><code>RoleArn</code></li> <li><code>RoleSessionName</code>: identifies the session, typically you pass the name or identifier associated with the user.</li> <li><code>WebIdentityToken</code>: the token provided by the oauth2 provider (access token) or the OpenID Connect ID token.</li> <li><code>ProviderId</code>: identifies the web identity provider and is only required for oauth2 access tokens (currently only www.amazon.com and graph.facebook.com are the only supported for oauth2).</li> <li><code>DurationSeconds</code> (optional): from 15min to 12h, default is 1h.</li> <li><code>Policy</code> (optional): this parameter includes IAM policy that you want to use as an inline session policy. The resulting session's permissions are the intersection of the role's identity-based policy and the session policies.</li> <li><code>PolicyArns.member.N</code> (optional): the ARNs of the IAM managed policies that you want to use as managed session policies. The policies must exist in the same account as the role. You can provide up to 10 managed policy ARNs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#assumerolewithwebidentity-response-sections","title":"AssumeRoleWithWebIdentity response sections","text":"<ul> <li><code>Issuer</code>: URL that uniquely identifies your SAML identity provider, SAML assertions sent to the service provider must match this value.</li> <li><code>AssumeRoleUser</code>: contains the ARN and ID of the role.</li> <li><code>Credentials</code>: contains the access key, secret access key and session token.</li> <li><code>Audience</code>: the service provider, is typically a URL.</li> <li><code>SubjectType</code>: provides information on the format of the name identifier of the Subject field. An identifier intended for a single session only is called a transient identifier.</li> <li><code>PackedPolicySize</code>: percentage that indicates the packed size of the session policies and tags combined, the request fails if is greater than 100, which means the policies and tags exceeded the allowed space.</li> <li><code>NameQualifier</code>: hash value based on the concatenation of the issuer response value, aws account id and the name of the saml provider in IAM. The combination of this value and the subject can be used to uniquely identify a federated user.</li> <li><code>Subject</code></li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-cognito-for-mobile-applications","title":"Amazon Cognito for mobile applications","text":"<ul> <li>The preferred way to use web identity federation for mobile applications.</li> <li>Supports sign-in with social identity providers, such as Apple, Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0. User sign-in can also be done directly via Amazon Cognito.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-iam-identity-center-successor-to-aws-single-sign-on","title":"AWS IAM Identity Center (successor to AWS Single Sign-On)","text":"<ul> <li>Built-in integrations with business cloud applications, such as Salesforce, Box, GitHub, and Office 365.</li> <li>Built-in directory for user and group management to serve as an IdP to authenticate users to IAM Identity Center enabled applications, the AWS Management Console, and SAML 2.0 compatible cloud-based applications.</li> <li>Integration with AWS services, such as AWS Organizations.</li> <li>AWS Access portal for users to sign in with their existing corporate credentials and access all of their assigned accounts and applications from one place.</li> <li>Ability to use AWS CLI v2 to access AWS resources via IAM Identity Center.</li> <li>An IAM Identity Center permission set is a collection of administrator-defined policies that IAM Identity Center uses to determine a user's effective permissions to access a given AWS account.</li> <li>Users who have multiple permission sets must choose one of the roles when they sign in to the AWS access portal.</li> <li>Permission sets are used for only AWS accounts. Permission sets are not used to manage access to cloud applications. Permission sets ultimately get created as IAM roles in a given AWS account, and trust policies allow users to assume the role through IAM Identity Center.</li> </ul>"},{"location":"cloud/aws/cert-sap/#iam-policy-simulator","title":"IAM policy simulator","text":"<ul> <li>You can test and troubleshoot identity-based policies, IAM permissions boundaries, AWS Organizations service control policies, and resource-based policies.</li> <li>You can allow console or API users to test policies that are attached to IAM users, groups, or roles in your AWS account. To do so, you must provide permission to retrieve those policies.</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases","title":"Use cases","text":"<ul> <li>Test policies that are attached to IAM users, groups, or roles in your AWS account.</li> <li>Test and troubleshoot the effect of permissions boundaries on IAM entities one permissions boundary at a time.</li> <li>Test policies that are attached to AWS resources.</li> <li>Test the impact of SCPs on your IAM policies and resource policies if your AWS account is a member of an organization in AWS Organizations.</li> <li>Test new policies that are not yet attached to a user, group, or role by typing or copying them into the simulator.</li> <li>Simulate real-world scenarios by providing context keys, such as an IP address or date, that are included in Condition elements in the policies being tested.</li> <li>Identify which specific statement in a policy results in allowing or denying access to a particular resource or action.</li> </ul>"},{"location":"cloud/aws/cert-sap/#iam-access-analyzer","title":"IAM Access Analyzer","text":"<ul> <li>It delivers comprehensive, detailed findings through the IAM, Amazon S3, and AWS Security Hub consoles and also through its APIs.</li> <li>Findings can also be exported as a report for auditing purposes.</li> <li>When you enable IAM Access Analyzer, you create an analyzer for your AWS account or your entire organization if it is using AWS Organizations. The organization or account you choose is known as the zone of trust for the analyzer. You can create only one analyzer per Region in an account.</li> <li>Each finding can be archived if the configuration is correct of you can perform actions to solve the problem navigating to the affected service.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-macie","title":"AWS Macie","text":"<ul> <li>Macie automates the discovery of sensitive data, such as personally identifiable information (PII), personal health information (PHI) and financial data, to provide you with a better understanding of the types of data in Amazon S3.</li> <li>You can configure a job to run only once, for on-demand analysis and assessment, or on a recurring basis for periodic analysis, assessment, and monitoring. You can also choose various options to control the breadth and depth of a job's analysis, such as the S3 buckets that you want to analyze, the sampling depth, and custom include and exclude criteria that derive from properties of S3 objects.</li> <li>You can also customize what Macie looks for to reflect your unique scenarios, intellectual property, or proprietary data, such as customer account numbers or internal data classifications.</li> <li>If you require an inventory of your S3 buckets, Macie automatically evaluates and monitors those buckets for security and access control. Within minutes, Macie can identify and report overly permissive or unencrypted buckets within your organization.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-guardduty","title":"Amazon GuardDuty","text":"<ul> <li>Amazon GuardDuty is a security monitoring service that analyzes and processes certain types of AWS logs, such as AWS CloudTrail data event logs for Amazon S3 and CloudTrail management event logs.</li> <li>It uses threat intelligence feeds, such as lists of malicious IP addresses and domains, and machine learning to identify unexpected and potentially unauthorized and malicious activity within your AWS environment.</li> </ul>"},{"location":"cloud/aws/cert-sap/#networking","title":"Networking","text":""},{"location":"cloud/aws/cert-sap/#ipv4","title":"IPv4","text":""},{"location":"cloud/aws/cert-sap/#public","title":"Public","text":"<ul> <li>Class A: 0.0.0.0 to 127.255.255.255 (2.1 billion)</li> <li>Class B: 128.0.0.0 to 191.255.255.255 (1 billion)</li> <li>Class C: 192.0.0.0 to 223.255.255.255 (2 million)</li> <li>Class D</li> <li>Class E</li> </ul>"},{"location":"cloud/aws/cert-sap/#private","title":"Private","text":"<ul> <li>Class A: 10.0.0.0 to 10.255.255.255</li> <li>Class B: 172.16.0.0 to 172.31.255.255 (16 ranges)</li> <li> <p>Class C: 192.168.0.0 to 192.168.255.255 (256 ranges)</p> </li> <li> <p>The default VPC is configured using a Class B range.</p> </li> </ul>"},{"location":"cloud/aws/cert-sap/#osi-model","title":"OSI model","text":"<ul> <li>1 - Physical: Cables.</li> <li>2 - Data link: Frames and MACs.</li> <li>3 - Network: IP.</li> <li>4 - Transport: Ports, segments, retransmission and flow control.</li> <li>5 - Session: Communication session between nodes.</li> <li>6 - Presentation: Data representation and encryption.</li> <li>7 - Application: Level 7 protocols (HTTP, SMTP, FTP, web-browsing, REST).</li> </ul>"},{"location":"cloud/aws/cert-sap/#vpc","title":"VPC","text":"<ul> <li>Every region have a default vpc (is open to internet).</li> <li>You choose your network size by using CIDR notation. In AWS, the smallest IP range you can have is /28, which provides 16 IP addresses. The largest IP range you can have is a /16, which provides 65,536 IP addresses.</li> <li>You define subnets inside a vpc in an AZ and in a IP range.</li> <li>A VPC can have connection to internet by internet gateway.</li> <li>When an internet gateway is attached to a subnet, it creates a public subnet by performing a type of Network Address Translation (NAT) called static NAT. The internet gateway will allocate a resource with a public IPv4 IP address. </li> <li>A VPC can connect to a on-premise data center using a vpn using a Virtual Private Gateway (Add in the private routing table the IP range from your datacenter to access the VGW).</li> <li>To maintain redundancy and fault tolerance, create at least two subnets configured in two Availability Zones.</li> <li>AWS reserves five IP addresses in each subnet. (0 - Network address, 1 - VPC local router, 2 - DNS server, 3 - for future use, 255 - broadcast)</li> <li>The default Amazon VPC IPv4 CIDR,172.31.0.0/16, is always the same, and is designed and configured the same too.</li> <li>The Amazon VPC IPv6 CIDR for your subnet range is /64, and the Amazon VPC CIDR range is /56.</li> <li>Tenancy: if dedicated forces all instances to be physically isolated.</li> <li>Design your Amazon VPC implementation based on your expansion requirements, looking ahead at least two years.</li> <li>Recommend to use a separate Amazon VPCs for development, production, and staging environments.</li> <li>Depending on your solution, your data-tier subnets are likely to need more IP addresses than your public subnets, but fewer than your application-tier subnets.</li> </ul>"},{"location":"cloud/aws/cert-sap/#routing","title":"Routing","text":"<ul> <li>When creating a VPC a main route table is created (you can not delete it).</li> <li>You can create a custom route table and associate to the desired subnets.</li> <li>A subnet can only be in one Availability Zone.</li> <li>A subnet can only have one route table associated with it at a time, but you can use one route table for many different subnets in your Amazon VPC.</li> <li>The only architectural difference between a public and private subnet is that a public subnet has a route to an internet gateway.</li> <li>By default, DNS is handled by Amazon VPC. It is possible, however, to use Amazon Route 53 to create your own DNS inside an Amazon VPC with private hosted zones.</li> <li>All traffic is unicast and Amazon VPCs do not require the Address Resolution Protocol (ARP).</li> <li>By default, all subnets in an Amazon VPC can access each other. You can use network network ACLs to restrict traffic into and out of your subnets.</li> <li>All traffic between two points in the same Amazon VPC is forwarded directly.</li> <li>You attach the IGW to the VPC, is an one to one relation.</li> <li>In the public routing table add a record from 0.0.0.0/0 to the IGW.</li> <li>In the private routing table add a record from 0.0.0.0/0 to the NAT-ID.</li> <li>You can't resize a CIDR block after it's been created, so any IP addresses you reserve in that CIDR block are potentially stuck there until you delete that subnet.</li> <li>The CIDR blocks can overlap. When the CIDR blocks for route table entries overlap, the more specific (smaller range) CIDR block takes priority.</li> </ul>"},{"location":"cloud/aws/cert-sap/#internet-gateway-ingress-routing","title":"Internet gateway: Ingress routing","text":"<ul> <li>Associates a route table with an internet gateway (instead of a subnet).</li> <li>Redirects incoming and outgoing VPC traffic through virtual appliances (for example a firewall for incoming traffic).</li> <li>Segments VPC traffic (for outgoing traffic).</li> </ul>"},{"location":"cloud/aws/cert-sap/#virtual-private-gateway","title":"Virtual private gateway","text":""},{"location":"cloud/aws/cert-sap/#site-to-site-vpn","title":"Site-to-site VPN","text":"<ul> <li>One virtual private gateway per VPC.</li> <li>BGP or static routes.</li> <li>Redundant IPsec tunnels.</li> <li>Redundant routes across two AZ.</li> <li>In VPC route table the destination is the CIDR of the on-premises subnet and the target the virtual private gateway VGW.</li> <li>Up to 10 VPN connections (different on-premises networks) can terminate to the same virtual private gateway (each adds an entry to the route table).</li> </ul>"},{"location":"cloud/aws/cert-sap/#direct-connect","title":"Direct Connect","text":"<ul> <li>One BGP per VPC.</li> <li>Restricted to one region.</li> <li>Maximum 50 VIFs (virtual interfaces) per Direct connection.</li> <li>If you want to communicate with a public aws services (like S3 or DynamoDB) you create a public VIF</li> <li>if you want to communicate with a resource inside a VPC you create a private VIF that terminates at the virtual private gateway of the VPC.</li> </ul>"},{"location":"cloud/aws/cert-sap/#ingress-routing","title":"Ingress routing","text":"<ul> <li>Associates a route table with a virtual private gateway (instead of a subnet).</li> <li>Redirects incoming and outgoing VPC traffic through virtual appliances (for example a firewall for incoming traffic).</li> <li>Segments VPC traffic (for outgoing traffic).</li> </ul>"},{"location":"cloud/aws/cert-sap/#customer-gateway-site-to-site-vpn","title":"Customer gateway: Site-to-site VPN","text":"<ul> <li>Located in on-premises data center.</li> <li>Physical or software appliance that the customers must configure.</li> <li>The same customer gateways can be reused for multiple site-to-site VPN connections, but you must consider high availability.</li> <li>Redundant IPSec tunnels (on the other end of the connections is the virtual private gateway).</li> <li>Each tunnel contains an Internet Key Exchange (IKE) security association, an IPSec security association, and a BGP peering.</li> </ul>"},{"location":"cloud/aws/cert-sap/#direct-connect-gateway","title":"Direct connect gateway","text":"<ul> <li>Connect (associate) up to 10 virtual private gateways globally and cross account.</li> <li>It is a global resource, not a regional one.</li> <li>Has one BGP peering per Direct connect gateway per Direct connect connection.</li> <li>Can be associates with a virtual private gateway or a transit gateway.</li> </ul>"},{"location":"cloud/aws/cert-sap/#local-gateways","title":"Local Gateways","text":"<ul> <li>One local gateway per Outpost.</li> <li>To allow the VPC in the Outpost on-premise to communicate to other networks on-premise outside the Outpost.</li> <li>Performs NAT for instances that have been assigned addresses from your customer-owner IP pool.</li> </ul>"},{"location":"cloud/aws/cert-sap/#components","title":"Components","text":"<ul> <li>Route tables: AWS creates a local gateway for your Outpost and local gateway route table as part of the installation process. You can only associate the local gateway route table with subnets that reside in the Outpost.</li> <li>Virtual interfaces: AWS crates one VIF for each Link Aggregation Group (LAG), and then associates the VIF with the default local gateway route table. The local gateway route table has a default route to the two VIFs for local network connectivity.</li> </ul>"},{"location":"cloud/aws/cert-sap/#nat-gateway","title":"NAT gateway","text":"<ul> <li>To allow access to internet from private subnets but prevents the internet from initiation a connection with those instances, it is deployed in a public subnet.</li> <li>Performs NAT for instances with private IPs.</li> <li>Is highly available (redundancy inside an AZ).</li> <li>Is regional resource, IPv4 only.</li> <li>You cannot route traffic to a NAT gateway through a VPC peering connection, site-to-site VPN connection or Direct Connect.</li> </ul>"},{"location":"cloud/aws/cert-sap/#bastion","title":"Bastion","text":"<ul> <li>To allow access private subnets from public subnets, it is an EC2 deployed in a public subnet.</li> </ul>"},{"location":"cloud/aws/cert-sap/#security_1","title":"Security","text":""},{"location":"cloud/aws/cert-sap/#network-acls","title":"Network ACLs","text":"<ul> <li>Attached at the subnet level.</li> <li>A network ACL is created for a default Amazon VPC and is associated with all subnets in your default Amazon VPC.</li> <li>Network ACLs manage traffic entering or leaving a subnet because network ACLs are associated with the subnet, not with resources inside the subnet. Network ACLs only manage traffic that is crossing the subnet boundary.</li> <li>Sateless resources, you have to configure in and out rules, for each port and ip ranges.</li> <li>The default network ACL allows all traffic in and out by default.</li> <li>Network ACLs are a great way to limit broad ranges of IP addresses from getting access to or from a subnet.</li> </ul>"},{"location":"cloud/aws/cert-sap/#security-groups","title":"Security groups","text":"<ul> <li>Attached to AWS resources, specifically the elastic network interfaces (ENIs), not Amazon VPC subnets.</li> <li>Security groups offer a few advantages compared to network ACLs in that they can recognize AWS resources and filter based on them.</li> <li>Security groups can reference other security groups and also reference themselves.  </li> <li>Stateful resources.</li> <li>Out traffic not checked.</li> <li>By default only allows outbound traffic.</li> <li>Security groups are not capable of explicitly blocking traffic. Security group rules only specify what traffic is allowed, while all other traffic is blocked.</li> </ul>"},{"location":"cloud/aws/cert-sap/#best-practices","title":"Best practices","text":"<ul> <li>Use multiple AZ deployments so you have high availability.</li> <li>Use CloudWatch to monitor your VPC components.</li> <li>Use VPC flow logs to capture traffic information. It does not affect network throughput or latency because it's collected outside of the path of your network traffic. You store them in S3. VPC flow logs do not capture packet payload.</li> <li>Chaining security groups together. Instead of IP ranges, define access to specific ports by other security groups (example: database servers sg would only allow traffic from application server sg).</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-networking-gateways","title":"AWS Networking Gateways","text":"<ul> <li>Internet gateway is an Amazon VPC component that allows communication between your computer and the internet. Applications include, Elastic Load Balancers, Amazon EC2 instances, Amazon S3, AWS Lambda and so on.</li> <li>Customer Gateway is a physical or software appliance that you own or manage in your on premises network. Applications include manages routing to and from your environment.</li> <li>VPN Gateway is the gateway on the AWS side of site-to-site VPN connection. Applications include Amazon EC2 instances, Amazon S3, Amazon RDS&lt; Amazon Lambda, and so on.</li> <li>Direct Connect Gateway establishes connectivity that spans Amazon VPCs spread across multiple AWS Regions. Applications include Amazon EC2 instances, Amazon RDS, AWS Lambda, and so on.</li> <li>NAT Gateway is a network address translation service that enables instances in a private subnet to connect to services outside your VPC. Applications include Amazon EC2 instances, Amazon RDS, AWS Lambda, and so on.</li> <li>AWS Transit Gateway connects Amazon VPCs, AWS accounts, and on premises networks to a single gateway. Applications include Amazon VPC connections, AWS VPN connection, AWS Direct Connect.</li> <li>Virtual gateway allows resources that are outside of your mesh network to communicate to resources that are inside. Applications include Amazon EC2, Amazon ECS, and Amazon EKS.</li> </ul>"},{"location":"cloud/aws/cert-sap/#vpc-peering","title":"VPC peering","text":"<ul> <li>VPC peering is a way to link multiple Amazon VPCs together and allows direct communications between two isolated Amazon VPCs using their private IP addresses.</li> <li>Adds entries in the route tables of each subnet with the destination of the other CIDR range and the peering connection VPX as target.</li> <li>VPC peers can span AWS accounts and also Regions (inter-region vpc peering).</li> <li>A VPC peering connection is highly available.</li> <li>This design is not scalable. As your environment grows and more Amazon VPCs are added, you will need to consider other options for connecting VPCs. VPC peering is a one-to-one connection, no more. You have a VPC peering connection between VPC A and VPC B, and between VPC A and VPC C. There is no VPC peering connection between VPC B and VPC C.</li> <li>The data shared is encrypted using the AWS global infrastructure.</li> <li>All data transfer over a VPC peering connection that stays within an Availability Zone is free of charge. All data transfer over a VPC peering connection that crosses Availability Zones will continue to be charged at the standard in-Region data transfer rates.</li> <li>Is great for shared services running in a single VPC to be accessible to other VPCs, vendor or partner systems that need to access your VPC or in reverse, security audits, requirements to split an application into multiple isolated VPCs to limit the impact of a potential outrage or application failure.</li> <li>You cannot create a VPC peering connection between VPCs with matching or overlapping IPv4 CIDR blocks. This limitation also applies to VPCs that have nonoverlapping IPv6 CIDR blocks. You cannot create a VPC peering connection if the VPCs have matching or overlapping IPv4 CIDR blocks. This applies even if you intend to use the VPC peering connection for IPv6 communication only.</li> <li>If either VPC in a peering relationship has one of the following connections, you cannot extend the peering relationship to that connection: A VPN connection or a Direct Connect connection to a corporate network, An internet connection through an internet gateway, An internet connection in a private subnet through a NAT device, A gateway VPC endpoint to an AWS service, for example, an endpoint to Amazon S3.</li> <li>There is no charge for setting up or running a VPC peering connection. Data transferred across peering connections is charged per gigabyte for send and receive, regardless of the Availability Zones involved.</li> </ul>"},{"location":"cloud/aws/cert-sap/#vpc-lattice","title":"VPC Lattice","text":"<ul> <li>Don't require sidecars or proxies.</li> <li>Works across all compute options.</li> <li>Has traffic and access controls.</li> <li>Don't require network expertise.</li> </ul>"},{"location":"cloud/aws/cert-sap/#components_1","title":"Components","text":"<ul> <li>Service: represents an application unit, build up of listeners, rules and target-groups. You define routing policies.</li> <li>Service network: logical grouping mechanism, you associate VPCs to it and also services.</li> <li>Service directory: centralized registry of services associated with VPC Lattice.</li> <li>Auth policies: you can apply IAM resource policies on the Service Network and the service level.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-cloud-wan","title":"AWS Cloud WAN","text":"<ul> <li>Global solution for connectivity for AWS and on-premise.</li> <li>Build global networks on AWS.</li> <li>If you need to connect cloud routing and software-defined wide area networks, you can use this to provide a central dashboard for making the connections between your offices, data centers and VPCs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#components_2","title":"Components","text":"<ul> <li>Core network</li> <li>Core network edge (exists in each desired region)</li> <li>Network policy: you can validate it before push to production</li> <li>Segments (Defined in desired regions): Layer 3 isolated routing boundaries. In traditional networks, this is very similar to VRF, virtual route forwarding, or Layer 3 IP VPN or MPLS networks.</li> <li>Routing (sharing between segments and static routes)</li> <li>Mapping attachments to segments (map using tags or metadata, also can have a human acceptance)</li> </ul>"},{"location":"cloud/aws/cert-sap/#elb-elastic-load-balancer","title":"ELB (Elastic Load Balancer)","text":"<ul> <li>High availability by design.</li> </ul>"},{"location":"cloud/aws/cert-sap/#application-load-balancer","title":"Application Load Balancer","text":"<ul> <li>Configured at layer 7 form http/https traffic.</li> <li>The listener is configured to listen to a port and send traffic to a target group if the rule match.</li> <li>Rules can be defined based on path, host, headers, methods and source IPs.</li> <li>Understands HTTPS traffic.</li> <li>Can authenticate users (uses OpenID Connect (OIDC) protocol and integrates with other AWS services like SAML, LDAP, Microsoft Active Directory, etc)</li> <li>Each target group needs a health check.</li> <li>A target type can be instances, IPs or lambda functions.</li> <li>Can be internet-facing or internal-facing.</li> <li>You configure security groups and the vpc.</li> <li>Can configure redirects and fixed responses.</li> <li>Supports sticky sessions (for stateful applications). Uses an HTTP cookie to remember to which server has to send the traffic.</li> <li>Automatically provides a static IP address per AZ.</li> <li>Lets users assign a custom, fixed IP address per AZ.</li> <li>Billed at an hourly rate and an additional rate based on the load placed on your load balancer.</li> </ul>"},{"location":"cloud/aws/cert-sap/#network-load-balancer","title":"Network Load Balancer","text":"<ul> <li>Configured at layer 4 for TCP/UDP/TLS.</li> <li>Low latency.</li> <li>Able to handle high-end workloads and scale to millions of requests per second. </li> <li>Supports sticky sessions.</li> <li>A target type can be instances, IPs or ALB.</li> <li>Source IP preservation.</li> <li>Automatically provides a static IP address per AZ.</li> <li>Lets users assign a custom, fixed IP address per AZ.</li> <li>Uses Route 53 to direct traffic to load balancer nodes in other zones.</li> <li>It maintains stickiness of flows to a specific target appliance using 5-tuple (for TCP/UDP flows) or 3-tuple (for non-TCP/UDP flows). </li> <li>The Gateway Load Balancer and its registered virtual appliance instances exchange application traffic using the GENEVE protocol on port 6081. It supports a maximum transmission unit (MTU) size of 8,500 bytes.</li> </ul>"},{"location":"cloud/aws/cert-sap/#gateway-load-balancer","title":"Gateway Load Balancer","text":"<ul> <li>Configured alt layer 3 gateway and layer 4 load balancing.</li> <li>A target type can be instances or IPs.</li> <li>Helps you to deploy, scale, and manage your third-party appliances, such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems.</li> <li>Provides a gateway for distributing traffic across multiple virtual appliances while scaling them up and down based on demand.</li> <li>Streamlined deployments: Can deploy a new virtual appliance by selecting it in the AWS Marketplace.</li> <li>Connects internet gateways, virtual private clouds (VPCs), and other network resources over a private network.</li> <li>Automatically provides a static IP address per AZ.</li> <li>Lets users assign a custom, fixed IP address per AZ.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-transit-gateway","title":"AWS Transit Gateway","text":"<ul> <li>A service to manage and simplify the connections and peering for your VPCs.</li> <li>A regional resource that resides outside of a VPC.</li> <li>Creates a one-to-many peering connection through a central hub.</li> <li>Within a Region, AWS Transit Gateway provides a method for consolidating and centrally managing routing between VPCs with a hub-and-spoke network architecture.</li> <li>Inter-Region peering connects AWS Transit Gateways together using the AWS global network. This adds automatic encryption for your data, and your data never travels over the public internet.</li> <li>Transit gateways can also be used to connect your AWS environment to your on premises infrastructure creating a hybrid network of AWS and physical networks.</li> <li>Transit gateway offers AWS Transit Gateway Network Manager(opens in a new tab), which adds a unique view over your entire network, even connecting to Software-Defined Wide Area Network (opens in a new tab)(SD-WAN) devices.</li> <li>In AWS Transit Gateway, you are charged for the number of connections that you make to the Transit Gateway per hour and the amount of traffic (GB of data processed) that flows through AWS Transit Gateway. For AWS VPN attachments, the Transit Gateway owner is billed hourly. For AWS Direct Connect attachments, the Direct Connect Gateway owner is billed hourly. For Transit Gateway Connect attachments (SD-WAN appliances), the Transit Gateway owner is billed hourly. For peering attachments, each Transit Gateway owner is billed hourly for the peering attachment with the other Transit Gateway.</li> <li>You can have up to 20 different route tables per transit gateway.</li> </ul>"},{"location":"cloud/aws/cert-sap/#concepts","title":"Concepts","text":"<ul> <li>Attachments: one or more VPCs, a compatible SD-WAN appliance, a Direct Connect gateway, a peering connection with another transit gateway, a VPN connection to a transit gateway.</li> <li>AWS Transit Gateway MTU: 8500 bytes for VPC connections, Direct Connect connections, connections to other transit gateway and peering connection. 1500 bytes for VPN connections.</li> <li>AWS Transit Gateway route table: A transit gateway has a default route table and can optionally have additional route tables. A route table includes dynamic and static routes that decide the next hop based on the destination IP address of the packet. The target of these routes can be any transit gateway attachment.</li> <li>Associations: Each attachment is associated with exactly one route table. Each route table can be associated with zero to many attachments.</li> <li>Route propagation: A VPC, VPN connection, or Direct Connect gateway can dynamically propagate routes to a transit gateway route table. With a Direct Connect attachment, the routes are propagated to a transit gateway route table by default. With a VPC, you must create static routes to send traffic to the transit gateway. With a VPN connection or a Direct Connect gateway, routes are propagated from the transit gateway to your on-premises router using BGP. With a peering attachment, you must create a static route in the transit gateway route table to point to the peering attachment.</li> </ul>"},{"location":"cloud/aws/cert-sap/#path-selection-behavior","title":"Path selection behavior","text":"<ul> <li> <ol> <li>Most specific route (longest prefix match)</li> </ol> </li> <li> <ol> <li>Static route entries, including site-to-site VPN routes</li> </ol> </li> <li> <ol> <li>BGP-propagated routes from Direct Connect gateway</li> </ol> </li> <li> <ol> <li>BGP-propagated routes from AWS Site-to-Site VPN</li> </ol> </li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-privatelink","title":"AWS PrivateLink","text":"<ul> <li>Helps to establish secure and private connectivity between VPCs, AWS services and your on premises network.</li> <li>Makes it easy to connect services across different accounts and Amazon VPCs to significantly simplify your network architecture.</li> <li>Network traffic that uses PrivateLink doesn't traverse the public internet.</li> <li>You can also associate security groups and attach an endpoint policy to interface endpoints, which allow you to control precisely who has access to a specified service.</li> <li>There is no need to configure an internet gateway, VPC peering connection, or manage VPC CIDRs. It provides a more secure connection for services across different accounts and Amazon VPCs, with no need for firewall rules, path definitions, or route tables.</li> <li>Gateway VPC endpoints targets specific IP routes ina a VPC route table in the form of a prefix lists. Used for DynamoDB or S3.</li> <li>Interface VPC endpoints, powered by AWS PrivateLink, connect you to services hosted by AWS Partners and supported solutions available in AWS Marketplace. It is and elastic network interface with a private IP address from the IP address range of your subnet. It servers as an entry point for traffic.</li> <li>Gateway Load Balancer endpoints, powered by AWS PrivateLink, brings the same level of security and performance to your virtual network appliances or custom traffic inspection logic. This type of endpoint serves as an entry point to intercept traffic and route it to a service that you've configured using Gateway Load Balancers, for example, for security inspection.</li> <li>You will be billed for each hour that your VPC endpoint remains provisioned in each Availability Zone, irrespective of the state of its association with the service.</li> <li>With AWS PrivateLink, services establish a Transmission Control Protocol (TCP) connection between the service provider's VPC and the service consumer's VPC. This provides a secure and scalable solution.</li> <li>A transit VPC connects multiple VPCs that might be geographically disparate or running in separate AWS accounts to a common VPC that serves as a global network transit center. This network topology simplifies network management and minimizes the number of connections that you need to set up and manage. It is implemented virtually and does not require any physical network equipment or a physical presence in a co-location transit hub.</li> <li>AWS PrivateLink gives on-premises networks private access to AWS services through Direct Connect. You can also make services available to other accounts and VPCs that are accessed securely as private endpoints. If you use AWS PrivateLink with a Network Load Balancer to route traffic to your service or application, clients can connect to any service you host. Services configured to support AWS PrivateLink can be offered as a subscription service through the AWS Marketplace.</li> <li>AWS PrivateLink does not support IPv6.</li> <li>Traffic will be sourced from the Network Load Balancer inside the service provider VPC. From the perspective of the service provider application, all IP traffic will originate from the Network Load Balancer. All IP addresses logged by the application will be the private IP addresses of the Network Load Balancer. The service provider application will never see the IP addresses of the customer or service consumer.</li> <li>You can activate Proxy Protocol v2 to gain insight into the network traffic. Network Load Balancers use Proxy Protocol v2 to send additional connection information such as the source and destination. This might require changes to the application.</li> <li>Endpoint services cannot be tagged.</li> <li>The private Domain Name System (DNS) of the endpoint does not resolve outside of the VPC. Private DNS hostnames can be configured to point directly to endpoint network interface IP addresses. Endpoint services are available in the AWS Region in which they are created and can be accessed in remote AWS Regions using inter-Region VPC peering.</li> <li>Availability Zone names in a customer account might not map to the same locations as Availability Zone names in another account. For example, the Availability Zone US-East-1A might not be the same Availability Zone as US-East-1A for another account. An endpoint service is configured in Availability Zones according to their mapping in a customer\u2019s account.</li> <li>When an interface endpoint is created, endpoint-specific DNS hostnames are generated that can be used to communicate with the service.</li> <li>An endpoint-specific DNS hostname is automatically generated and includes all zonal DNS hostnames generated for the interface endpoint. The hostname includes a unique endpoint identifier, service identifier, Region, and vpce.amazonaws.com in its name.</li> <li>You can generate a zonal-specific DNS hostname for each Availability Zone in which the endpoint is available. The hostname includes the Availability Zone in its name. </li> <li>You can use a private DNS hostname to alias the automatically created zonal-specific or regional-specific DNS hostnames into a friendly hostname.</li> <li>Interface endpoints and Gateway Load Balancer endpoints are charged for each hour the VPC endpoint remains provisioned in each Availability Zone and for each gigabyte processed through the VPC endpoint.</li> <li>There is no additional charge for using gateway endpoints. Standard charges for data transfer and resource usage apply. You might be able to reduce costs by selecting gateway endpoints for traffic destined to DynamoDB or Amazon S3.</li> </ul> Feature Gateway Endpoints Interface Endpoints Security Uses endpoint policies for security Uses endpoint policies and security groups Amazon network connectivity Uses Amazon network to connect to S3 and DynamoDB Uses Amazon Network through AWS PrivateLink to connect to AWS services Access outside of the VPC Not accessible from outside the VPC Can be accessed from on-premises and across Regions IP connectivity Uses the public IPs of S3 and modifies the route table Uses private IPs and can use public or endpoint-specific DNS names Cost No additional charge for using gateway endpoints Charges a fee"},{"location":"cloud/aws/cert-sap/#aws-direct-connect","title":"AWS Direct Connect","text":"<ul> <li>Your internal network is linked to an AWS Direct Connect location over a standard Ethernet fiber-optic cable. </li> <li>This connection allows you to create virtual interfaces directly to public AWS services or to your VPC.</li> <li>When choosing to implement a Direct Connect connection, you should first consider bandwidth, connection type, protocol configurations, and other network configuration specifications.</li> <li>Only supports 802.1Q encapsulation. All equipment that will be part of the physical connection linking your location with AWS must support 802.1Q encapsulation.</li> <li>You pay only for what you use and there is no minimum fee. Direct Connect has two billing elements. Port hours and outbound data transfer. There might be additional external billing elements to consider.</li> </ul>"},{"location":"cloud/aws/cert-sap/#decision-1-form-of-connection","title":"Decision 1: Form of connection","text":""},{"location":"cloud/aws/cert-sap/#dedicated-connection-collocated-at-a-direct-connection-location","title":"Dedicated connection collocated at a Direct Connection location","text":"<ul> <li>AWS has partnered with companies around the world to offer physical uplinks to AWS through the Direct Connect service.</li> <li>You deploy a router and supporting equipment to that location.</li> <li>The equipment you deploy will be the physical connection between your on-premises location and the AWS router at the Direct Connect location.</li> <li>You are responsible for the deployed equipment, the circuit that connects your op-premises location to the deployed equipment, and the connection from the deployed equipment to the AWS router.</li> </ul>"},{"location":"cloud/aws/cert-sap/#contracting-with-a-direct-connect-partner","title":"Contracting with a Direct Connect Partner","text":"<ul> <li>The Direct Connect Partner will provide you with the necessary equipment at the Direct Connect location that will connect to the AWS router.</li> <li>You will need to provide the physical connection between your on-premises location and the Direct Connect Partner equipment.</li> <li>The Direct Connect Partner will configure and maintain the physical equipment at the Direct Connect location.</li> </ul>"},{"location":"cloud/aws/cert-sap/#connecting-directly-to-a-direct-connect-node","title":"Connecting directly to a Direct Connect node","text":"<ul> <li>You are responsible for all the equipment from the node to your location.</li> </ul>"},{"location":"cloud/aws/cert-sap/#decision-2-choose-the-bandwidth","title":"Decision 2: Choose the bandwidth","text":"<ul> <li>One gigabit per second, require a 1,310-nanometer 1000BASE-LX transceiver.</li> <li>10 gigabits per second, require a 1,310-nanometer 10-gigabit BASE-LR transceiver.</li> <li> <p>100 gigabits per second, require a 100-gigabit BASE-LR4 transceiver.</p> </li> <li> <p>All types must all be single-mode fiber and you must disable auto-negotiation and configure your port speed and full duplex mode.</p> </li> <li>Direct Connect supports the Link Aggregation Control Protocol (LACP), facilitating multiple dedicated physical connections to be grouped into link aggregation groups (LAGs). When you group connections into LAGs, you can stream the multiple connections as a single, managed connection.</li> <li>All connections in the LAG must use the same bandwidth.</li> <li>ou can have a maximum of two 100-Gbps connections in a LAG, or four connections with a port speed less than 100 Gbps. Each connection in the LAG counts toward your overall connection limit for the Region.</li> <li>All connections in the LAG must terminate at the same Direct Connect endpoint.</li> <li>When you create a LAG, you can download the Letter of Authorization and Connecting Facility Assignment (LOA-CFA) for each new physical connection individually from the Direct Connect console.</li> </ul>"},{"location":"cloud/aws/cert-sap/#decision-3-bgp-bfd-and-public-or-private-asn","title":"Decision 3: BGP (+ BFD) and public or private ASN","text":"<ul> <li>The router that will connect to the AWS router must support Border Gateway Protocol, or BGP, and Border Gateway Protocol MD5 authentication.</li> <li>Direct Connect also supports asynchronous Bidirectional Forwarding Detection, or BFD. BFD can be configured with BGP, but is optional.</li> <li>BGP requires an Autonomous System Number, or ASN.</li> <li>ASNs can be public or private.</li> <li>Private ASNs can be self-determined.</li> <li>Public ASNs must be purchased and registered.</li> <li>Your choice of private or public ASN will determine which type of virtual interface you can use later. Private virtual interfaces require a private ASN and public virtual interfaces require a public ASN.</li> </ul>"},{"location":"cloud/aws/cert-sap/#decision-4-ip","title":"Decision 4: IP","text":"<ul> <li>IPv4</li> <li>IPv6</li> <li> <p>Both</p> </li> <li> <p>Public virtual interface will advertise your IP prefixes over BGP.</p> </li> <li>AWS has implemented BGP community tags for private virtual interfaces to achieve load balancing and route preferences for traffic inbound to AWS.</li> </ul>"},{"location":"cloud/aws/cert-sap/#decision-5-ethernet-frame-size","title":"Decision 5: Ethernet frame size","text":"<ul> <li>Direct Connect virtual interfaces support a default Ethernet frame size of 1522 bytes and a jumbo Ethernet frame size of 9023 bytes.</li> <li>Ensure that all the equipment you will use to connect your on-premises location to your AWS environment supports the Ethernet frame size you want to implement.</li> </ul>"},{"location":"cloud/aws/cert-sap/#step-6-configure-direct-connect-connection-in-the-dashboard","title":"Step 6: Configure Direct Connect connection in the dashboard.","text":"<ul> <li>When you have configured your connection, AWS will provide you with a Letter of Authorization and Connecting Facility Assignment, or LOA-CFA. You will share your LOA-CFA with your Direct Connect Partner, showing them that AWS has authorized the completion of the last physical step for your Direct Connect connection.</li> <li>After they receive the LOA-CFA, your Direct Connect Partner will physically complete the connection between your router and the AWS router with a cross connect.</li> </ul>"},{"location":"cloud/aws/cert-sap/#decision-7-configure-the-virtual-interface","title":"Decision 7: Configure the virtual interface","text":"<ul> <li>Private: Choosing a private virtual interface lets you connect to all virtual private cloud, or VPC, resources within the private IP space in your AWS environment. Connect a single private virtual interface to multiple VPCs through private gateways within an AWS Region by associating it with your Direct Connect gateway.</li> <li>Public: Choosing a public virtual interface lets you route traffic to all VPC resources with a public IP address or that are connected to an AWS public endpoint. If you connect a public virtual interface to a Direct Connect location, you can connect to all public global AWS IP addresses and access AWS global IP route tables.</li> <li>Transit: Choosing a transit virtual interface lets you connect your Direct Connect connection to AWS Transit Gateway. Then you can use the power of the AWS Transit Gateway and the AWS Transit Gateway Network Manager to manage the traffic moving between your AWS environment and your physical location. A transit virtual interface supports connecting three transit gateways to your Direct Connect gateway. Each connected transit gateway can connect to multiple VPCs within the same Region, even if they belong to different accounts.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-site-to-site-vpn","title":"AWS Site-to-Site VPN","text":"<ul> <li>Create a secure and encrypted connections quickly if you need to connect remote offices to AWS.</li> <li>Based on IPsec technology, AWS Site-to-Site VPN uses a VPN tunnel to pass data from the customer network to or from AWS.</li> <li>One AWS Site-to-Site VPN connection consists of two tunnels. Each tunnel terminates in a different Availability Zone on the AWS side, but it must terminate on the same customer gateway on the customer side.</li> </ul>"},{"location":"cloud/aws/cert-sap/#components_3","title":"Components","text":"<ul> <li>Customer gateway: A resource you create and configure in AWS that represents your on-premise gateway device. The resource contains information about the type of routing used by the Site-to-Site VPN, BGP, ASN and other optional configuration information.</li> <li>Customer gateway device: A customer gateway device is a physical device or software application on your side of the AWS Site-to-Site VPN connection.</li> <li>Virtual private gateway: A virtual private gateway is the VPN concentrator on the Amazon side of the AWS Site-to-Site VPN connection. You use a virtual private gateway or a transit gateway as the gateway for the Amazon side of the AWS Site-to-Site VPN connection.</li> <li>Transit gateway: A transit gateway is a transit hub that can be used to interconnect your VPCs and on-premises networks. You use a transit gateway or virtual private gateway as the gateway for the Amazon side of the AWS Site-to-Site VPN connection.</li> </ul>"},{"location":"cloud/aws/cert-sap/#limitations","title":"Limitations","text":"<ul> <li>IPv6 traffic is partially supported. AWS Site-to-Site VPN supports IPv4/IPv6-Dualstack through separate tunnels for inner traffic. IPv6 for outer tunnel connection not supported.</li> <li>AWS Site-to-Site VPN does not support Path MTU Discovery. The greatest Maximum Transmission Unit (MTU) available on the inside tunnel interface is 1,399 bytes.</li> <li>Throughput of AWS Site-to-Site VPN connections is limited. When terminating on a virtual private gateway, only one tunnel out of the pair can be active and carry a maximum of 1.25 Gbps. However, real-life throughput will be about 1 Gbps. When terminating on AWS Transit Gateway, both tunnels in the pair can be active and carry an aggregate maximum of 2.5 Gbps. However, real-life throughput will be 2 Gbps. Each flow (for example, TCP stream) will still be limited to a maximum of 1.25 Gbps, with a real-life value of about 1 Gbps.</li> <li>Maximum packets per second (PPS) per VPN tunnel is 140,000.</li> <li>AWS Site-to-Site VPN terminating on AWS Transit Gateway supports equal-cost multi-path routing (ECMP) and multi-exit discriminator (MED) across tunnels in the same and different connection. ECMP is only supported for Site-to-Site VPN connections activated on an AWS Transit Gateway. MED is used to identify the primary tunnel for Site-to-Site VPN connections that use BGP. Note, BFD is not yet supported on AWS Site-to-Site VPN, though it is supported on Direct Connect.</li> <li>AWS Site-to-Site VPN endpoints use public IPv4 addresses and therefore require a public virtual interface to transport traffic over Direct Connect. Support for AWS Site-to-Site VPN over private Direct Connect is not yet available.</li> <li>For globally distributed applications, the accelerated Site-to-Site VPN option provides a connection to the global AWS backbone through AWS Global Accelerator. Because the Global Accelerator IP space is not announced over a Direct Connect public virtual interface, you cannot use accelerated Site-to-Site VPN with a Direct Connect public virtual interface.</li> </ul>"},{"location":"cloud/aws/cert-sap/#monitoring","title":"Monitoring","text":"<ul> <li>You can monitor VPN tunnels using Amazon CloudWatch, which collects and processes raw data from the VPN service into readable, near real-time metrics.</li> <li>These statistics are recorded for a period of 15 months.</li> <li>You can access historical information and gain a better perspective on how your web application or service is performing.</li> <li>VPN metric data is automatically sent to CloudWatch as it becomes available.</li> </ul>"},{"location":"cloud/aws/cert-sap/#pricing","title":"Pricing","text":"<ul> <li>AWS Site-to-Site VPN: connection per hour and data transfer out charges.</li> <li>Accelerated Site-to-Site VPN: connection per hour, data transfer out charges, hourly charges for two AWS Global Accelerators per VPN connection and data transfer out premium (DT-Premium) fees.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-client-vpn","title":"AWS Client VPN","text":"<ul> <li>If you need to connect remote team access to AWS and your on premises resources.</li> <li>Based on OpenVPN technology.</li> </ul>"},{"location":"cloud/aws/cert-sap/#components_4","title":"Components","text":"<ul> <li>Client VPN endpoint: Your Client VPN administrator creates and configures a Client VPN endpoint in AWS. Your administrator controls which networks and resources you can access when you establish a VPN connection.</li> <li>VPN client application: This is the software application that you use to connect to the Client VPN endpoint and establish a secure VPN connection.</li> <li>Client VPN endpoint configuration file: This is a configuration file that is provided to you by your Client VPN administrator. The file includes information about the Client VPN endpoint and the certificates required to establish a VPN connection. You load this file into your chosen VPN client application.</li> </ul>"},{"location":"cloud/aws/cert-sap/#limitations_1","title":"Limitations","text":"<ul> <li>Client VPN supports IPv4 traffic only. IPv6 is not supported.</li> <li>Security Assertion Markup Language (SAML) 2.0-based federated authentication only works with an AWS provided client v1.2.0 or later. </li> <li>SAML integration with AWS Single Sign-On requires a workaround. Better integration is being worked on.</li> <li>Client CIDR ranges must have a block size of at least /22 and must not be greater than /12.</li> <li>A Client VPN endpoint does not support subnet associations in a dedicated tenancy VPC.</li> <li>Client VPN is not compliant with Federal Information Processing Standards (FIPS).</li> <li>Client CIDR ranges cannot overlap with the local CIDR of the VPC in which the associated subnet is located. It also cannot overlap any routes manually added to the Client VPN endpoint's route table.</li> <li>A portion of the addresses in the client CIDR range is used to support the availability model of the Client VPN endpoint and cannot be assigned to clients. Therefore, we recommend that you assign a CIDR block that contains twice the number of required IP addresses. This will ensure the maximum number of concurrent connections that you plan to support on the Client VPN endpoint.</li> <li>The client CIDR range cannot be changed after you create the Client VPN endpoint.</li> <li>The subnets associated with a Client VPN endpoint must be in the same VPC.</li> <li>You cannot associate multiple subnets from the same Availability Zone with a Client VPN endpoint.</li> <li>AWS Certificate Manager (ACM) certificates are not supported with mutual authentication because you cannot extract the private key. You can use an ACM server as the server-side certificate. But, to add a client certificate to your customer configuration, you cannot use a general ACM certificate because you can't extract the required private key details. So you must access the keys in one of two ways. Either generate your own certificate where you have the key or use AWS Certificate Manager Private Certificate Authority (ACM PCA), which gives the private keys. If the customer is authenticating based on Active Directory or SAML, they can use a general ACM-generated certificate because only the server certificate is required.</li> </ul>"},{"location":"cloud/aws/cert-sap/#monitoring_1","title":"Monitoring","text":"<ul> <li>End-user usage reporting is possible through Amazon CloudWatch Logs.</li> <li>You can use a client connect handler to do basic posture assessment with Lambda because Client VPN does not support native posture assessment.</li> <li>Client VPN publishes metrics to CloudWatch for your Client VPN endpoints. Metrics are published to CloudWatch every five minutes.</li> </ul>"},{"location":"cloud/aws/cert-sap/#pricing_1","title":"Pricing","text":"<ul> <li>Charged for the number of active client connections per hour and the number of subnets associated to Client VPN per hour.</li> <li>Billing starts when the subnet association is made.</li> <li>Each partial hour consumed is prorated for the hour.</li> <li>Any client connection that is less than an hour is also prorated for the hour.</li> </ul>"},{"location":"cloud/aws/cert-sap/#networking-design-patterns","title":"Networking Design Patterns","text":""},{"location":"cloud/aws/cert-sap/#simplifying-multi-vpc-routing-hub-and-spoke-scenario","title":"Simplifying Multi-VPC Routing: Hub-and-spoke scenario","text":"<ul> <li>Deploy a transit gateway and phase out the VPC peering connections. This will reduce the complexity of the network and move from a distributed to a centrally managed model, reducing operational overhead.</li> <li>All routing tables are centralized on the transit gateway, simplifying management.</li> <li>The VPN connections terminates on the transit gateway.</li> <li>The Direct Connect connection terminates on the transit gateway.</li> </ul>"},{"location":"cloud/aws/cert-sap/#resilient-hybrid-networks","title":"Resilient Hybrid Networks","text":"<ul> <li>AWS guarantees Direct Connect service as highly available by providing the service with a four nines (99.99%) service-level agreement. However, that guarantee only extends from the AWS router that the company connects to, not the segments of the connection before that.</li> </ul>"},{"location":"cloud/aws/cert-sap/#regional-high-availability","title":"Regional High Availability","text":"<ul> <li>Route 53: used to route traffic based on latency or region.</li> <li>ELB: Public load balancer that passes the traffic to EC2 instances.</li> <li>Database: Clustered with data synchronization between regions.</li> <li>VPC peering connection: This permits application resources to communicate using a cost-effective way to replicate data between Regions. Inter-Region traffic is automatically encrypted and never traverses the public internet, always staying on the global AWS backbone.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-transit-gateway-peering","title":"AWS Transit Gateway Peering","text":"<ul> <li>For large, multiregional AWS environments, AWS Transit Gateway supports inter-Region peering.</li> <li>This connection type creates a nontransitive, bidirectional route from a transit gateway in one Region to a transit gateway in another Region. Traffic that crosses an inter-Region peering connection is automatically encrypted and never leaves the global AWS backbone.</li> <li>To make a highly available multiregional AWS environment using transit gateways, a mesh network design is required. A mesh design ensures that each transit gateway has a peered connection with every other transit gateway in the environment, so traffic moves across the environment as needed.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-app-mesh","title":"AWS App Mesh","text":"<ul> <li>Connects containers and microservices with application-level networking.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-api-gateway","title":"Amazon API Gateway","text":"<ul> <li>To manage APIs at any scale.</li> <li>You can import from OpenAPI.</li> <li>Integration with lambda function, http, mock or aws service proxying (like s3, dynamodb, etc).</li> <li>APIs are deployed to stages. Previous deployments are keep as snapshots that can be restores.</li> <li>Stage variables can be configured.</li> <li>You can generate a SDK to consume the stage.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-cloud-map","title":"AWS Cloud Map","text":"<ul> <li>Discovers access to the most recent resources and services.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cloudfront","title":"CloudFront","text":"<ul> <li>AWS has edge locations and regional edge caches that are used by CloudFront.</li> <li>It is a content delivery network (CDN).</li> <li>Delivers data, videos, applications and APIs with lower latency and higher transfer speeds.</li> <li>You create a distribution by one or multiple origins (different base api path defined in behaviors).</li> <li>You can associate custom CNAMEs.</li> <li>Origins can be S3, ALB, MediaStore container, MediaPackage channel, Lambda function url, EC2 or AWS API Gateway.</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases_1","title":"Use cases","text":"<ul> <li>Static Asset Caching</li> <li>Live and On-Demand Video Streaming</li> <li>Security and DDoS protection</li> <li>Dynamic and Customized Content</li> <li>API acceleration</li> <li>Software distribution</li> </ul>"},{"location":"cloud/aws/cert-sap/#route-53","title":"Route 53","text":"<ul> <li>The DNS for AWS.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-global-accelerator","title":"AWS Global Accelerator","text":"<ul> <li>Optimizes your user traffic, from the user to your application.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-shield","title":"AWS Shield","text":"<ul> <li>Adds a safeguard to your applications against DDoS attacks.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-waf","title":"AWS WAF","text":"<ul> <li>Protects your web application from common web exploits.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-network-firewall","title":"AWS Network Firewall","text":"<ul> <li>Deploys network security access for your VPC.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-firewall-manager","title":"AWS Firewall Manager","text":"<ul> <li>Helps to centrally configure and manage your firewall rules.</li> </ul>"},{"location":"cloud/aws/cert-sap/#monitoring-and-troubleshooting","title":"Monitoring and troubleshooting","text":""},{"location":"cloud/aws/cert-sap/#metrics","title":"Metrics","text":"<ul> <li>Bandwidth capacity: the maximum data transmission rate possible on a network.</li> <li>Throughput: data transmission rate. A low throughput means there are a lot of failed or dropped packets that need to be sent again.</li> <li>Latency: delay that happens between a node or device requesting data and when the data is finished being delivered.</li> <li>Packet loss: how many data packets are dropped during data transmissions on your network.</li> <li>Retransmission: retransmission rate lets you know how often packets are being dropped. Retransmission delay measures the time it takes for a dropped packet to be retransmitted.</li> <li>Availability: measures whether the network is currently operational.</li> <li>Connectivity: jitter is a variation in delay or disruption that occurs while data packets travel across the network.</li> <li>Network and server response time: metric that measures how quickly the application is responding to requests.</li> <li>CloudWatch metrics: you can monitor AWS Network Manager for global network, devices, connections, links and sites.</li> </ul>"},{"location":"cloud/aws/cert-sap/#security-kinds-of-controls","title":"Security kinds of controls","text":"<ul> <li>Directive: controls establish the governance, risk, and compliance models within which the environment operates.</li> <li>Detective: controls are intended to identify and characterize an incident in progress and provide assistance during investigations and audits after the event has occurred.</li> <li>Preventive: controls are designed to prevent an incident from occurring.</li> <li>Responsive: controls are intended to limit the extent of any damage caused by the incident.</li> </ul>"},{"location":"cloud/aws/cert-sap/#tools","title":"Tools","text":""},{"location":"cloud/aws/cert-sap/#cloudwatch","title":"CloudWatch","text":"<ul> <li>Can configure alarms to be triggered when certain metrics values reach a threshold.</li> <li>Can configure alarms to be triggered when certain messages appear in logs.</li> <li>Can configure alarms to be triggered when an event is triggered.</li> <li>CloudWatch data is recorded for a period of 15 months to access historical information and gain a better perspective on how your network, application, or service is performing.</li> <li>Some CloudWatch metrics are not collected by default. The full range of CloudWatch metrics requires extra configuration and authorization. In that case, CloudWatch metrics need a CloudWatch agent installed to collect metrics for your Amazon EC2 instances and your on-premises servers.</li> <li>Data points with a 1-minute period are available for 15 days.</li> <li>For a peering connection, namespace would be AWS/PCX.</li> </ul>"},{"location":"cloud/aws/cert-sap/#data-points","title":"Data points","text":"<ul> <li>VPC Registered transited gateways (BytesIn, BytesOut, PacketsIn, PacketsOut)</li> <li>VPC Associated Site-to-Site VPN connections (TunnelDataIn, TunnelDataOut, TunnelState) </li> <li>VPC peering connections (awsRegion, vpcPeeringConnectionId, accepterVPCInfo, requesterVPCInfo)</li> <li>VPC On-premises resources (BytesIn, BytesOut, VPCTunnelDown)</li> <li>VPC Flow Logs metrics</li> </ul>"},{"location":"cloud/aws/cert-sap/#dashboards","title":"Dashboards","text":"<ul> <li>A single view for selected metrics and alarms to help you assess the health of your network, resources, and applications across one or more Regions and from multiple AWS accounts.</li> <li>A track for the same metric across multiple graphs by selecting the color used for each metric on each graph or widget.</li> <li>An operational playbook that provides guidance for team members during operational events about how to respond to network-specific incidents.</li> <li>A common view of critical resources and application measurements that can be shared by team members for faster communication flow during network performance or operational events.</li> <li>Metrics explorer widgets to your dashboard helps you to troubleshoot your network more efficiently.</li> <li>Alarm widget to display the status a single alarm or multiple alarms.</li> <li>Animated dashboard that replays CloudWatch metric data that was captured over time.</li> <li>You can share your CloudWatch dashboards across teams, with stakeholders, and with people external to your organization.</li> </ul>"},{"location":"cloud/aws/cert-sap/#logs","title":"Logs","text":"<ul> <li>Monitor, store, and access your log files from Amazon VPC, AWS Transit Gateway, AWS VPN, AWS Network Load Balancer, and other sources.</li> <li>You can then easily view the logs, search the logs for specific error codes or patterns and filter the logs based on specific fields or archive them securely for future analysis.</li> <li>Log events are a record of some activity recorded by the application or resource being monitored. The log event record that CloudWatch Logs understands contains two properties: the timestamp of when the event occurred, and the raw event message. Event messages must be UTF-8 encoded.</li> <li>Log streams are a sequence of log events that share the same source. More specifically, a log stream is generally intended to represent the sequence of events coming from the application instance or resource being monitored.</li> <li>Log groups define groups of log streams that share the same retention, monitoring, and access control settings. Each log stream has to belong to one log group. There is no limit on the number of log streams that can belong to one log group.</li> <li>Metric filters can extract metric observations from ingested events and transform them to data points in a CloudWatch metric. Metric filters are assigned to log groups and all of the filters assigned to a log group are applied to their log streams. You can choose to assign dimensions and a unit to the metric, use any type of CloudWatch statistic, including percentile statistics, when viewing these metrics or setting alarms and use subscriptions to get access to real-time feed of log events and have it delivered as an Kinesis stream, Kinesis Data Firehose stream or Lambda for custom processing. When log event are sent to the receiving service, they are base64-encoded and compressed with the gzip format.</li> <li>Retention settings can be used to specify how long log events are kept in CloudWatch Logs. Expired log events get deleted automatically. Just like metric filters, retention settings are also assigned to log groups, and the retention assigned to a log group is applied to its log streams. By default, log data is stored in CloudWatch Logs indefinitely. You can also export log data from your log groups to an S3.</li> </ul>"},{"location":"cloud/aws/cert-sap/#logs-insights","title":"Logs insights","text":"<ul> <li>Helps you efficiently identify patterns, interactively search, and analyze your log data with bar charts, line charts, and stacked area charts.</li> <li>You can save queries to run them again latter and structure them in folders. You are limited to save 1000 queries per region per account.</li> <li>You can add a query to a dashboard, they run every time you load the dashboard and every time the dashboard refreshes. These queries count toward your limit of 10 concurrent logs insights queries.</li> <li>Includes a purpose-build query language.</li> <li>To perform a query yo need to specify the log groups, time range to query and the query string to use.</li> <li>A single request can query up to 20 log groups. Queries time out after 15 minutes. Query results are available for 7 days.</li> <li>CloudWatch Logs Insights queries incur charges based on the amount of data that is queried.</li> <li>For every log sent, five system fields are automatically generated: @message, @timestamp, @ingestionTime, @logStream, @log (log group identifier).</li> </ul>"},{"location":"cloud/aws/cert-sap/#alarms","title":"Alarms","text":"<ul> <li>CloudWatch alarms facilitate more complex workflows such as sending an email, running a lambda function, viewing metrics per transit gateway for a global network and remediating changes to network ACLs or security groups.</li> <li>Metrics are grouped first by namespace and then by the various dimension combinations within each namespace.</li> <li>A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods.</li> <li>A composite alarm includes a rule expression that takes into account the alarm states of other alarms that you have created. The composite alarm goes into an ALARM state only if all conditions of the rule are met. Using composite alarms can reduce alarm noise.</li> <li>Add alarms to your CloudWatch dashboards and monitor them visually. When an alarm is on a dashboard, it turns red when it is in the ALARM state, making it easier for you to monitor its status proactively.</li> <li>An alarm invokes actions only when the alarm changes state. The exception is for alarms with Auto Scaling actions. For Auto Scaling actions, the alarm continues to invoke the action once per minute that the alarm remains in the new state.</li> <li>The alarm can be in one of three states: OK, ALARM and INSUFFICIENT_DATA.</li> <li>When you create an alarm, you specify three settings to evaluate when to change the alarm state: period is the length of time to evaluate the metric in seconds (if you specify 1 minute, the alarm evaluates the metric once per minute), evaluation periods is the number of the mos recent periods to evaluate when determining the alarm state and datapoints to alarm is the number of data points withing the evaluation period that must be breaching (don't have to be consecutive) to cause the alarm to go to the ALARM state.</li> <li>Is recommended to set alarms when a configuration change is made to: VPC internet gateway, VPC VPN customer gateway, ACLs, Security group, CloudTrail, Root account and VPC Flow Logs.</li> <li>You can also configure alarms to create OpsItems in Systems Manager Ops Center or create incidents in Systems Manager Incident Manager. These actions can be performed only when the alarm goes into ALARM state.</li> <li>For each alarm you create, you can specify how to treat missing data points as notBreaching, breaching, ignore (current alarm state is maintained) or missing (alarm transitions to INSUFFICIENT_DATA).</li> <li>CloudWatch sends events to Amazon EventBridge whenever a CloudWatch alarm changes alarm state. You can use EventBridge and these events to write rules that take actions, such as notifying you, when an alarm changes state. CloudWatch guarantees the delivery of alarm state change events to EventBridge.</li> <li>You can create an alarm for a custom metric before you've created that custom metric.</li> <li>You can disable and enable alarms.</li> <li>You can test an alarm by setting it to any state. This temporary state change lasts only until the next alarm comparison occurs.</li> <li>You can list any or all of the currently configured alarms, and list any alarms in a particular state.</li> <li>You can create up to 5000 alarms per region per account.</li> <li>You only can use ACII characters for alarm names.</li> </ul>"},{"location":"cloud/aws/cert-sap/#pricing_2","title":"Pricing","text":"<ul> <li>When you exceed three dashboards with up to 50 metrics.</li> <li>By ingesting and storing logs, as well as the amount of ingested logs scanned for each CloudWatch Insights query.</li> <li>Based on the number of custom events.</li> <li>Charges are also incurred when you monitor more than 10 custom metrics. Custom metrics can be metrics you create and also metrics from tools such as the CloudWatch agent.</li> <li>You are charged for each metric associated with a CloudWatch alarm.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-ec2-network-maximums-at-the-instance-level-to-be-monitored","title":"Amazon EC2 network maximums at the instance level to be monitored","text":"<ul> <li>Bandwidth capability: Each EC2 instance has a maximum bandwidth for aggregate inbound and outbound traffic, based on instance type and size. Amazon EC2 also has maximum bandwidth for traffic to AWS Direct Connect and the internet.</li> <li>Packet-per-second (PPS) performance: Each EC2 instance has a maximum PPS performance, based on instance type and size. When the limit is reached, AWS queues the extra packets for delivery at a late time. If the queue reaches its capacity, packets might be dropped.</li> <li>Connections tracked: The security group tracks each connection established to ensure that return packets are delivered as expected. There is a maximum number of connections that can be tracked per instance.</li> <li>Link-local service access: Amazon EC2 provides a maximum PPS per network interface for traffic to services such as the Domain Name System (DNS) service, the instance metadata service (IMDS), and the Amazon Time Sync Service.</li> </ul>"},{"location":"cloud/aws/cert-sap/#vpc-flow-logs","title":"VPC Flow Logs","text":"<ul> <li>VPC Flow Logs is a feature that lets you to capture information about the IP traffic going to and from network interfaces in your VPC.</li> <li>Flow logs can help you with a number of tasks, such as diagnosing overly restrictive security group rules, monitor the traffic that is reaching your instance and determining the direction of the traffic to and from the network interfaces.</li> <li>Levels of monitoring: VPC level, subnet level and network interface level.</li> <li>Logging parameter can be defined by choosing the traffic filter type (all, accept or rejected), log name, destination (S3 or CloudWatch) and necessary permissions (IAM role).</li> <li>Flows are collected, processed, and stored in capture windows that are approximately 10 minutes long. You can create up to two flow logs on one resource.</li> <li>When publishing to CloudWatch Logs, flow log data is published to a log group. Once the log group is created, the first flow records are visible in the console in about 15 minutes. Each network interface has a unique log stream in the log group.</li> <li>If the same network interface is present in one or more flow logs in the same log group, it has one combined log stream. If you've specified that one flow log should capture rejected traffic, and the other flow log should capture accepted traffic, then the combined log stream captures all traffic.</li> <li>When publishing to Amazon S3, flow log data is published to an existing Amazon S3 bucket that you specify using a folder structure that is determined by the flow log's ID, Region, and the date on which they are created.</li> <li>Flow log records for all of the monitored network interfaces are published to a series of log file objects that are stored in the bucket. If the flow log captures data for a VPC, the flow log publishes flow log records for all of the network interfaces in the selected VPC.</li> <li>Flow logs can quickly grow into the hundreds of gigabytes.</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases_2","title":"Use cases","text":"<ul> <li>Performance: provides flow duration, latency, and bytes sent and can be used to identify latencies, establish performance baselines, and improve applications.</li> <li>Security: can be used to log all traffic from an Amazon VPC, an interface, or a subnet for root cause analysis to identify gaps in your security. Identifying suspicious traffic tightens security and points to malicious traffic traversing your network.</li> <li>Compliance: can be used to show that your organization is compliant with specific industry, federal, state, and local regulations that your organization must follow. By directing logs into Amazon S3 to build a data lake, you can ensure that data is available for audits.</li> </ul>"},{"location":"cloud/aws/cert-sap/#anatomy-of-a-flow-log","title":"Anatomy of a flow log","text":"<ul> <li>Account number</li> <li>Network interface</li> <li>Source IP</li> <li>Target IP</li> <li>Source port</li> <li>Target port</li> <li>Protocol (6 is TCP)</li> <li>Packets</li> <li>Bytes</li> <li>Timestamp</li> <li>Timestamp</li> <li>Action (ACCEPT or REJECT)</li> </ul>"},{"location":"cloud/aws/cert-sap/#limitations_2","title":"Limitations","text":"<ul> <li>Changes cannot be made to the configuration of a flow log or the format of a flow log record after they have been created. If the flow log you have created is not gathering the data you expected or if the nature of what you need to gather changes, you have to delete the existing flow log and create a new one.</li> <li>Flow logs can only be configured for VPC peering connections deployed by your account. VPC peering connections deployed by another account cannot be monitored using VPC flow logs even if they have been authorized to link to VPCs within your account.</li> <li>Network interfaces for EC2-Classic instances are not supported. This includes instances linked to your VPC through ClassicLink.</li> <li>Sent to an EC2 with multiple IP addresses is recorded under the primary private IP address, listed in the dstaddr field, for the EC2 network interface. To separate log traffic by destination IP, configure the flow log to use the pkt-dstddr log field.</li> <li>Sent through an intermediate device, such as a NAT gateway, will record the IP of the intermediate device in the srcaddr field. To ensure that the original source IP address of device that generated the packet is recorded, configure the flow log to use the pkt-srcaddr field.</li> <li>Received from an intermediate device, such as a NAT gateway, will record the IP of the intermediate device in the dstaddr field. To ensure that the original destination IP address is recorded, configure the flow log to use the pkt-dstaddr field.</li> <li>Flow logs do not capture IP traffic to and from Amazon reserved IPs. This includes but is not limited to Amazon Domain Name Service (DNS), Amazon Windows license activation service, instance metadata, and Amazon Time Sync. The five IPv4 addresses reserved by Amazon for each subnet are also excluded from VPC flow logs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#traffic-mirroring","title":"Traffic Mirroring","text":"<ul> <li>You can use Traffic Mirroring to copy network traffic from an elastic network interface of an Amazon EC2 instance. In addition, you can send the traffic to out-of-band security and monitoring appliances for: Content inspection, Threat monitoring and Troubleshooting.</li> <li>You can choose to capture all traffic or you can use filters to capture the packets that are of particular interest to you, with an option to limit the number of bytes captured per packet.</li> <li>You can use VPC Traffic Mirroring in a multi-account AWS environment, capturing traffic from VPCs spread across many AWS accounts and then routing it to a central VPC for inspection.</li> <li>You can mirror traffic from any EC2 instance that is powered by the AWS Nitro system and 12 Xen-based instance types.</li> <li>You can use open-source tools such as Suricata and Zeek to monitor network traffic from an Amazon EC2 instance. These open-source tools support VXLAN decapsulation, and they can be used at scale to monitor Amazon VPC traffic.</li> <li>The following traffic types cannot be mirrored: ARP, DHCP, Instance metadata service, NTP and Windows activation.</li> </ul>"},{"location":"cloud/aws/cert-sap/#key-concepts","title":"Key concepts","text":"<ul> <li>Traffic mirror source: is the network interface of an Amazon EC2 instance where AWS copies the network traffic from. VPC Traffic Mirroring supports the use of Elastic Network Interfaces (ENIs) as mirror sources.</li> <li>Traffic mirror target: is the destination for mirrored traffic. The traffic mirror target can be a network interface or a network load balancer and used in more than one traffic mirror session.</li> <li>Traffic mirror filter: by default, no traffic is mirrored. To mirror traffic, add traffic mirror rules to the filter. Traffic mirror filter rules define what traffic gets mirrored. Rules are numbered and processed in order. The filter can specify a protocol, ranges for source and destination ports and CIDR block for the source and destination.</li> <li>Traffic mirror session: establishes a relationship between a traffic mirror source and a traffic mirror target that makes use of a traffic mirror filter. Traffic mirror sessions are evaluated based on the ascending session number that you define when you create the session. The first match (accept or reject) is used to determine the fate of the packet.</li> <li>Connectivity: the mirrored traffic is sent to the traffic mirror target using the source Amazon VPC route table. Before you configure Traffic Mirroring, make sure that the traffic mirror source can route to the traffic mirror target.</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases_3","title":"Use cases","text":"<ul> <li>Performance monitoring and network visibility to analyze specific traffic patterns to identify any vulnerable blind spots or choke points between application tiers or Amazon EC2 instances.</li> <li>Troubleshooting by extracting traffic of interest from any workload in an Amazon VPC and sending it to the right tools to detect and respond faster to attacks often missed by traditional log-centric tools. Analyzing the actual packets helps to perform a root-cause analysis on a performance issue and assist with diagnosis of network issues. This gives visibility beyond what is available through VPC Flow Logs.</li> <li>Security monitoring using packet inspection, signature analysis, anomaly detection, and machine learning based techniques provides further protection, threat prevention, and network forensics to your network traffic.</li> <li>Better response time using the right tools to detect, extract, and respond to traffic of interest from any workload in an Amazon VPC that is often missed by traditional log-centric tools.</li> <li>Replicate production traffic into develop environments.</li> </ul>"},{"location":"cloud/aws/cert-sap/#vpc-reachability-analyzer","title":"VPC Reachability Analyzer","text":"<ul> <li>The VPC Reachability Analyzer is a network diagnostics tool that troubleshoots reachability between two endpoints in an Amazon VPC, or within multiple Amazon VPCs.</li> <li>Specify the path of communication for the traffic from a source to a destination for any of the following endpoint types: VPN Gateways, Instances, Network Interfaces, Internet Gateways, VPC Endpoints, VPC Peering Connections, and Transit Gateways.</li> <li>The source and destination resources must be: owned b yhe same account, in the same region and in the same VPC or VPCs connected through VPC peering.</li> <li>You can choose to check for connectivity through either the TCP or UDP protocols. Optionally, you can also specify a port number, source, or destination IP address.</li> <li>Once the analysis completes, you can se if the destination is reachable or not reachable (provides one or mode explanation codes to help diagnose).</li> <li>To resolve automatically, you can configure VPC Reachability Analyzer with CloudWatch to alert on connectivity issues and possibly automatically remediate using AWS Lambda.</li> <li>With VPC Reachability Analyzer, you are charged per analysis run between a source and a destination. It is best practice to run analysis during networking configuration changes and to troubleshoot connectivity issues that arise. Optionally, you can tag your analysis to keep track of the cost.</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases_4","title":"Use cases","text":"<ul> <li>Troubleshooting connectivity issues caused by network misconfiguration.</li> <li>Verifying that your network configuration matches your intended connectivity.</li> <li>Automating the verification of your connectivity intent as your network configuration changes.</li> <li>Determining whether a destination resource in your Amazon VPC is reachable from a source resource. </li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-transit-gateway-network-manager","title":"AWS Transit gateway Network Manager","text":"<ul> <li>The AWS Transit Gateway Network Manager lets you to centrally manage your networks that are built around transit gateways. You can visualize and monitor your global network across Regions and on-premises locations.</li> <li>There is no additional cost for using Network Manager.</li> </ul>"},{"location":"cloud/aws/cert-sap/#steps-to-configure","title":"Steps to configure","text":"<ul> <li>Create a global network.</li> <li>Register your transit gateways (automatically includes transit gateway attachments like the VPCs, Site-to-Site VPN connections ,AWS Direct Connect gateways, Transit Gateway Connect and Transit gateway peering connections, but not its attachments)</li> <li>Define and add devices, links, and sites for your on-premises resources to your global network.</li> <li>Analyze your network wit the Route Analyzer.</li> <li>Monitor your networks through a dashboard on the Network Manager console. You can view network activity and health using CloudWatch metrics and CloudWatch Events.</li> </ul>"},{"location":"cloud/aws/cert-sap/#section-pages","title":"Section pages","text":"<ul> <li>Overview: you can view the inventory of your global network and a list of transit gateways that are registered.</li> <li>Details: you can view information about the global network resource.</li> <li>Geographic: you can view the locations of the resources that are registered in your global network on a map. Lines on the map represent connections between the resources, and the line colors represent the type of connection and their state. You can choose any of the location points to view information about the resources in that location.</li> <li>Topology: you can view the network tree for your global network.</li> <li>Events: you can view the system events that describe changes in your global network.</li> <li>Monitoring: you can view CloudWatch metrics for the transit gateways, VPN connections, and on-premises resources in your global network.</li> <li>Route Analyzer: you can perform an analysis of the routes in your transit gateway route tables.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cloudtrail","title":"CloudTrail","text":"<ul> <li>AWS service that logs all API actions in your account. CloudTrail maintains the audit logs of changes to the AWS account.</li> <li>Management Events: By default, CloudTrail only logs management events such as creating an Amazon EC2 instance and an Amazon VPC. It provides information about management operations.</li> <li>Data Events: By default, CloudTrail only logs management events because data events occur more often. Data events are the resource operations in a resource such as AWS Lambda functions or objects uploaded to Amazon S3.</li> <li>CloudTrail does not log in real time. There is a delay, but you can create a CloudTrail and store that data in Amazon S3 or CloudWatch logs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cots-tools","title":"COTS Tools","text":"<ul> <li>COTS network monitoring tools are available under license. The AWS Marketplace has third-party COTS network monitoring tools available for deployment within a customer's AWS environment.  Some have a licensing fee, some do not.</li> </ul>"},{"location":"cloud/aws/cert-sap/#iperf-and-iperf3","title":"iPerf and iPerf3","text":"<ul> <li>Tools for active measurements of the maximum achievable bandwidth on IP networks. It supports tuning of various parameters related to timing, buffers, and protocols (TCP, UDP, SCTP with IPv4 and IPv6). For each test, they report the bandwidth, loss, and other parameters. iPerf3 is a new implementation that shares no code with the original iPerf and also is not backwards compatible.</li> </ul>"},{"location":"cloud/aws/cert-sap/#extrahop","title":"ExtraHop","text":"<ul> <li>ExtraHop is a monitoring solution for security, network performance, and the cloud. It gives detailed metrics on average bandwidth utilization, average throughput, and more.</li> </ul>"},{"location":"cloud/aws/cert-sap/#netperf","title":"Netperf","text":"<ul> <li>Netperf is a CLI tool similar to iPerf that measures throughput and benchmarking speeds.</li> </ul>"},{"location":"cloud/aws/cert-sap/#compute","title":"Compute","text":""},{"location":"cloud/aws/cert-sap/#choose-options","title":"Choose options","text":"<ul> <li>Main focus is time to market.</li> <li>Scalability is important but not the main priority.</li> <li>For startups starting from scratch Serverless with SAM is the best option to focus only on code.</li> <li>For teams that already have containers ECS or EKS running on Fargate.</li> <li>For a team with an existing monolithic app the recommended solution is Elastic Beanstalk.</li> </ul>"},{"location":"cloud/aws/cert-sap/#ec2","title":"EC2","text":"<ul> <li>Allows to have a custom launch script.</li> <li>Provision and launch one or more EC2 instances in minutes.</li> <li>Stop or shut down EC2 instances when you finish running a workload.</li> <li>Pay by the hour or second (depending on the type of instance) for each instance type (minimum of 60 seconds).</li> <li>Hardware specifications: CPU, memory, network, and storage</li> <li>Logical configurations: Networking location, firewall rules, authentication, and the operating system of your choice</li> <li>When launching an EC2 instance, the first setting you configure is which operating system you want by selecting an Amazon Machine Image (AMI).</li> <li>An AMI includes the operating system, storage mapping, architecture type, launch permissions, and any additional preinstalled software applications.</li> <li>You can create an AMI from your running instance and use the AMI to start a new instance.</li> <li>AMI origins: Quick Start AMIs, AWS Marketplace AMIs, My AMIs, Community AMIs and Custom image.</li> <li>Each AMI in the AWS Management Console has an AMI ID, which is prefixed by ami-, followed by a random hash of numbers and letters. The IDs are unique to each AWS Region.</li> <li>The default VPC is suitable for getting started quickly and launching public EC2 instances without having to create and configure your own VPC.</li> <li>When architecting any application for high availability, consider using at least two EC2 instances in two separate Availability Zones.</li> <li>Anything you can run on a physical server can be run on Amazon EC2.  Amazon EC2 gives you access to the OS and to the underlying files and can scale out and in as necessary.</li> </ul>"},{"location":"cloud/aws/cert-sap/#instance-types","title":"Instance types","text":"<p>EC2 instances are a combination of virtual processors (vCPUs), memory, network, and, in some cases, instance storage and graphics processing units (GPUs).</p> <ul> <li>The first position indicates the instance family.</li> <li>The second position indicates the generation of the instance.</li> <li>The remaining letters before the period indicates additional attributes.</li> <li>After the period indicates the instance size.</li> </ul> Family Description Use cases General purpose (t/m) General purpose instances provide a balance of compute, memory, and networking resources, and can be used for a variety of workloads. Ideal for applications that use these resources in equal proportions, such as web servers and code repositories Compute optimized (c) Compute optimized instances are ideal for compute-bound applications that benefit from high-performance processors. Well-suited for batch processing workloads, media transcoding, high performance web servers, high performance computing (HPC), scientific modeling, dedicated gaming servers and ad server engines, machine learning inference, and other compute intensive applications Memory optimized (r/u/x) Memory optimized instances are designed to deliver fast performance for workloads that process large datasets in memory. Memory-intensive applications, such as high-performance databases, distributed web-scale in-memory caches, mid-size in-memory databases, real-time big-data analytics, and other enterprise applications. Accelerated computing (p/g/trn) Accelerated computing instances use hardware accelerators or co-processors to perform functions such as floating-point number calculations, graphics processing, or data pattern matching more efficiently than is possible in software running on CPUs. Machine learning, HPC, computational fluid dynamics, computational finance, seismic analysis, speech recognition, autonomous vehicles, and drug discovery. Storage optimized (i/d) Storage optimized instances are designed for workloads that require high sequential read and write access to large datasets on local storage. They are optimized to deliver tens of thousands of low-latency random I/O operations per second (IOPS) to applications that replicate their data across different instances. NoSQL databases (Cassandra, MongoDB and Redis), in-memory databases, scale-out transactional databases, data warehousing, Elasticsearch, and analytics. HPC optimized (hpc) High performance computing (HPC) instances are purpose built to offer the best price performance for running HPC workloads at scale on AWS. Ideal for applications that benefit from high-performance processors, such as large, complex simulations and deep learning workloads"},{"location":"cloud/aws/cert-sap/#lifecycle","title":"Lifecycle","text":"<pre><code>stateDiagram\n    [*] --&gt; Pending : Launch\n    Pending --&gt; Running : Start\n    Running --&gt; Rebooting : Reboot\n    Rebooting --&gt; Running : Complete\n    Running --&gt; Stopping : Stop\n    Running --&gt; ShuttingDown : Terminate\n    Stopping --&gt; Stopped : Complete\n    Stopped --&gt; Pending : Start\n    ShuttingDown --&gt; Terminated : Complete\n    Stopped --&gt; Terminated : Terminate\n</code></pre> <ul> <li>When an instance is pending, billing has not started.</li> <li>Rebooting keeps its public DNS name (IPv4) and private and public IPv4 addresses.</li> <li>When you stop and start an instance, your instance can be placed on a new underlying physical server.</li> <li>Termination of an instance means that you can no longer access the machine.</li> <li>As soon as the status of an instance changes to shutting down or terminated, you stop incurring charges for that instance.</li> <li>When you stop your instance, the data from the instance memory (RAM) is lost.</li> <li>When you stop-hibernate an instance, Amazon EC2 signals the operating system to perform hibernation (suspend-to-disk), which saves the contents from the instance memory (RAM) to the EBS root volume. </li> </ul>"},{"location":"cloud/aws/cert-sap/#pricing_3","title":"Pricing","text":"<ul> <li>On-demand: pay as you go, recommended for unpredicted load and test.</li> <li>Spot: for flexible start and end times, recommended for stateless workloads.</li> <li>Saving plan: flexible pricing model that offers low usage prices for a 1-year or 3-year term commitment to a consistent amount of usage.</li> <li>Reserved: for applications with steady state usage that might require reserved capacity, can be standard, convertible or scheduled.</li> <li>Dedicated host: physical Amazon EC2 server that is dedicated for your use, can be purchased hourly or as reservation.</li> </ul>"},{"location":"cloud/aws/cert-sap/#auto-scaling","title":"Auto Scaling","text":"<ul> <li>Vertical scaling: Increase the instance size in active-passive system.</li> <li>Horizontal scaling: Add additional instances in active-active system.</li> <li>Automatic scaling based on demand.</li> <li>Scheduled scaling based on user-defined schedules.</li> <li>Fleet management automatically replacing unhealthy EC2 instances.</li> <li>Predictive scaling using ML to help schedule optimum number of instances.</li> <li>ELB integrates seamlessly with EC2 Auto Scaling.</li> </ul>"},{"location":"cloud/aws/cert-sap/#configuring-components","title":"Configuring components","text":""},{"location":"cloud/aws/cert-sap/#launch-template-or-configuration","title":"Launch template or configuration","text":"<ul> <li>Recommended over launch configuration (you can not use previously created launch configurations as a template or can create one from an existing instance).</li> <li>Can create one from an existing instance, from an existing template or previous version, or define a template from scratch (AMI, instance type, key pair, security group, storage, resource tags)</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-ec2-auto-scaling-groups","title":"Amazon EC2 Auto Scaling groups","text":"<ul> <li>You specify the VPC and subnets where instances should be launched.</li> <li>You can specify the type of purchase (on-demand, spot or combination of the two).</li> <li>You configure the minimum, desired and maximum capacity.</li> </ul>"},{"location":"cloud/aws/cert-sap/#scaling-policies","title":"Scaling policies","text":"<ul> <li>Simple scaling policy: Use CloudWatch alarm to scale based on number or percentage. Has a cooldown before taking any other action.</li> <li>Step scaling policy: Respond to additional alarms even when scaling activity is in progress.</li> <li>Target tracking policy: You define the target value to track and it automatically creates the required CloudWatch alarms.</li> </ul>"},{"location":"cloud/aws/cert-sap/#ec2-image-builder","title":"EC2 Image builder","text":"<ul> <li>Used to build custom EC2 images.</li> </ul>"},{"location":"cloud/aws/cert-sap/#ecs","title":"ECS","text":"<ul> <li>End-to-end container orchestration service that helps you spin up new containers.</li> <li>Your containers are defined in a task definition that you use to run an individual task or a task within a service.</li> <li>You have the option to run your tasks and services on a serverless infrastructure that's managed by another AWS service called AWS Fargate.</li> <li>For more control over your infrastructure, you can run your tasks and services on a cluster of EC2 instances that you manage.</li> <li>You need to install the Amazon ECS container agent on your EC2 instances.</li> <li>To prepare your application to run on Amazon ECS, you create a task definition.</li> <li>Recommended for large monolithic applications that you want to break into container or move directly without any change.</li> </ul>"},{"location":"cloud/aws/cert-sap/#eks","title":"EKS","text":"<ul> <li>If you already use Kubernetes, you can use Amazon EKS to orchestrate the workloads in the AWS Cloud.</li> <li>The machine that runs the containers is called a worker node or Kubernetes node.</li> <li>An EKS container is called a pod.</li> <li>Recommended for large monolithic applications that you want to break into container or move directly without any change.</li> <li>EKS standard: AWS manages the Kubernetes control plane when you create a cluster with EKS. Components that manage nodes, schedule workloads, integrate with the AWS cloud, and store and scale control plane information to keep your clusters up and running, are handled for you automatically.</li> <li>EKS Auto Mode: Using the EKS Auto Mode feature, EKS extends its control to manage Nodes (Kubernetes data plane) as well. It simplifies Kubernetes management by automatically provisioning infrastructure, selecting optimal compute instances, dynamically scaling resources, continuously optimizing costs, patching operating systems, and integrating with AWS security services.</li> </ul>"},{"location":"cloud/aws/cert-sap/#components_5","title":"Components","text":""},{"location":"cloud/aws/cert-sap/#pods","title":"Pods","text":"<ul> <li>Its best practice to run a single container in a pod.</li> <li>Multiple container in the same pod share the same IP.</li> <li>Pods are immutable.</li> <li>Worker nodes are where we run our apps.</li> <li>The control plane needs scaling fo high availability.</li> </ul>"},{"location":"cloud/aws/cert-sap/#control-plane","title":"Control plane","text":"<ul> <li>Four main control plane components: kube-apiserver, etcd, kube-scheduler and kube-controller-manager.</li> <li>If you are using the cloud you will need to use the cloud-controller-manager.</li> <li>You can run control plane nodes across multiple machines.</li> </ul>"},{"location":"cloud/aws/cert-sap/#namespaces","title":"Namespaces","text":"<ul> <li>Helps to isolate groups of resources in a cluster.</li> </ul>"},{"location":"cloud/aws/cert-sap/#deployments","title":"Deployments","text":"<ul> <li>Manager ReplicaSets (list the number of each pod to run) and provides declarative updates.</li> </ul>"},{"location":"cloud/aws/cert-sap/#services","title":"Services","text":"<ul> <li>Services allow you to group pods together and expose them over a network.</li> <li>Tou can create a service for pods in a cluster and outside of the cluster.</li> <li>Services are often used alongside Ingresses.</li> </ul>"},{"location":"cloud/aws/cert-sap/#ingress","title":"Ingress","text":"<ul> <li>Without Ingress, we need a LB per service.</li> <li>Uses port 80 or 443 and host-based and path-based routing.</li> <li>Two core components, the controller (implements the rules) and the object spec (defines rules to govern traffic).</li> <li>Operates at layer 7.</li> <li>Insects HTTP headers and forwards traffic based on hostnames and paths.</li> </ul>"},{"location":"cloud/aws/cert-sap/#fargate","title":"Fargate","text":"<ul> <li>AWS Fargate is a purpose-built serverless compute engine for containers.</li> <li>Supports both Amazon ECS and Amazon EKS architecture and provides workload isolation and improved security by design.</li> </ul>"},{"location":"cloud/aws/cert-sap/#lambda","title":"Lambda","text":"<ul> <li>Lambda runs your code on a high availability compute infrastructure and requires no administration from the user.</li> <li>You have the option of configuring your Lambda functions using the Lambda console, Lambda API, AWS CloudFormation, or AWS Serverless Application Model (AWS SAM).</li> <li>You can create a function from scratch, blueprint, select a container image or browse the AWS Serverless Application Repository.</li> <li>You can deploy by zip or by container.</li> <li>You define the execution parameters, such as memory, timeout and concurrency.</li> <li>The runtime provides a language-specific environment that runs in an application environment.</li> <li>The AWS Lambda function handler is the method in your function code that processes events.</li> <li>The handler method takes two objects: the event object and the context object. </li> <li>Billing is rounded up to the nearest 1 millisecond (ms). It can be cost effective to run functions whose execution time is very low.</li> <li>Price depends on the amount of memory you allocate to your function, not the amount of memory your function uses.</li> <li>Is a suitable choice for any short-lived application that can finish running in under 15 minutes.</li> <li>Great option for even-driven applications.</li> <li>If is compute intensive consider using a container instead of lambda.</li> <li>Supported languages: Node, Python, Java, Go, C#, Ruby, PowerShell.</li> <li>AWS provided plugins for a number of popular IDEs.</li> <li>The AWS Lambda Free Tier includes 1 million free requests per month and 400,000 GB-seconds of compute time per month.</li> </ul>"},{"location":"cloud/aws/cert-sap/#invocation-models","title":"Invocation models","text":""},{"location":"cloud/aws/cert-sap/#synchronous-invocation","title":"Synchronous invocation","text":"<ul> <li>Runs the function and waits for a response.</li> <li>Synchronous events expect an immediate response from the function.</li> <li>There are no built-in retries. You must manage your retry strategy within your application code.</li> <li>Synchronous services: API Gateway, Cognito, CloudFormation, Alexa, Lex, CloudFront.</li> </ul>"},{"location":"cloud/aws/cert-sap/#asynchronous-invocation","title":"Asynchronous invocation","text":"<ul> <li>Events are queued and the requestor doesn't wait for the function to complete.</li> <li>With the asynchronous model, you can make use of destinations. Use destinations to send records of asynchronous invocations to other services.</li> <li>There is a built in retries twice.</li> <li>A destination can send records of asynchronous invocations to other services.</li> <li>You can configure separate destinations for events that fail processing and for events that process successfully.</li> <li>You can configure destinations on a function, a version, or an alias, similarly to how you can configure error handling settings.</li> <li>Asynchronous services: SNS, S3, EventBridge.</li> </ul>"},{"location":"cloud/aws/cert-sap/#polling-invocation","title":"Polling invocation","text":"<ul> <li>Lambda will poll (or watch) these services, retrieve any matching events, and invoke your functions.</li> <li>With this model, the retry behavior varies depending on the event source and its configuration.</li> <li>The configuration of services as event triggers is known as event source mapping. This process occurs when you configure event sources to launch your Lambda functions and then grant theses sources IAM permissions to access the Lambda function.</li> <li>Polling services: Kinesis, SQS, Amazon MQ, Managed Streaming for Apache Kafka (MSK), self-managed Kafka, DynamoDB Streams.</li> </ul>"},{"location":"cloud/aws/cert-sap/#lambda-execution-environment","title":"Lambda execution environment","text":""},{"location":"cloud/aws/cert-sap/#init-phase","title":"Init phase","text":"<ul> <li>Lambda creates or unfreezes an execution environment with the configured resources, downloads the code for the function and all layers, initializes any extensions, initializes the runtime, and then runs the function\u2019s initialization code (the code outside the main handler).</li> <li>The Init phase happens either during the first invocation, or before function invocations if you have enabled provisioned concurrency.</li> <li>Three sub-phases: extension init (starts all extensions), runtime init (bootstraps the runtime) and function init (runs the function static code)</li> </ul>"},{"location":"cloud/aws/cert-sap/#invoke-phase","title":"Invoke phase","text":"<ul> <li>Lambda invokes the function handler. After the function runs to completion, Lambda prepares to handle another function invocation.</li> </ul>"},{"location":"cloud/aws/cert-sap/#shutdown-phase","title":"Shutdown phase","text":"<ul> <li>If the Lambda function does not receive any invocations for a period of time, this phase initiates.</li> <li>Lambda shuts down the runtime, alerts the extensions to let them stop cleanly, and then removes the environment.</li> <li>Lambda sends a shutdown event to each extension, which tells the extension that the environment is about to be shut down.</li> </ul>"},{"location":"cloud/aws/cert-sap/#design-best-practices","title":"Design best practices","text":"<ul> <li>Separate business logic: separate your core business logic from the handler event. This makes your code more portable and you can target unit-tests on your code without worrying about the configuration of the function.</li> <li>Write modular functions: module functions will reduce the amount of time that it takes for your deployment package to be downloaded and unpacked before invocation.</li> <li>Treat functions as stateless: if you need to store state data consider using DynamoDB, ElastiCache or S3.</li> <li>Only include what you need: minimize both your deployment package dependencies and its size. Choose the modules that you need instead of the full AWS SDK. In TypeScript bundle and tree shaking dependencies. In Java use simple DI like Dagger or Guice over Spring Framework and put your dependency .jar files in a separate /lib directory.</li> </ul>"},{"location":"cloud/aws/cert-sap/#best-practices-for-writing-code","title":"Best practices for writing code","text":"<ul> <li>Include logging statements.</li> <li>Use return coding.</li> <li>Provide environment variables.</li> <li>Add secret and reference data. AWS Secrets Manager helps you organize and manage important configuration data such as credentials, passwords, and license keys. Parameter Store, a capability of AWS Systems Manager, is integrated with Secrets Manager so you can retrieve Secrets Manager secrets when using AWS Lambda. By using Parameter Store to reference Secrets Manager secrets, you create a consistent and secure process for calling and using secrets and reference data in your code and configuration scripts. Parameter Store also integrates with IAM, giving you fine-grained access control to individual parameters or branches of a hierarchical tree.</li> <li>Avoid recursive code. If you accidentally deploy recursive code, you can quickly set the concurrent execution limit to zero by using the console or command line to immediately throttle requests while you fix the code.</li> <li>Gather metric with CloudWatch.</li> <li>Reuse execution context (store dependencies locally, limit re-initialization of variable, reuse connections, use tmp space ad check that background process have completed).</li> </ul>"},{"location":"cloud/aws/cert-sap/#performance-optimization","title":"Performance optimization","text":"<ul> <li>A cold start occurs when a new execution environment is required to run a Lambda function.</li> <li>In a warm start, the Lambda service retains the environment instead of destroying it immediately. This allows the function to run again within the same execution environment. This saves time by not needing to initialize the environment.</li> <li>Billing starts after the runtime has been initialized.</li> <li>After optimizing your function, another way to minimize cold starts is to use provisioned concurrency. Provisioned concurrency is a Lambda feature that prepares concurrent execution environments before invocations.</li> <li>If you need predictable function start times for your workload, provisioned concurrency ensures the lowest possible latency. This feature keeps your functions initialized and warm, and ready to respond in double-digit milliseconds at the scale you provision.</li> </ul>"},{"location":"cloud/aws/cert-sap/#best-practice-write-functions-to-take-advantage-of-warm-starts","title":"Best practice: Write functions to take advantage of warm starts","text":"<ul> <li>Store and reference dependencies locally.</li> <li>Limit re-initialization of variables.</li> <li>Add code to check for and reuse existing connections.</li> <li>Use tmp space as transient cache.</li> <li>Check that background processes have completed.</li> </ul>"},{"location":"cloud/aws/cert-sap/#permissions","title":"Permissions","text":"<ul> <li>Permissions to invoke the function are controlled using an IAM resource-based policy. An IAM execution role defines the permissions that control what the function is allowed to do when interacting with other AWS services.</li> </ul>"},{"location":"cloud/aws/cert-sap/#resource-based-policy","title":"Resource-based policy","text":"<ul> <li>A resource policy (also called a function policy) tells the Lambda service which principals have permission to invoke the Lambda function.</li> <li>An AWS principal may be a user, role, another AWS service, or another AWS account.</li> <li>A consideration with cross-account permissions is that a resource policy does have a size limit. If you have many different accounts that need to invoke the function and you have to add permissions for each account via the resource policy, you might reach the policy size limit. In that case, you would need to use IAM roles instead of resource policies.</li> </ul>"},{"location":"cloud/aws/cert-sap/#execution-role","title":"Execution role","text":"<ul> <li>The execution role gives your function permissions to interact with other services. You provide this role when you create a function, and Lambda assumes the role when your function is invoked.</li> <li>The role must include a trust policy that allows Lambda to \u201cAssumeRole\u201d so that it can take that action for another service.</li> <li>You can also use (opens in a new tab)IAM Access Analyzer to help identify the required permissions for the IAM execution role. IAM Access Analyzer reviews your AWS CloudTrail logs over the date range that you specify and generates a policy template with only the permissions that the function used during that time.</li> </ul>"},{"location":"cloud/aws/cert-sap/#accessing-resources-in-a-vpc","title":"Accessing resources in a VPC","text":"<ul> <li>Enabling your Lambda function to access resources inside your VPC requires additional VPC-specific configuration information, such as VPC subnet IDs and security group IDs.</li> <li>To establish a private connection between your VPC and Lambda, create an interface VPC endpoint via PrivateLink.</li> </ul>"},{"location":"cloud/aws/cert-sap/#building-lambda-functions","title":"Building Lambda functions","text":""},{"location":"cloud/aws/cert-sap/#lambda-console-editor","title":"Lambda console editor","text":"<ul> <li>If your code does not require custom libraries (other than the AWS SDK), you can edit your code inline through the console.</li> <li>The Lambda console editor is based on the AWS Cloud9 IDE where you can author and test code directly.</li> <li>When working with Lambda via the console, note that when you save your Lambda function the Lambda service creates a deployment package that it can run.</li> </ul>"},{"location":"cloud/aws/cert-sap/#deployment-packages","title":"Deployment packages","text":"<ul> <li>Your Lambda function's code consists of scripts or compiled programs and their dependencies.</li> <li>Lambda supports two types of deployment packages: container images and .zip file archives.</li> <li>You can create and upload a .zip file to S3 or use a container image and push to Amazon Elastic Container Registry (Amazon ECR).</li> </ul>"},{"location":"cloud/aws/cert-sap/#automate-using-tools","title":"Automate using tools","text":"<ul> <li>You can automate the deployment process of your applications by using AWS SAM and other AWS services, such as AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-sam","title":"AWS SAM","text":"<ul> <li>Open-source framework for building serverless applications.</li> <li>AWS SAM transforms and expands the AWS SAM syntax into AWS CloudFormation syntax.</li> <li>A variety of serverless frameworks are available.</li> <li>AWS SAM provides a number of predefined, commonly used templates that you can use to build for least privilege security access.</li> <li>For a Lambda function, AWS SAM scopes the permissions of your Lambda functions to the resources used by your application.</li> <li>You can add IAM policies as part of the AWS SAM template. The policies property can be the name of AWS managed policies, inline IAM policy documents, or AWS SAM policy templates.</li> <li>AWS SAM CLI launches a Docker container that you can interact with to test and debug your Lambda functions. Note that even with a tool like AWS SAM CLI, local testing will only cover a subset of what must be tested before code should go into your production application.</li> <li>Testing: invoke functions and run automated tests locally, generate sample event source payloads, run api gateway locally, debug code, review logs and validate SAM templates.</li> <li>Commands: init, local, validate, deploy and build.</li> </ul>"},{"location":"cloud/aws/cert-sap/#configuring-your-lambda-functions","title":"Configuring your Lambda functions","text":"<ul> <li>Depending on the function, you might find that the higher memory level might actually cost less because the function can complete much more quickly than at a lower memory configuration.</li> <li>You can use an open-source tool called Lambda Power Tuning to find the best configuration for a function. Powered by AWS Step Functions, supports three optimization strategies: cost, speed, and balanced. It's language-agnostic.</li> </ul>"},{"location":"cloud/aws/cert-sap/#memory","title":"Memory","text":"<ul> <li>You can allocate up to 10 GB of memory.</li> <li>Lambda allocates CPU and other resources linearly in proportion to the amount of memory configured.</li> <li>Because Lambda charges are proportional to the memory configured and function duration (GB-seconds), the additional costs for using more memory may be offset by lower duration.</li> </ul>"},{"location":"cloud/aws/cert-sap/#timeout","title":"Timeout","text":"<ul> <li>The maximum timeout for a Lambda function is 900 seconds.</li> <li>There are many cases when an application should fail fast and not wait for the full timeout value.</li> <li>It is important to analyze how long your function runs.</li> <li>Load testing your Lambda function is the best way to determine the optimum timeout value.</li> <li>Avoiding lengthy timeouts for functions can prevent you from being billed while a function is simply waiting to time out.</li> </ul>"},{"location":"cloud/aws/cert-sap/#concurrency-and-scaling","title":"Concurrency and scaling","text":"<ul> <li>Concurrency is the number of invocations your function runs at any given moment.</li> <li>Concurrency = request rate * average duration, example: 25 requests per second * 10 seconds of duration = 250, if limits are lower, then calls are throttled.</li> <li>Function concurrency of 0 halts all invocations (emergency stop).</li> <li>Default regional account level limit is 1000. You can increase the limit by requests.</li> <li>You limit concurrency to limit costs, regulate how long it takes to process batch events or match with a downstream resource that cannot scale as quickly as Lambda.</li> <li>You reserve concurrency to ensure that you can handle peak expected volume for a critical function or address invocation errors.</li> <li>The burst concurrency quota is not per function. It applies to all of your functions in the Region. Differ by region: 3000 for US West (Oregon), US East (N. Virginia), Europe (Ireland), 1000 for Asia Pacific (Tokyo), Europe (Frankfurt), US East (Ohio) and 500 for other regions.</li> <li>After the initial burst, your functions' concurrency can scale by an additional 500 instances each minute. This continues until there are enough instances to serve all requests, or until a concurrency limit is reached.</li> <li>CloudWatch includes two built-in metrics that help determine concurrency: ConcurrentExecutions and UnreservedConcurrentExecutions.</li> </ul>"},{"location":"cloud/aws/cert-sap/#unreserved-concurrency","title":"Unreserved concurrency","text":"<ul> <li>The amount of concurrency that is not allocated to any specific set of functions.</li> <li>The minimum is 100 unreserved concurrency.</li> </ul>"},{"location":"cloud/aws/cert-sap/#reserved-concurrency","title":"Reserved concurrency","text":"<ul> <li>Guarantees the maximum number of concurrent instances for the function.</li> <li>When a function has reserved concurrency, no other function can use that concurrency.</li> <li>No charge is incurred for configuring reserved concurrency for a function.</li> </ul>"},{"location":"cloud/aws/cert-sap/#provisioned-concurrency","title":"Provisioned concurrency","text":"<ul> <li>Initializes a requested number of runtime environments so that they are prepared to respond immediately to your function's invocations.</li> <li>This option is used when you need high performance and low latency.</li> <li>You pay for the amount of provisioned concurrency that you configure and for the period of time that you have it configured.</li> <li>For example, you might want to increase provisioned concurrency when you are expecting a significant increase in traffic.</li> </ul>"},{"location":"cloud/aws/cert-sap/#versions-and-aliases","title":"Versions and aliases","text":"<ul> <li>Versioning: You can use versions to manage the deployment of your functions. Lambda creates a new version of your function each time that you publish the function. When you create a Lambda function, only one version exists, which is identified by $LATEST at the end of the Amazon Resource Name (ARN).</li> <li>Publish: Publish makes a snapshot copy of $LATEST. Enable versioning to create immutable snapshots of your function every time you publish it. Publish as many versions as you need. Each version results in a new sequential version number. Add the version number to the function ARN to reference it. The snapshot becomes the new version and is immutable.</li> <li>Aliases: A Lambda alias is like a pointer to a specific function version. You can access the function version using the alias ARN. Each alias has a unique ARN. An alias can point only to a function version, not to another alias. You can update an alias to point to a new version of the function.</li> <li>You can also use routing configuration on an alias to send a portion of traffic to a second function version.</li> <li>You can point an alias to a maximum of two Lambda function versions. Both versions must have the same runtime role, same dead-letter queue configuration or no dead-letter queue configuration, be published and the alias cannot point to $LATEST.</li> </ul>"},{"location":"cloud/aws/cert-sap/#monitoring-and-troubleshooting_1","title":"Monitoring and Troubleshooting","text":"<ul> <li>Lambda automatically tracks the following: Number of requests, Invocation duration per request and Number of requests that result in an error.</li> <li>Built in metrics: invocations, duration, errors, throttles (number of times that a process failed because of concurrency limits), iteratorAge (amount of time between when the stream receives the record and when the event source mapping sends the event to the function), deadLetterErrors, concurrentExecutions.</li> <li>The Lambda Insights dashboard has two views in the CloudWatch console: the multi-function overview and the single-function view.</li> <li>The multi-function overview aggregates the runtime metrics for the Lambda functions in the current AWS account and Region.</li> <li>The single-function view shows the available runtime metrics for a single Lambda function.</li> <li>You can use AWS X-Ray to visualize the components of your application, identify performance bottlenecks, and troubleshoot requests that resulted in an error. Your Lambda functions send trace data to X-Ray, and X-Ray processes the data to generate a service map and searchable trace summaries.</li> <li>Dead-letter queues help you capture application errors that must receive a response.</li> </ul>"},{"location":"cloud/aws/cert-sap/#serverless-architecture","title":"Serverless architecture","text":""},{"location":"cloud/aws/cert-sap/#migration-patterns","title":"Migration patterns","text":"<ul> <li>Leapfrog: go straight from an on-premises legacy architecture to a serverless cloud architecture.</li> <li>Organic: you move on-premises applications to the cloud in more of a lift-and-shift model (EC2 or EKS). Then experiment with serverless, adopt and take strategic decisions.</li> <li>Strangler: incrementally and systematically decomposes monolithic applications by creating APIs and building event-driven components that gradually replace components of the legacy application. New feature branches can be serverless first, and legacy components can be decommissioned as they are replaced.</li> </ul>"},{"location":"cloud/aws/cert-sap/#migration-considerations","title":"Migration considerations","text":"<ul> <li>Establish what part of the total application each microservice will represent. Then pull the associated data into its own data store. Teams have to agree on the boundaries of each domain, what is shared, who owns, and who uses the data.</li> <li>Break up your data based on the command query responsibility segregation, or CQRS, pattern.</li> <li>Design the data stores that match how the data will be used and stop thinking in terms of a single, general purpose database.</li> <li>Tasks or code that uses cron jobs are good targets to replace with Lambda functions. You can use Amazon CloudWatch Events as triggers for Lambda functions, or create an EventBridge rule that runs a Lambda function on a schedule.</li> <li>Workers listening to a queue may be an easy place to introduce SQS queue without requiring a lot of untangling of the existing code.</li> <li>You can use both Application Load Balancer and API Gateway to direct traffic to different targets to easily incorporate new components without disrupting the existing system.</li> <li>You can use API Gateway stages to maintain multiple versions of an API.</li> <li>A steady stream of traffic may cost less with ALB, but if request patterns are very spiky, API Gateway may be more cost effective.</li> </ul>"},{"location":"cloud/aws/cert-sap/#fargate-or-lambda","title":"Fargate or Lambda","text":""},{"location":"cloud/aws/cert-sap/#fargate_1","title":"Fargate","text":"<ul> <li>Lift and shift with minimal rework</li> <li>Long running processes</li> <li>Predictable and consistent workload</li> <li>Need more than 3GB of memory</li> <li>Application with a non-http/s listener</li> <li>Run side cars with your service</li> <li>Container image portability with Docker runtime</li> </ul>"},{"location":"cloud/aws/cert-sap/#lambda_1","title":"Lambda","text":"<ul> <li>Task that run less than 15 minutes</li> <li>Spiky, unpredictable workloads</li> <li>Unknown demand</li> <li>Light-weight application focused in stateless computing</li> <li>Simplified IT automation</li> <li>Real-time data processing</li> <li>Reduced complexity form development and operations</li> </ul>"},{"location":"cloud/aws/cert-sap/#serverless-it-automation","title":"Serverless IT automation","text":"<ul> <li>A powerful IT automation pattern is to trigger a Lambda function that assesses whether a configuration change is allowed and deletes the change automatically if it is not allowed.</li> </ul>"},{"location":"cloud/aws/cert-sap/#serverless-web-applications-and-mobile-apps","title":"Serverless web applications and mobile apps","text":"<ul> <li>A common event-driven pattern forms the basic backbone of a serverless web application architecture using Amazon API Gateway to handle HTTP requests, Lambda to provide the application layer, and Amazon DynamoDB to provide database functionality. Also add Cognito, S3 and SQS. The SPA can be published using S3. Use CloudFront as an entry point to serve static data. Use AWS AppSync for GraphQL for mobile apps. Also Amazon Pinpoint captures analytics data from clients and also sends targeted texts or emails based on user data.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-eventbridge","title":"Amazon EventBridge","text":"<ul> <li>EventBridge was formerly called Amazon CloudWatch Events. The default event bus and the rules you created in CloudWatch Events also display in the EventBridge console. EventBridge uses the same CloudWatch Events API, so your code that uses the CloudWatch Events API stays the same. New features added to EventBridge are not added to CloudWatch Events.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-step-functions","title":"AWS Step Functions","text":"<ul> <li>You can use to coordinate the components of distributed applications and microservices using visual workflows.</li> <li>States are elements in your state machine. A state is referred to by its name, which can be any string but it must be unique within the scope of the entire state machine. Individual states can make decisions based on their input, perform actions, and pass output to other states.</li> <li>You pay for each transition from one state to the next. Billing is metered by state transition, and you do not pay for idle time, regardless of how long each state persists (up to one year).</li> <li>You can configure your Step Functions workflow to call other AWS services such as compute services (Lambda, ECS, EKS, Fargate), database services (DynamoDB), messaging services (SNS, SQS), data processing and analytics services (Athena, AWS Batch, AWS Glue, Amazon EMR, and AWS Glue DataBrew), machine learning services (Amazon SageMaker),  APIs created by Amazon API Gateway and AWS SDK integrations to call over two hundred AWS services.</li> <li>Automatically handles errors and exceptions with built-in try/catch and retry, whether the task takes seconds or months to complete. You can automatically retry failed or timed-out tasks, respond differently to different types of errors, and recover gracefully by falling back to designated cleanup and recovery code.</li> <li>Support event rates greater than 100,000 per second.</li> <li>The Amazon States Language is a JSON-based, structured language used to define your state machine.</li> <li>Transitions link states together, defining the control flow for the state machine.</li> <li>Step Functions supports JSON path expressions.</li> <li>The fields that filter and control the flow from state to state are InputPath, ResultPath, OutputPath, Parameters and ResultSelector.</li> <li>The AWS Step Functions Workflow Studio is a low-code visual workflow designer.</li> </ul>"},{"location":"cloud/aws/cert-sap/#states","title":"States","text":"<ul> <li>A Pass state passes its input to its output, without performing work. Pass states are useful when constructing and debugging state machines.</li> <li>A Task state represents a single unit of work performed by a state machine. Tasks perform all work in your state machine. A task performs work by using an activity or an AWS Lambda function, or by passing parameters to the API actions of other services.</li> <li>A Choice state adds branching logic to a state machine.</li> <li>A Wait state delays the state machine from continuing for a specified time. You can choose either a relative time, specified in seconds from when the state begins, or an absolute end time, specified as a timestamp.</li> <li>A Succeed state stops an activity successfully. The Succeed state is a useful target for Choice state branches that don't do anything except stop the activity.</li> <li>A Fail state stops the activity of the state machine and marks it as a failure, unless it is caught by a Catch block.</li> <li>The Parallel state can be used to create parallel branches of activity in your state machine.</li> <li>The Map state can be used to run a set of steps for each element of an input array.</li> </ul>"},{"location":"cloud/aws/cert-sap/#intrinsic-functions","title":"Intrinsic functions","text":"<ul> <li>States.Format: format a string with the parameters (\"States.Format('Welcome to {} {}\\'s playlist.', $.firstName, $.lastName)\")</li> <li>States.StringToJson</li> <li>States.JsonToString</li> <li>States.Array: the interpreter returns a JSON array containing the values of the arguments, in the order provided.</li> </ul>"},{"location":"cloud/aws/cert-sap/#security_2","title":"Security","text":"<ul> <li>AWS Step Functions can invoke code and access AWS resources. In order for AWS Step Functions to invoke AWS resources, and maintain security, you need to grant Step Functions access to those resources by using an IAM role.</li> </ul>"},{"location":"cloud/aws/cert-sap/#standard-and-express-workflows","title":"Standard and Express Workflows","text":"<ul> <li>You cannot change the workflow type after you have created your state machine.</li> <li>Standard Workflows are ideal for long-running, durable, and auditable workflows.</li> <li>Express Workflows are ideal for high-volume, event-processing workloads such as IoT data ingestion, streaming data processing and transformation, and mobile application backends.</li> <li>There are two types of Express Workflows, asynchronous and synchronous.</li> </ul> Feature Standard Workflows Express Workflows Maximum duration 1 year 5 minutes Workflow run start rate Over 2,000 per second Over 100,000 per second Start transition rate Over 4,000 per second per account Nearly unlimited Pricing Priced per state transition. A state transition is counted each time a step in your run is completed. Priced by the number of times you run, their duration, and memory consumption. Workflow run history Runs can be listed/described via APIs, debugged in console, and logged in CloudWatch Logs. Not recorded in Step Functions. Can be logged to CloudWatch Logs if configured. Workflow run semantics Exactly-once workflow run. Async: At-least-once. Sync: At-most-once. Service integrations Supports all service integrations and patterns. Supports all integrations, but not <code>.sync</code> (Job-run) or <code>.waitForTaskToken</code> (Callback) patterns. Step Functions activities Supports Step Functions activities. Does not support Step Functions activities. Feature Asynchronous Express Workflows Synchronous Express Workflows Response behavior Return confirmation that the workflow has started, but do not wait for the workflow to complete. Start a workflow, wait until it completes, and then return the result. Use case Use when immediate response is not required, e.g., messaging services or background data processing. Use to orchestrate microservices, simplifying error handling, retries, and parallel task management. Invocation sources Can be started by a nested workflow in Step Functions or via the <code>StartExecution</code> API call. Can be invoked from Amazon API Gateway, AWS Lambda, or via the <code>StartSyncExecution</code> API call."},{"location":"cloud/aws/cert-sap/#use-cases_5","title":"Use cases","text":"<ul> <li>Data processing</li> <li>IT Automation</li> <li>E-commerce</li> <li>Web applications with human approval</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-batch","title":"AWS Batch","text":"<ul> <li>Batch dynamically provisions the optimal quantity and type of compute resources, such as CPU- or memory-optimized compute resources, based on the volume and specific resource requirements of the batch jobs submitted.</li> <li>Runs your batch computing workloads using Amazon EC2 and AWS compute resources with Fargate or Fargate Spot. </li> </ul>"},{"location":"cloud/aws/cert-sap/#elastic-beanstalk","title":"Elastic Beanstalk","text":"<ul> <li>Automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. </li> <li>Automatically scales your application up and down based on your application's specific need using adjustable Auto Scaling settings.</li> <li>No cost for the service, only the cost of the underlying infrastructure.</li> </ul>"},{"location":"cloud/aws/cert-sap/#lightsail","title":"Lightsail","text":"<ul> <li>VPS provider and is a useful way to get started with AWS for users who need a solution to build and host their applications on AWS Cloud.</li> <li>Provides low-cost, pre-configured cloud resources for simple workloads just starting on AWS.</li> <li>Cheap prices.</li> </ul>"},{"location":"cloud/aws/cert-sap/#scaling-serverless-architecture","title":"Scaling Serverless Architecture","text":""},{"location":"cloud/aws/cert-sap/#api-gateway","title":"API Gateway","text":"<ul> <li>If backend services starts to fail because of the load, then configure throttling on the API method.</li> <li>Consider limits in size of the payload (10MB).</li> <li>Consider the authorizer function for lambda concurrency calculations. You can mitigate this by enabling caching.</li> <li>Consider limits of the full system, for example API gateway is limited to 10MB but the SQS related queue only 256KB.</li> </ul>"},{"location":"cloud/aws/cert-sap/#sqs","title":"SQS","text":"<ul> <li>Lambda defaults to using five parallel processes to get messages off the queue, make sure that the reserved concurrency on the function is at least five.</li> <li>If the Lambda service detects an increase in queue size, it will automatically increase the number of concurrent Lambda functions it invokes each minute until maximum concurrency is reached or the queue has slowed down.</li> <li>If your Lambda function returns errors when processing messages, the Lambda service will decrease the number of processes polling the queue, assuming that the errors indicate too much pressure on downstream targets.</li> <li>SQS will continue to try a failed message up to the maximum receive count specified in the receive policy, at which point, if a dead-letter queue is configured, the failed message will be put into the dead-letter queue and deleted from your SQS queue.</li> <li>If the choice you made for batch size is too large, your Lambda function may time out, and that\u2019s going to result in an error back to the Lambda service.</li> <li>If the visibility timeout expires, before your Lambda function has processed the messages in that batch, any message in that batch that hasn\u2019t been deleted by your function will become visible again.</li> <li>The best practice, is to set your visibility timeout to six times the function timeout.</li> <li>In terms of the receive policy that determines when to send failed records to your dead-letter queue, you need to choose a max receive count that balances keeping things moving in the queue, vs. sending too many messages to the dead-letter queue, when the concurrency is high and functions are getting throttled.</li> </ul> Parameter Value or Limit How the Parameter is Set or Changed Number of messages that can be in a batch 1 to 10 Configured with the event source on the Lambda function Number of default pollers (batches returned at one time) 5 Managed by the Lambda service Rate at which Lambda increases the number of parallel pollers Up to 60 per minute Managed by the Lambda service Number of batches that Lambda manages simultaneously Up to 1,000 Managed by the Lambda service Number of Lambda functions that can be running simultaneously The lesser of 1,000 functions and the account limit Configured by setting a limit (reserved concurrency) on the function Messages per queue No limit N/A Visibility timeout 0 seconds to 12 hours Configured on the queue Number of retries 1 to 1,000 Configured on the queue (maxReceiveCount) Function timeout 0 seconds to 15 minutes Configured on the function"},{"location":"cloud/aws/cert-sap/#lambda_2","title":"Lambda","text":"<ul> <li>Consider a multi-account strategy to avoid functions of different apps competing for concurrency.</li> <li>Take advantage of warm-starts storing parameters locally when data is retrieved and use /tmp space as transient cache.</li> </ul>"},{"location":"cloud/aws/cert-sap/#dynamodb","title":"DynamoDB","text":"<ul> <li>The on-demand option scales to instantly double the previous peak, then you can configure throttling to only scale if new peak is more than double previous within 30 min.</li> <li>Use DAX (in memory cache installed in VPC) to increase the read and write speed. DAX is definitely worth a look if you have read-heavy workloads with the need for speed.</li> </ul>"},{"location":"cloud/aws/cert-sap/#traditional-relational-databases","title":"Traditional relational databases","text":"<ul> <li>Initialize one connection outside the handler and check for connection.</li> <li>Use database TTL to clean up connections because you can no close them.</li> <li>Use Lambda concurrency limits to limit the number of potential connections.</li> <li>The best practice is to implement an external mechanism for managing the connections.</li> <li>You could also use a method called Dynamic Content Management. This method uses an Amazon DynamoDB table to track connections allowed and connections in use and manipulates the count with a helper function packaged as a Lambda layer.</li> </ul>"},{"location":"cloud/aws/cert-sap/#step-functions","title":"Step Functions","text":"<ul> <li>Use wait states and callbacks.</li> <li>Use timeouts (by default is not enabled).</li> <li>Be aware of payload limits between steps (use S3 to store heavy data).</li> <li>Know the API limits.</li> </ul>"},{"location":"cloud/aws/cert-sap/#sns","title":"SNS","text":"<ul> <li>Use to execute tasks in parallel.</li> <li>Use AWS Event Fork Pipeline applications to deploy pre-built applications that use Amazon SNS to run common tasks in parallel.</li> </ul>"},{"location":"cloud/aws/cert-sap/#kinesis-data-streams","title":"Kinesis Data Streams","text":"<ul> <li>Stream processing is dependent of the number of shards, one lambda per shard.</li> <li>One shard can take up to 1000RPS or 1MB/s.</li> <li>Retention period from 24h to 1 week.</li> <li>If one message fails, the full batch fails.</li> <li>GetRecords requests can only be made at five transactions per second, per shard. Each request can return a maximum of 2MB of data. You can have up to five standard consumers on a stream, but all of them have to share the polling capacity and the data capacity.</li> <li>Enhanced Fan-Out subscribe to the stream, so they have the full 2MB capacity.</li> <li>If you have three consumers or less, and latency isn\u2019t critical, you probably want to use a standard stream to minimize the cost.</li> <li>More shards increase throughput and lower the impact of error but increases costs.</li> <li>An increasing IteratorAge metric may mean you need to re-shard.</li> <li>Any data record before the re-shard, they remain in the original shard, only new records are split.</li> </ul>"},{"location":"cloud/aws/cert-sap/#storage","title":"Storage","text":""},{"location":"cloud/aws/cert-sap/#choose-options_1","title":"Choose options","text":"<ul> <li>Understand characteristics like shareable, file size, cache size, access patterns, latency, throughput and persistence of data.</li> <li>Conduct a performance analysis to measure IOPS and throughput.</li> <li>Determine the expected growth rate for your workload and choose a storage solution that will meet those rates.</li> </ul>"},{"location":"cloud/aws/cert-sap/#questions","title":"Questions","text":"<ul> <li>Is it a new or existing workflow?</li> <li>What are the know workflow requirements?</li> <li>What is the type of use case?</li> <li>Wat are the requirements for storage location?</li> <li>Wat are the requirements for storage type?</li> <li>Wat are the requirements for storage performance?</li> <li>Wat are the requirements for access protocol?</li> <li>Wat are the requirements for data transfer?</li> <li>Wat are the requirements for data protection?</li> <li>What is the best combination of storage services?</li> <li>How often and how quickly do you need to access your data?</li> <li>Does your data store require high IOPS or throughput?</li> <li>What storage access protocols are required?</li> <li>How critical (durable) is your data?</li> <li>How sensitive is your data?</li> <li>How large is your dataset?</li> <li>How transient is your data?</li> <li>How much are you prepared to pay to store the data?</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-efs","title":"Amazon EFS","text":"<ul> <li>File storage.</li> <li>Operates as a Regional service.</li> <li>Automatically grows and shrinks as you add and remove files.</li> <li>Can grow to petabyte scale.</li> <li>You can connect tens, hundreds, and even thousands of compute instances to an Amazon EFS file system at the same time.</li> <li>You pay only for the storage used.</li> <li>EFS Standard and EFS Standard-Infrequent Access (Standard-IA) offer Multi-AZ resilience and the highest levels of durability and availability.</li> <li>EFS One Zone and EFS One Zone-Infrequent Access (EFS One Zone-IA) provide additional savings by saving your data in a single availability zone.</li> <li>For Linux.</li> <li>Supports the Network File System version 4 (NFSv4.1 and NFSv4.0) protocol.</li> <li>You can use Amazon EFS to control access to your file systems through Portable Operating System Interface (POSIX) permissions.</li> <li>Supports authentication, authorization, and encryption capabilities to help you meet your security and compliance requirements.</li> <li>The default General Purpose performance mode is ideal for latency-sensitive use cases, such as web serving environments, content management systems, home directories, and general file serving. </li> <li>File systems in the Max I/O mode can scale to higher levels of aggregate throughput and IOPS. The tradeoff is slightly higher latencies for file metadata operations.</li> <li>Max I/O performance mode is available only on Amazon EFS file systems using Standard storage classes.</li> <li>Using the default Bursting Throughput mode, throughput scales as your file system grows. Using Provisioned Throughput mode, you can specify the throughput of your file system independent of the amount of data stored.</li> <li>You can start saving on your storage costs by enabling EFS lifecycle management for your file system and choosing an age-off policy of 7,14, 30, 60, or 90 days. With EFS lifecycle management policies enabled, files automatically move from Amazon EFS Standard storage to EFS Standard-IA storage, or from Amazon EFS One Zone storage to EFS One Zone-IA storage.</li> <li>Amazon EFS transparently serves files from both frequently accessed and infrequent access storage classes from a common file system namespace.</li> <li>You can control network access to your file systems by using Amazon Virtual Private Cloud (Amazon VPC) security group rules.</li> <li>You can also control application access to your file systems by using AWS Identity and Access Management (IAM) policies and Amazon EFS access points.</li> <li>Amazon EFS can support performance over 10 GB/sec and over 500.000 IOPS.</li> <li>Amazon EC2 instances can access your file system across Availability Zones and Regions. By contrast, on-premises servers can access file systems using AWS Direct Connect or AWS Virtual Private Network (AWS VPN).</li> <li>Offers encryption for data at rest and in transit.</li> <li>Data at rest is transparently encrypted by using encryption keys managed by the AWS Key Management Service (AWS KMS).</li> <li>Encryption of data in transit uses open-standard Transport Layer Security (TLS) to secure network traffic without having to modify your applications. </li> <li>Amazon EFS provides applications running on Amazon ECS, Amazon EKS, AWS Fargate, and AWS Lambda, access to shared file systems for stateful workloads.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-fsx","title":"Amazon FSx","text":"<ul> <li>File storage.</li> </ul>"},{"location":"cloud/aws/cert-sap/#fsx-for-lustre","title":"FSx for Lustre","text":"<ul> <li>Parallel file system built on Lustre for high performance computing (HPC) workloads. FSx for Lustre supports the Lustre POSIX-compliant protocol.</li> <li>Operates in a single AZ.</li> <li>Delivers the highest levels of throughput (up to 1+ TB/s) and IOPS (millions). Customers can seamlessly integrate, access, and process their Amazon S3 datasets using the Lustre high-performance file system.</li> <li>You can link FSx for Lustre file systems to data repositories on Amazon Simple Storage Service (Amazon S3) or to on-premises data stores.</li> <li>FSx for Lustre integrates with Amazon CloudWatch metrics.</li> <li>FSx for Lustre appears as a native drive for Amazon Elastic Compute Cloud (Amazon EC2) Linux-based instances and Amazon Elastic Kubernetes Service (Amazon EKS) containers.</li> <li>Support for cloud bursting from your on-premises data repositories.</li> <li>FSx for Lustre maintains read-after-write close consistency for file sharing for your high-performance computing workloads.</li> <li>SSD storage is optimized for latency-sensitive workloads or workloads requiring the highest levels of IOPS and throughput.</li> <li>HDD storage is optimized for throughput-focused workloads that aren't latency-sensitive.</li> <li>For HDD-based file systems, the optional SSD cache improves performance by placing your most frequently read data on SSD automatically.</li> <li>The number of Amazon S3 PUT and GET requests to load and save your data are minimized retaining data used in FSx, which helps optimize costs.</li> <li>You can launch and delete FSx for Lustre file systems in minutes.</li> <li>Data imported into FSx for Lustre is encrypted. </li> <li>FSx for Lustre is compliant eligible with PCI from DSS, ISO and HIPAA.</li> <li>You control access to your application using Amazon Virtual Private Cloud (Amazon VPC) security groups.</li> <li>You control access using AWS Identity and Access Management (IAM) to set up users, groups, and roles and assign access permissions. IAM access permissions are applied for management and application programming interface (API) access to the FSx for Lustre file system.</li> <li>You can monitor and audit API calls using AWS CloudTrail.</li> <li>If you use AWS Batch to manage your HPC compute resource deployment, you can also use it to manage the FSx for Lustre persistent and scratch file systems.</li> <li>FSx for Lustre integrates with SageMaker as a direct data source.</li> <li>You can use AWS ParallelCluster to create a new Amazon FSx for Lustre file system automatically.</li> <li>Horizontal use cases: machine learning and HPC.</li> <li>Vertical use cases: genomics, media processing, autonomous vehicles and SAS Grid computing.</li> <li>For scratch FSx for Lustre file systems, you pay for your use of a file system based on the configured storage capacity measured in gigabyte-months (GB-months).</li> <li>For persistent FSx for Lustre file systems, you pay for your use of a file system based on the configured storage capacity measured in gigabyte-months (GB-months). In addition, you have three tiers of SSD-based performance and four tiers of HDD-based performance to choose from.</li> <li>You can create automatic daily backups and manual backups of persistent file systems that are not linked to an Amazon S3 durable data repository.</li> <li>Backups are available with only persistent file systems that are not linked to Amazon S3 data repositories.</li> <li>Backup pricing is based on the actual backed up data capacity.</li> </ul>"},{"location":"cloud/aws/cert-sap/#fsx-for-netapp-ontap","title":"FSx for NetApp ONTAP","text":"<ul> <li>Provides rich data management features and flexible shared file storage that are broadly accessible from Linux, Windows, and macOS compute instances running in AWS or on premises.</li> <li>Sub-millisecond latencies.</li> <li>These block services use NetApp's application programming interface (API) calls and management interface.</li> <li>FSx for ONTAP provides highly available and durable storage with fully managed backups and support for cross-region disaster recovery, and supports popular data security and anti-virus applications that make it even easier to protect and secure your data.</li> <li>Provides rich integration with other AWS services, such as AWS Identity and Access Management (IAM), Amazon WorkSpaces, AWS Key Management Service (AWS KMS), and AWS CloudTrail.</li> <li>You can create snapshots, clones, and replication, with the click of a button.</li> </ul>"},{"location":"cloud/aws/cert-sap/#administration-and-management","title":"Administration and Management","text":"<ul> <li>Administration: With FSx for ONTAP, you can use both AWS-native and NetApp management tools to set up, manage, and monitor your file systems.</li> <li>Data migration: FSx for ONTAP fully supports NetApp SnapMirror replication. You can configure SnapMirror to replicate your files, file metadata, and file system configuration, in a matter of minutes.</li> <li>Point-in-time, instantaneous cloning: FSx for ONTAP supports NetApp's FlexClone feature, enabling you to create a clone of the volumes in your file system instantaneously.</li> </ul>"},{"location":"cloud/aws/cert-sap/#accessibility","title":"Accessibility","text":"<ul> <li>Multi-protocol: NFS, SMB, and iSCSI. Support iSCSI for block storage, NFS protocol for POSIX-compliant access, and SMB protocol for Windows-compatible access.</li> <li>Offers block storage services over an iSCSI access protocol.</li> <li>Access from AWS compute services. Offers a multiple availability (AZ) deployment option. The multi-AZ option is designed to provide continuous availability to data, even in the event that an AZ is unavailable. Multi-AZ file systems include an active and standby file server in separate AZs. Any changes written to disk in your file system are synchronously replicated across AZs to the standby.</li> <li>Network connectivity: FSx for ONTAP provides shared storage for up to thousands of simultaneous clients running in Amazon EC2, Amazon ECS, Amazon EKS, VMware Cloud on AWS, Amazon WorkSpaces, and Amazon AppStream 2.0 instances.</li> </ul>"},{"location":"cloud/aws/cert-sap/#performance-and-scale","title":"Performance and Scale","text":"<ul> <li>Performance: FSx for ONTAP is designed to deliver fast, predictable, and consistent performance. It provides multiple GB/s of throughput per file system, and hundreds of thousands of IOPS per file system. You can also create read replicas of your data to scale the performance of read-heavy workloads to tens of GB/s of throughput.</li> <li>Low-latency access: FSx for ONTAP is built to deliver consistent sub-millisecond latencies when accessing data on SSD storage, and tens of milliseconds of latency when accessing data in capacity pool storage.</li> <li>Support for high performance database workloads: It supports common database features such as application-consistent snapshots using NetApp SnapCenter, data cloning using FlexClone, Continuously Available (CA) SMB shares, and Instant File Initialization.</li> <li>Storage scalability \u2013 Each FSx for ONTAP file system can scale to petabytes in size.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Elastic capacity pool tiering: Primary storage is provisioned and capacity pool storage is fully elastic. Automatically tiers data.</li> <li>Compression and deduplication: Includes all of ONTAP storage efficiency and cost-savings features, including compression and deduplication (in normal workloads 65% of savings).</li> <li>Flexible storage management: Thin provisioned (only consumes storage capacity from your file system for the data stored in the volume). You define the volume limit and can also apply quotas to users and groups. </li> <li>Flexible throughput and IOPS selection: Offers multiple throughput capacity levels that you can choose from. You can also optionally provision higher levels of IOPS as needed (independent from the storage and throughput capacity).</li> </ul>"},{"location":"cloud/aws/cert-sap/#availability-and-data-protection","title":"Availability and Data Protection","text":"<ul> <li>Snapshots and file restore: Supports restoring individual files and folders to previous versions using NetApp Snapshots.</li> <li>Cross-region replication: NetApp SnapMirror replication technology. You can use to replicate data between two ONTAP file systems. You can configure replication with a Recovery Point Objective (RPO) of as low as 5 minutes, and a Recovery Time Objective (RTO) in single-digit minutes. You can configure SnapMirror using the ONTAP CLI or REST API.</li> <li>Automated backups: FSx for ONTAP automatically takes highly durable daily backups of every volume in your file system. Backups are incremental relative to one another and crash-consistent. You can take additional backups of your volumes at any point.</li> </ul>"},{"location":"cloud/aws/cert-sap/#hybrid-workflow-support","title":"Hybrid Workflow Support","text":"<ul> <li>On-premises caching: Supports NetApp Global File Cache and FlexCache that are deployed on premises.</li> <li>Backup and disaster recovery to AWS: You can back up, archive, or replicate data from your on-premises file servers to FSx for ONTAP.</li> <li>Cloud bursting: You can configure FSx for ONTAP as an in-cloud cache for your on-premises data by using NetApp FlexCache.</li> </ul>"},{"location":"cloud/aws/cert-sap/#security-and-compliance","title":"Security and Compliance","text":"<ul> <li>Network isolation: You access your FSx for ONTAP file system from the Amazon VPC that it's associated with, or from any network that you peer with your VPC. You can also optionally use ONTAP export policies to configure which clients can read and write to the volumes.</li> <li>Resource-level permissions: FSx for ONTAP is integrated with IAM. You can also tag your Amazon FSx for NetApp ONTAP resources and control the actions that your IAM users and groups can take based on those tags.</li> <li>Identity-based authentication: FSx for ONTAP supports identity-based authentication over NFS or SMB if you join your file system to an Active Directory (AD). Your users can then use their existing AD-based user identities to authenticate themselves. Once authenticated, users can access the \ufb01le system and control access to individual files and folders.</li> <li>Encryption: All data is automatically encrypted using KMS. Also supports Kerberos-based encryption in transit if you join your file system to Active Directory.</li> <li>Logging and auditing: Integrates with CloudTrail to monitor and log administrative actions made in the NetApp ONTAP console, API and CLI.</li> <li>Compliance: PCI DSS, ISO 9001, 27001, 27017, and 27018, and SOC 1, 2, and 3, in addition to being HIPAA eligible.</li> <li>File access auditing: Supports auditing end-user access to your files and folders using ONTAP native audit logging. Also supports ONTAP FPolicy feature with AWS Partner solutions to monitor for file access events.</li> <li>Antivirus: Supports ONTAP vscan feature. You can use vscan with AWS Partner antivirus applications to automatically scan new files as they are written to your file system.</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases_6","title":"Use Cases","text":"<ul> <li>Migrate workloads to AWS seamlessly: Move workloads running on NetApp or other NFS/SMB/iSCSI servers.</li> <li>Build modern applications.</li> <li>Modernize your data management.</li> <li>Simplify business continuity.</li> </ul>"},{"location":"cloud/aws/cert-sap/#pricing_4","title":"Pricing","text":"<ul> <li>SSD storage capacity.</li> <li>SSD IOPS.</li> <li>Capacity Pool usage.</li> <li>Throughput Capacity.</li> <li>Backups.</li> <li>Data transfer in and out of FSx for ONTAP across Availability Zones and VPC peering connections.</li> <li>Capacity pool read and write requests.</li> </ul>"},{"location":"cloud/aws/cert-sap/#fsx-for-openzfs","title":"FSx for OpenZFS","text":"<ul> <li>Implementation of the Open Zettabyte File System (ZFS).</li> <li>Supports NFS and SMB protocols for a wide range of application implementations. </li> <li>Delivers leading performance for latency-sensitive and small-file workloads with popular NAS data management capabilities (snapshots, and cloning), at a lower price than commercially licensed alternatives.</li> </ul>"},{"location":"cloud/aws/cert-sap/#performance-and-scale_1","title":"Performance and scale","text":"<ul> <li>High-speed, low-latency file storage in the cloud: Lowest file storage latencies available in the cloud.</li> <li>Throughput and IOPS performance: Supporting up to 12,5 GB/s of throughput and up to 1 million IOPS for frequently accessed cached data. For data accessed from persistent disk storage deliver up to 4 GB/s and up to 160.000 IOPS. You can also enable data compression to increase your effective throughput.</li> <li>Scalable performance for up to thousands of clients: Supports simultaneous access from up to thousands of clients. With support for multiple parallel connections per client.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cost-optimization_1","title":"Cost optimization","text":"<ul> <li>Flexible storage and performance capacity: Independent storage and performance capacity.</li> <li>ZFS-powered storage efficiency capabilities: Supports the latest Z-Standard compression technologies. Z-standard compression.</li> </ul>"},{"location":"cloud/aws/cert-sap/#accessibility_1","title":"Accessibility","text":"<ul> <li>Support for the latest versions of NFS: Full support for NFS v3, v4.0, v4.1, and v4.2</li> <li>Accessible from AWS compute instances and containers: Supports EC2, ECS, EKS, VMware Cloud, Amazon WorkSpaces and Amazon AppStream 2.0</li> <li>Accessible in AWS and on premises: You can access file systems from another VPC (including a VPC in another region) using AWS Transit Gateway or VPC Peering, and you can access file systems from on premises using AWS Direct Connect or VPN.</li> </ul>"},{"location":"cloud/aws/cert-sap/#administration-and-management_1","title":"Administration and management","text":"<ul> <li>Rich ZFS capabilities for working with data: Provides rich ZFS capabilities for working with data, like point-in-time snapshots and in-place data cloning, natively via the FSx API. Clones are created almost instantly, they consume no additional capacity upon creation, and any data modifications are isolated from your original dataset.</li> <li>Simple, flexible administration: You can manage your file systems using the AWS Management Console, AWS Command Line Interface (AWS CLI), and AWS SDK.</li> <li>Integrated with AWS services: Integrated with IAM, CloudWatch, CloudTrail and CloudFormation.</li> </ul>"},{"location":"cloud/aws/cert-sap/#availability-and-data-protection_1","title":"Availability and data protection","text":"<ul> <li>Automatic file system backups for disaster recovery: Daily backups to S3. You can take additional backups or restore one to a new file system.</li> <li>Easy file-level restore with snapshots: Supports near instant point-in-time volume snapshots that are stored directly within your file system. End users can easily restore volumes to past snapshots, or even undo changes and compare versions of individual files or directories.</li> </ul>"},{"location":"cloud/aws/cert-sap/#security-and-compliance_1","title":"Security and compliance","text":"<ul> <li>Encryption: All data is automatically encrypted using KMS. Data is automatically encrypted in transit with 256-bit encryption when accessed from supported Amazon EC2 instance types.</li> <li>Network isolation and resource-level permissions and tagging: Integrated with IAM.</li> <li>API activity monitoring: You can monitor and secure API calls using AWS CloudTrail and IAM and detect and flag suspicious API usage patterns using Amazon GuardDuty.</li> <li>File and directory-level access control: Supports POSIX permissions and POSIX ACLs.</li> <li>Compliance: Complying with PCI DSS, ISO 9001, 27001, 27017, and 27018; SOC 1, 2, and 3; in addition to being HIPAA eligible.</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases_7","title":"Use Cases","text":"<ul> <li>Migrate workloads to AWS seamlessly: Move workloads running on ZFS or other Linux-based file servers.</li> <li>Deliver insights faster for data analytics workloads: Data intensive applications with high-IOPS storage</li> <li>Accelerate content management.</li> <li>Increase dev/test velocity: Fast storage for repositories and DevOps solutions, such as Git, Bitbucket, and Jenkins.</li> </ul>"},{"location":"cloud/aws/cert-sap/#pricing_5","title":"Pricing","text":"<ul> <li>SSD storage capacity.</li> <li>SSD IOPS.</li> <li>Throughput Capacity.</li> <li>Backups.</li> <li>Data transfer in and out of FSx for OpenZFS across Availability Zones and VPC peering connections.</li> <li>Data transfer out of FSx for OpenZFS to other AWS Regions.</li> </ul>"},{"location":"cloud/aws/cert-sap/#fsx-for-windows-file-server","title":"FSx for Windows File Server","text":"<ul> <li>Provides file storage that is accessible over the Service Message Block (SMB) protocol and has the ability to serve as a drop-in replacement for existing Windows file server deployments.</li> <li>Built on Windows Server.</li> <li>To control user access, Amazon FSx integrates with your on-premises Microsoft Active Directory and with AWS Microsoft Managed AD.</li> <li>Accessible from EC2, ECS, VMWare Cloud, Workspaces and Amazon AppStream 2.0.</li> <li>FSx for Windows File Server supports all Windows versions, starting from Windows Server 2008 and Windows 7, and current versions of Linux and macOS.</li> <li>Sx for Windows File Server also supports on-premises access through AWS Direct Connect or AWS VPN. It also supports access from multiple virtual private clouds (VPCs), AWS accounts, and AWS Regions by using VPC Peering or AWS Transit Gateway.</li> <li>For frequently accessed file data, Amazon FSx File Gateway provides efficient, low-latency on-premises access with a local cache.</li> <li>FSx for Windows File Server provides storage of up to 64 TB per file system.</li> <li>You can use Distributed File System (DFS) namespaces to create shared common namespaces spanning multiple FSx for Windows File Server file systems.</li> <li>Encryption of data at rest uses keys managed with AWS Key Management Service (AWS KMS).</li> <li>Data is encrypted automatically before being written to the file system and decrypted automatically as it is read.</li> <li>Amazon FSx automatically encrypts data in transit using SMB Kerberos session keys, when accessed from compute instances that support SMB protocol 3.0 or newer.</li> <li>Compliance with PCI from DSS, ISO 9001, 27001, 27017 and 27018 and SOC 1, 2 and 3 and HIPAA.</li> <li>FSx for Windows File Server supports Windows access control lists (ACLs) for fine-grained file and folder access control.</li> <li>For network-level access control, you can use Amazon Virtual Private Cloud (Amazon VPC) security groups to control access to your FSx for Windows File Server resources.</li> <li>Use IAM to control the actions that your IAM users and groups can take on specific FSx for Windows File Server resources.</li> <li>FSx for Windows File Server integrates with AWS CloudTrail to monitor and log administration actions.</li> <li>Amazon FSx also offers user storage quotas to monitor and control user-level storage consumption.</li> <li>FSx for Windows File Server supports auditing user access to your files, folders, and file shares by using Windows Event Logs. Logs are published to Amazon CloudWatch Logs or streamed to Amazon Kinesis Data Firehose.</li> <li>FSx for Windows File Server offers Single-AZ (by default) and multi-AZ deployment options for your Windows file-based workloads.</li> <li>Any changes written to disk in your file system are synchronously replicated across Availability Zones to the standby.</li> <li>During planned maintenance, or in the event of a failure of the active file server or its Availability Zone, FSx for Windows File Server automatically fails over to the standby.</li> <li>Support for High Availability Microsoft SQL Server deployments.</li> <li>Automated daily backups to S3. Uses the Volume Shadow Copy Service (VSS) to make your backups file system-consistent. You can take additional backups of your file system at any point.</li> <li>Supports restoring individual files and folders to previous versions using Windows shadow copies.</li> <li>Cross-Region and cross-account backup compliance.</li> <li>Storage options of HDD and SSD.</li> <li>Dada deduplication to reduces costs associated with redundant data. 50%-60% for general purpose file shares, 30%-50% for user documents and 70%-80% for software development datasets.</li> <li>Microsoft SQL Server database workloads without the need for Enterprise licenses.</li> <li>Use cases: business applications (lift-and-shift), home directories, High Availability Microsoft SQL Server deployments, media workflows, web serving and content management, data analytics.</li> <li>You pay data transfer in and out of FSx for Windows File Server across Availability Zones and VPC peering connections.</li> <li>You pay data transfer out of FSx for Windows File Server to other AWS Regions.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-ec2-instance-store","title":"Amazon EC2 Instance Store","text":"<ul> <li>Block storage.</li> <li>Close to the physical server. Provides submillisecond latencies.</li> <li>Tied to the instance.</li> <li>Only specific Amazon EC2 instance types support instance stores.</li> <li>Ideal if you host applications that replicate data to other EC2 instances, such as Hadoop clusters. For these cluster-based workloads, having the speed of locally attached volumes and the resiliency of replicated data helps you achieve data distribution at high performance.</li> <li>It's also ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content.</li> <li>As ephemeral storage, instance stores are not replicated or spread across multiple devices to improve durability and availability.</li> <li>If an instance reboots (intentionally or unintentionally), data in the instance store persists. However, data in the instance store is lost if disc drive fails, instance stops, hibernates or terminates.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-ebs","title":"Amazon EBS","text":"<ul> <li>Block storage.</li> <li>Can attach multiple volumes to one instance.</li> <li>We can de-attach from one instance and attach to another.</li> <li>Depending on the instance type and EBS volume, you can have the same volume attached to multiple instances (EBS Multi-Attach).</li> <li>Multi-attach feature that permits Provisioned IOPS SSD (io1 or io2) volumes to be attached to multiple EC2 instances at one time. This feature is not available for all instance types, and all instances must be in the same Availability Zone.</li> <li>Multi-attach allows a single EBS volume to be concurrently attached to up to 16 Nitro-based EC2 instances within the same Availability Zone.</li> <li>Backups are incremental snapshots.</li> <li>Amazon EBS does not manage data consistency for multiple writers. Your application or operating system environment must manage data consistency operations.</li> <li>Increase the volume size only if it doesn't increase above the maximum size limit. Depending on the volume selected, Amazon EBS currently supports a maximum volume size of 64 tebibytes (TiB).</li> <li>Attach multiple volumes to a single EC2 instance. Amazon EC2 has a one-to-many relationship with EBS volumes. You can add these additional volumes during or after EC2 instance creation to provide more storage capacity for your hosts.</li> <li>Boot and root volumes can be used to store an operating system.</li> <li>Can be used as a storage layer for databases running on Amazon EC2 that will scale with your performance needs and provide consistent and low-latency performance.</li> <li>Provides high availability and high durability block storage to run business-critical applications.</li> <li>Offers data persistence, dynamic performance adjustments, and the ability to detach and reattach volumes, so you can resize clusters for big data analytics.</li> <li>When you create an EBS volume, it is automatically replicated in its Availability Zone to prevent data loss from single points of failure.</li> <li>Storage persists even when your instance doesn't.</li> <li>When activated by the user, all EBS volumes support encryption on creation. Snapshots are also encrypted.</li> <li>EBS volumes support on-the-fly changes. Modify volume type, volume size, and input/output operations per second (IOPS) capacity without stopping your instance.</li> <li>Amazon EBS provides the ability to create backups of any EBS volume.</li> <li>EBS snapshots can be used to create multiple new volumes, whether they're in the same Availability Zone or a different one.</li> <li>When you delete a snapshot, only the data unique to that snapshot is removed.</li> <li>Multi-volume snapshots let you take exact point-in-time, data-coordinated, and crash-consistent snapshots across multiple EBS volumes attached to an EC2 instance.</li> <li>Pay only for the storage and resources that you provision.</li> <li>Root EBS volumes created with an EC2 instance are terminated with the instance by default. However, you can modify the volume to be persistent.</li> <li>The Elastic Volumes feature makes it easier to adapt your resources to changing application demands. You can make modifications in the future as your business needs change.</li> <li>With Elastic Volumes, volume sizes can only be increased within the same volumes. To decrease a volume size, you must copy the EBS volume data to a new smaller EBS volume.</li> <li>Amazon EBS volumes are designed to provide 99.8\u201399.9 percent durability with an annual failure rate (AFR) of 0.1\u20130.2 percent.</li> <li>Amazon EBS offers a higher durability io2 volume that is designed to provide 99.999 percent durability with an AFR of 0.001 percent.</li> <li>Performance metrics, such as bandwidth, throughput, latency, and average queue length, are available through the AWS Management Console.</li> <li>The maximum amount of data that a volume type counts as a single I/O is 256 KiB for SSD and 1024 KiB for HDD. When small IO operations are physically contiguous, EBS attempts to merge them.</li> <li>Throughput is the measurement the volume of data transferred.</li> <li>For some SSD-backed and the HDD-backed EBS volume types, you are able to burst your performance above your provisioned baseline limits. They accumulate and you use them when needed.</li> <li>Latency is the true round trip time of an I/O operation or the elapsed time, between sending an I/O to Amazon EBS and receiving an acknowledgement from Amazon EBS that the I/O read or write operation is complete.</li> <li>Volume queue length can affect latency. The volume queue length is the number of pending I/O requests for a device. Queue length must be correctly calibrated with I/O size and latency to avoid creating bottlenecks, either on the guest operating system or on the network link to Amazon EBS.</li> <li>Your account has a limit on the number of EBS volumes that you can use and the total storage available to you. You can request an increase in your limits if required.</li> <li>For gp3 and io2 volumes types, you can dynamically change the provisioned IOPS or provisioned throughput performance settings for your volume.</li> <li>io1, io2 and gp3 support provisioning IOPS performance separately from the volume size.</li> <li>Once your EBS volumes are in operation, you can monitor them and verify that your volumes are providing optimal performance and cost effectiveness using AWS Compute Optimizer.</li> <li>EBS Snapshot events are tracked through CloudWatch events.</li> <li>You can copy any accessible snapshot that has a completed status.</li> <li>Pricing is based on provisioned volume size, IOPS and throughput billed per second (prices in months are 30 days based). When calculated you need to take into account the default IOPS and throughput (gp3 3000 and 125).</li> <li>Striped volumes: The striped configuration uses a RAID 0 style process to increase the volume size and increase performance for the combined EBS volumes.</li> <li>You can encrypt both the EBS boot and data volumes of an EC2 instance.</li> <li>To determine the minimum provisioned capacity to support your desired sustained IOPS, you divide the sustained IOPS by the IOPS-to-volume capacity ratio.</li> </ul>"},{"location":"cloud/aws/cert-sap/#snapshot-encryption","title":"Snapshot encryption","text":"<ul> <li>Snapshots of encrypted volumes are automatically encrypted.</li> <li>Volumes created from encrypted snapshots are automatically encrypted.</li> <li>Volumes created from an unencrypted snapshot can be encrypted during the creation process.</li> <li>When you copy an unencrypted snapshot, you can encrypt it during the copy process.</li> <li>When you copy an encrypted snapshot, you can re-encrypt it with a different encryption key during the copy process.</li> <li>The first snapshot taken of an encrypted volume that was created from an unencrypted snapshot is always a full snapshot.</li> <li>The first snapshot taken of a re-encrypted volume that has a different encryption key from the source snapshot is always a full snapshot.</li> <li></li> </ul>"},{"location":"cloud/aws/cert-sap/#workload-characteristics-questions","title":"Workload characteristics questions","text":"<ul> <li>Is your workload more IOPS-intensive or throughput-intensive? If IOPS then SSD else then HDD.</li> <li>Do the workload requirements exceed the maximum performance characteristics for a selected EBS volume type? If yes eliminate the volume type from consideration.</li> <li>What is the application's latency sensitivity? If low up to 1 digit then io2 Provisioned IOPS. If single or two digits then gp3. if no latency requirements then HDD could be most cost effective choice.</li> <li>Do you prefer to optimize for price or performance? When multiple types could satisfy the requirements.</li> </ul>"},{"location":"cloud/aws/cert-sap/#volume-types","title":"Volume types","text":"Category Type Description Size Max IOPS per volume Max throughput per volume EBS Multi-attach SSD gp3 Balance of price and performance for transactional workloads. Use cases like virtual desktops, test and development environments, interactive gaming applications. 1 GiB - 16 TiB 16.000 (3.000 min) 1.000 MiB/s No SSD gp2 Balance of price and performance for transactional workloads. Use cases like virtual desktops, test and development environments, interactive gaming applications. 1 GiB - 16 TiB 16.000 (100 min) 250 MiB/s No SSD io2 Block Express High performance designed for latency-sensitive transactional workloads. Use cases like SAP HANA, Microsoft SQL Server, and IBM DB2. 4 GiB - 64 TiB 256.000 4.000 MiB/s Yes SSD io2 High performance designed for latency-sensitive transactional workloads. Use cases like SAP HANA, Microsoft SQL Server, and IBM DB2. 4 GiB - 16 TiB 64.000 1.000 MiB/s Yes SSD io1 High performance designed for latency-sensitive transactional workloads. Use cases like SAP HANA, Microsoft SQL Server, and IBM DB2. 4 GiB - 16 TiB 64.000 1.000 MiB/s Yes HHD st1 Low cost designed for frequently accessed, throughput intensive workloads 125 GiB - 16 TiB 500 500 MiB/s No HHD sc1 Lowest cost designed for less frequently accessed workloads 125 GiB - 16 TiB 250 250 MiB/s No"},{"location":"cloud/aws/cert-sap/#gp2-general-purpose-ssd-volumes","title":"gp2 General Purpose SSD volumes","text":"<ul> <li>These volumes deliver single-digit millisecond latencies and the ability for smaller volumes to burst to 3,000 IOPS for extended periods of time.</li> <li>Baseline performance scales linearly at 3 IOPS per GiB of volume size.</li> <li>Performance ranges from a minimum of 100 IOPS at 33.33 GiB and below to a maximum of 16,000 IOPS at 5,334 GiB and above.</li> <li>A flat-rate pricing model is based on the provisioned volume size.</li> </ul>"},{"location":"cloud/aws/cert-sap/#gp2-io-burst-credits-and-burst-performance","title":"gp2 I/O burst credits and burst performance","text":"<ul> <li>I/O credits represent the available bandwidth that your gp2 volume can use to burst large amounts of I/O when more than the baseline performance is needed. </li> <li>Larger volumes have higher baseline performance levels and accumulate I/O credits faster.</li> <li>Each volume receives an initial I/O credit balance of 5.4 million I/O credits, which is enough to sustain the maximum burst performance of 3,000 IOPS for at least 30 minutes.</li> <li>gp2 volumes earn I/O credits at the baseline performance rate of 3 IOPS per GiB of provisioned volume size.</li> <li>When your volume uses fewer I/O credits than it earns in a second, unused I/O credits are added to the I/O credit balance. </li> <li>When your volume requires more than the baseline performance I/O level, it draws on I/O credits in the credit balance to burst to the required performance level up to a maximum of 3.000 IOPS.</li> <li>When the baseline performance of a volume is higher than maximum burst performance, I/O credits are never spent.  </li> </ul>"},{"location":"cloud/aws/cert-sap/#gp3-general-purpose-ssd-volumes","title":"gp3 General Purpose SSD volumes","text":"<ul> <li>gp3 consistent baseline performance of 3.000 IOPS and 125 MB/s throughput is included with the price of storage.</li> <li>The maximum ratio of provisioned IOPS to provisioned volume size is 500 IOPS per GiB up to 32 GiB.</li> <li>The maximum ratio of provisioned throughput to provisioned IOPS is 0.25 MB/s per provisioned IOPS (with 8 GiB you reach the limit of 1000 MB/s).</li> </ul>"},{"location":"cloud/aws/cert-sap/#io1-and-io2-provisioned-iops-ssd-volume-comparison","title":"io1 and io2 Provisioned IOPS SSD volume comparison","text":"<ul> <li>Provisioned IOPS SSD volumes can range in size from 4 GiB to 16 TiB. You can provision 100\u201364.000 IOPS per volume on instances built on the Nitro System and up to 32.000 on other instances.</li> <li>Provisioned IOPS SSD volumes provisioned with up to 32.000 IOPS support a maximum I/O size of 256 KiB and yield as much as 500 MiB/s of throughput. With the I/O size at the maximum, peak throughput is reached at 2.000 IOPS.</li> <li>A volume provisioned with more than 32.000 IOPS (up to the cap of 64.000 IOPS) supports a maximum I/O size of 16 KiB and yields as much as 1.000 MiB/s of throughput.</li> </ul>"},{"location":"cloud/aws/cert-sap/#io1-and-io2-provisioned-iops-ssd-volume-differences","title":"io1 and io2 Provisioned IOPS SSD volume differences","text":"<ul> <li>io2 durability is better than io1 (99.999 vs 99.8-99.9)</li> <li>io1 are available to all EC2 instance types, io2 not for R5b.</li> <li>The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1 for io1 volumes and 500:1 for io2 volumes.</li> </ul>"},{"location":"cloud/aws/cert-sap/#throughput-optimized-hdd-st1","title":"Throughput Optimized HDD (st1)","text":"<ul> <li>This volume type is a good fit for large, sequential workloads such as Amazon EMR, data warehouses, log processing, and extract, transform, and load (ETL) workloads.</li> <li>Maximum IOPS is based on 1MB I/O size, with a baseline throughput of 40MB/s per TB of volume size for st1 volumes. </li> <li>Sustained throughput performance ranges from 5 MB/s at 125 GiB to a maximum of 500 MB/s at 12,775 GiB and above.</li> <li>Baseline burst performance scales from 40 MB/s per TiB to 500 MB/s.</li> <li>st1 volumes are designed to deliver their provisioned performance 90 percent of the time.</li> <li>st1 volume size can range from 125 GiB to 16 TiB.</li> <li>For a 1-TiB st1 volume, burst throughput is limited to 250 MB/s. Larger volumes scale these limits linearly with throughput capped at a maximum of 500 MB/s.</li> <li>The bucket fills with credits at 40 MB/s, and it can hold up to 1 TiB of credits.</li> <li>When the baseline performance of a volume is higher than maximum burst performance, I/O credits are never spent.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cold-hdd-sc1","title":"Cold HDD (sc1)","text":"<ul> <li>Maximum IOPS is based on 1MB I/O size, with a baseline throughput of 12 MB/s per TB of volume size for sc1 volumes.</li> <li>Sustained throughput performance ranges from 1.5 MB/s at 125 GiB to a maximum of 192 MB/s at 16,384 GiB.</li> <li>Baseline burst performance scales from 12 MB/s per TiB to 250 MB/s.</li> <li>sc1 volumes are designed to deliver their provisioned performance 90 percent of the time.</li> <li>sc1 volume size can range from 125 GiB to 16 TiB.</li> <li>For a 1-TiB st1 volume, burst throughput is limited to 80 MB/s. Larger volumes scale these limits linearly, with throughput capped at a maximum of 250 MB/s.</li> <li>The bucket fills with credits at 12 MB/s, and it can hold up to 1 TiB of credits.</li> </ul>"},{"location":"cloud/aws/cert-sap/#magnetic-volumes","title":"Magnetic volumes","text":"<ul> <li>Magnetic is a previous-generation EBS volume type that is still in use in some customer production environments and available on AWS Management Console.</li> <li>AWS recommends General Purpose SSD gp3 volumes for new workloads. gp3 volumes deliver higher performance and better consistency than Magnetic volumes.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-ebs-metrics","title":"Amazon EBS metrics","text":"<ul> <li>All EBS volume types automatically send 1-minute metrics to CloudWatch.</li> </ul> Metric Description VolumeReadBytes Provides information on the read operations in a specified period of time VolumeWriteBytes Provides information on the write operations in a specified period of time VolumeReadOps The total number of read operations in a specified period of time VolumeWriteOps The total number of write operations in a specified period of time VolumeTotalReadTime The total number of seconds spent by all read operations that completed in a specified period of time. If multiple requests are submitted at the same time, this total could be greater than the length of the period. (Not supported with multi-attach enabled volumes) VolumeTotalWriteTime The total number of seconds spent by all write operations that completed in a specified period of time. If multiple requests are submitted at the same time, this total could be greater than the length of the period. (Not supported with multi-attach enabled volumes) VolumeIdleTime The total number of seconds in a specified period of time when no read or write operations were submitted. (Not supported with multi-attach enabled volumes) VolumeQueueLength The number of read and write operation requests waiting to be completed in a specified period of time. VolumeThroughputPercentage Used with Provisioned IOPS SSD volumes only. The percentage of I/O operations per second (IOPS) delivered of the total IOPS provisioned for an Amazon EBS volume. (Not supported with multi-attach enabled volumes) VolumeConsumedReadWriteOps Used with Provisioned IOPS SSD volumes only. The total amount of read and write operations (normalized to 256K capacity units) consumed in a specified period of time. BurstBalance Used with General Purpose SSD (gp2), Throughput Optimized HDD (st1), and Cold HDD (sc1) volumes only. Provides information about the percentage of I/O credits (for gp2) or throughput credits (for st1 and sc1) remaining in the burst bucket. Data is reported to CloudWatch only when the volume is active. If the volume is not attached, no data is reported."},{"location":"cloud/aws/cert-sap/#amazon-cloudwatch-events-for-amazon-ebs","title":"Amazon CloudWatch events for Amazon EBS","text":"<ul> <li>createVolume, deleteVolume, attachVolume, reattachVolume and modifyVolume</li> <li>createSnapshot, createSnapshots, copySnapshot and shareSnapshot</li> </ul>"},{"location":"cloud/aws/cert-sap/#s3","title":"S3","text":"<ul> <li>Object storage.</li> <li>Regional service.</li> <li>Stores objects in buckets.</li> <li>A bucket name cannot be used by another AWS account in the same partition until the bucket is deleted.</li> <li>You can create between 1 and 100 buckets in each AWS account. You can increase the bucket limit to a maximum of 1.000 buckets by submitting a service limit increase.</li> <li>Everything in Amazon S3 is private by default.</li> <li>A single object can be up to 5 terabytes in size.</li> <li>When uploading data via the AWS Management Console, the maximum file that you can upload is 160GB.</li> <li>You can upload or copy objects of up to 5 GB in a single PUT operation. For objects, up to 5 TB you must use the multipart upload API. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.</li> <li>It is recommended to enable and use lifecycle rules to clean up incomplete multipart uploads automatically (AbortIncompleteMultipartUpload action).</li> <li>Performance scales per prefix.</li> <li>If you need to retrieve the object in parts, use the Range HTTP header in a GET request (bytes range).</li> <li>You can permanently delete individual versions of an object by invoking a DELETE request with the object's key and version ID. To completely remove the object from your bucket, you must delete each individual version.</li> <li>Amazon S3 buckets are owned by the account that creates them and cannot be transferred to other accounts.</li> <li>Bucket names are globally unique, you can not change a bucket name and need to be DNS-compliant (3-63 characters, lower letters, numbers, dots and hyphens, start with letter or number, not begin with xn--, not be an ip address, only use . in the name if is for static website).</li> <li>Buckets are permanent storage entities and only removable when they are empty. After deleting a bucket, the name becomes available for reuse by any account after 24 hours if not taken by another account.</li> <li>An object consists of the following: Key, version ID, value, metadata, and access control information.</li> <li>Each S3 bucket has a tag set. A tag set contains all of the tags that are assigned to that bucket and can contain as many as 50 tags, or it can be empty.</li> <li>You can associate up to 10 tags with an object they must have unique tag keys. They need to be up to 128 characters in length in key and 255 in value, and are case sensitive.</li> <li>With object tags you can enable fine-grained access with IAM, fine grained lifecycle management, S3 analytics and CloudWatch metrics.</li> <li>If you want to add or replace a tag in a tag set (all the tags associated with an object or bucket), you must download all the tags, modify the tags, and then replace all the tags at once.</li> <li>Calls to the api could temporarily response redirection to other facilities (DNS redirection to different ips).</li> <li>Amazon S3 performance supports at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data.</li> <li>Amazon S3 provides strong read-after-write (new or override) consistency for GET, LIST, PUT, HEAD and DELETE actions.</li> <li>For bucket operation such as reading a bucket policy or metadata, the consistency model is eventually consistent.</li> <li>When IAM policies are attached to your resources (buckets and objects) or IAM users, groups, and roles, the policies define which actions they can perform.</li> <li>S3 bucket policies can only be attached to S3 buckets. Use this instead of IAM if you need a simple way to do cross-account or the policies bump up against the defined IAM size limit.</li> <li>Amazon S3 reinforces encryption in transit (as it travels to and from Amazon S3) and at rest.</li> <li>Amazon S3 automatically encrypts all objects on upload and applies server-side encryption with S3-managed keys as the base level of encryption for every bucket in Amazon S3 at no additional cost.</li> <li>Versioning keep multiple versions of a single object.</li> <li>Version states: unversioned (default), versioning-enabled, versioning-suspended.</li> <li>Lifecycle: transition actions define when objects should transition to another storage class and expiration actions define when objects expire and should be permanently deleted.</li> <li>You can use S3 Storage Class Analysis to monitor access patterns across objects to discover data that should be moved to lower-cost storage classes.</li> <li>You can also use S3 Batch Operations to run AWS Lambda functions across your objects to run custom business logic.</li> <li>To prevent accidental deletions, enable multi-factor authentication (MFA) Delete on an S3 bucket.</li> <li>With S3 Replication, you can replicate objects and their respective metadata and object tags to one or more destination buckets into the same or different AWS Regions.</li> <li>Cross-Region Replication (CRR) helps you store data in multiple regions. You can replicate all bucket or use tags to only replicate desired objects.</li> <li>Same-Region Replication (SRR) allows you to automatically aggregate logs from different S3 buckets for in-region processing, or configure live replication between test and development environments.</li> <li>You can use Amazon S3 to host a static website. you can configure index and error documents, and also redirection.</li> <li>If you include dots in a bucket name that is not a static website, you can't use virtual-host-style addressing over HTTPS.</li> <li>You can enforce write-once-read-many (WORM) policies with S3 Object Lock. In Governance mode can remove objects if correct IAM permissions, in Compliance mode none can.</li> <li>You can configure S3 Event Notifications to trigger workflows, alerts, and invoke AWS Lambda when a specific change is made to your S3 resources.</li> <li>You can use Amazon Macie to discover and protect sensitive data stored in Amazon S3.</li> <li>Amazon S3 supports a bucket policy size limit of up 20 kb.</li> <li>Because bucket policies grant access to another AWS account or IAM user, you must specify the principal, or the user to whom you are granting access, as a \"Principal\" in the bucket policy.</li> <li>Use bucket policy if you need to grant cross-account permissions without IAM roles, or IAM policies reach size limits or you prefer to keep access control in the S3 environment.</li> <li>Use IAM user policies if you want centralized management or have numerous S3 buckets with different permissions requirements or prefer to only use IAM environment.</li> <li>Access Analyzer for Amazon S3 alerts you to buckets configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization. For each public or shared bucket, you receive findings that report the source and level of public or shared access.</li> </ul>"},{"location":"cloud/aws/cert-sap/#storage-classes","title":"Storage classes","text":"Class Description S3 Standard General purpose S3 Intelligent-Tiering For unknown or changing access patterns. S3 Standard-IA For data that is accessed less frequently but required rapid access when need. Ideal for long-term backups, disaster recovery files, etc. S3 One Zone-IA Ideal to store data that is recreatable and needs fast access but infrequent S3 Glacier Instant Retrieval Lower-cost archival storage that may require retrieval at any time. S3 Glacier Flexible Retrieval Has default retrieval time of 1-5 minutes using expedited retrieval. Free bulk is up to 5-12 hours. Ideal for data accessed 1 or 2 times per year. Minimum storage duration of 90 days. S3 Glacier Deep Archive Has default retrieval time of 12h. Designed to meet regulatory compliance requirements and store data sets for 7-10 years. Minimum storages duration of 180 days. S3 on Outpost Delivers object storage to your on-premise"},{"location":"cloud/aws/cert-sap/#cost-optimization_2","title":"Cost optimization","text":"<ul> <li>Amazon S3 Storage Class Analysis observes data access patterns over a period of time. Can be for an entire bucket or filter by prefix and/or tags (up to 1000 filters per bucket). It allows daily exports of analysis data. It takes 24h to generate the first report.</li> <li>Amazon S3 Intelligent-Tiering optimizes costs by automatically moving data between three access tiers with the option to activate a fourth and fifth archive and deep archival tiers. The first tier is optimized for frequent Access, the next lower-cost tier is optimized for infrequent Access, and the Archive Instant Access tier is a very low-cost tier optimized for rarely accessed data.</li> <li>The archive access tier that must be activated before using. Once activated, S3 Intelligent Tiering moves data that has not been accessed for 90+ consecutive day to the this archival tier. This tier has the same performance as the S3 Glacier Flexible Retrieval storage class. The last, optional tier, is the Deep Archive Access tier. Once activated, objects that have not been access for 180 days automatically move to this lowest cost tier. This tier has the same performance as the S3 Glacier Deep Archive storage class.</li> <li>Amazon S3 inventory is one of the tools Amazon S3 provides to help manage your storage. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs.</li> <li>Amazon S3 Server access logging provides detailed records for the requests that are made to a bucket.</li> <li>S3 Storage Lens delivers organization-wide visibility into object storage usage, activity trends, and makes actionable recommendations to improve cost-efficiency and apply data protection best practices.</li> <li>AWS Budgets allows you to set custom budgets to track your cost and usage.</li> <li>AWS Cost and Usage Reports provides both a billing and usage report.</li> <li>Use Amazon QuickSight to visualize S3 usage reports.</li> <li>Object data in a lifecycle policy can transition between storage classes in a downward, waterfall model.</li> <li>Amazon S3 does not transition objects between storage classes if they are smaller than 128 KB because it's not cost effective to do so.</li> <li>Objects must remain for a minimum of 30 days in S3 Standard before they can transition to S3 Standard-IA, and S3 One Zone-IA.</li> <li>Objects in S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA storage are charged for a minimum storage duration of 30 days, and objects deleted before 30 days incur a pro-rated charge equal to the storage charge for the remaining days.</li> <li>You can define separate lifecycle rules for current and noncurrent object versions.</li> <li>When an object reaches the end of its lifetime, Amazon S3 queues it for removal and removes it asynchronously. You are not charged for storage time associated with an object that has expired.</li> <li>If the current object version is not a delete marker,  the lifecycle expiration action causes Amazon S3 to add a delete marker with a unique version ID.</li> <li>In a versioning-suspended bucket, the expiration action causes Amazon S3 to create a delete marker with null as the version ID. This delete marker replaces any object version with a null version ID in the version hierarchy, which effectively deletes the object.</li> <li>For non-version-enabled buckets, when an object expires, the object is permanently deleted.</li> <li>Lifecycle configuration on multi-factor authentication (MFA)-enabled buckets is not supported.</li> <li>Lifecycle actions are not captured by AWS CloudTrail object level logging. If logging is required, you can use Amazon S3 Server access logs to capture S3 lifecycle-related actions.</li> <li>You should consider using multipart uploads if your object size is over 100 MB or uploading files over a network with inconsistent or erratic uptime.</li> <li>If you initiate a multipart upload but the upload does not complete, the in-progress upload occupies storage space and incurs storage charges. After you initiate a multipart upload, Amazon S3 retains all the parts until you either complete or stop the upload. Throughout its lifetime, you are billed for all storage, bandwidth, and requests for this multipart upload and its associated parts. A Lifecycle rule can be configured to automatically remove incomplete uploads (AbortIncompleteMultipartUpload).</li> <li>Each unit of provisioned capacity allows at least three expedited retrievals (Glacier Flexible Retrieval) to be performed every five minutes and provides up to 150 MB/s of retrieval throughput.</li> <li>The S3 Glacier storage classes must complete a restore request job before you can view the output. After completion, a job will not expire for at least 24 hours (you can configure number of days to be kept), allowing you to download the output within the 24-hour period after the job is completed. if you want to keep it permanently yo need to do a copy of it.</li> <li>You can specify an Amazon SNS topic to which S3 Glacier Flexible Retrieval can post a notification after the job is completed. S3 Glacier Flexible Retrieval sends a notification only after it completes the job (Configure in event notification). It also can send to SQS and Lambda function.</li> <li>Amazon S3 Storage Lens provides organization-wide visibility into object storage usage and activity trends.</li> <li>Amazon S3 Storage Lens requires specific permissions in IAM to authorize access to S3 Storage Lens actions. To do this, you attach the IAM policy to IAM users, groups, or roles to grant them permissions to enable or disable S3 Storage Lens, or to access any S3 Storage Lens dashboard or configuration. You cannot use your account's root user credentials to view Amazon S3 Storage Lens dashboards. To access S3 Storage Lens dashboards, you must grant the requisite IAM permissions to a new or existing IAM user and then log in with those user credentials to access S3 Storage Lens dashboards. </li> </ul>"},{"location":"cloud/aws/cert-sap/#encryption","title":"Encryption","text":""},{"location":"cloud/aws/cert-sap/#server-side","title":"Server-side","text":"<ul> <li>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3): each object encrypts with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Uses AES-256.</li> <li>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS): similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a CMK that provides added protection against unauthorized access of your objects in Amazon S3. SSE-KMS also provides you with an audit trail showing when and who used the CMK. Additionally, you can choose to create and manage customer managed CMKs, or use AWS managed CMKs that are unique to you, your service, and your Region. AWS KMS generates a bucket-level key that is used to create unique data keys for new objects that you add to the bucket. This S3 Bucket Key is used for a time-limited period within Amazon S3, reducing the need for Amazon S3 to make requests to AWS KMS to complete encryption operations.</li> <li> <p>Server-Side Encryption with Customer-Provided Keys (SSE-C): you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. With this option, the customer is responsible for managing and rotating the keys, and without access to these keys the Amazon S3 data can not be decrypted.</p> </li> <li> <p>If you have existing objects in your bucket and you set the default encryption, the setting does not retroactively encrypt existing objects. To encrypt existing objects, use Amazon S3 Batch Operations.</p> </li> <li>If you are using a custom AWS KMS key, you must grant users access to use the key. Otherwise, they will not be able to decrypt the objects.</li> <li>The default encryption on the bucket is used on all objects unless the object PUT request header contains a different encryption method.</li> </ul>"},{"location":"cloud/aws/cert-sap/#client-side-encryption","title":"Client-side encryption","text":"<ul> <li>You encrypt the data before sending it to S3 and decrypt it when received. It's implemented in the SDKs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#encrypted-connections","title":"Encrypted connections","text":"<ul> <li>To protect data in transit, use an S3 bucket policy to force the use of HTTPS requests. (Deny actions with condition of SecureTransport false)</li> <li>You can use AWS Config rules to implement ongoing detective controls using the s3-bucket-ssl-requests-only AWS Config managed rule.</li> </ul>"},{"location":"cloud/aws/cert-sap/#service-integration","title":"Service Integration","text":""},{"location":"cloud/aws/cert-sap/#as-a-data-lake","title":"As a data lake","text":"<ul> <li>Decoupling storage from compute</li> <li>Centralize data architecture</li> <li>Integration with clusterless and serverless AWS services (Amazon Athena, Amazon Redshift Spectrum, Amazon Rekognition, and AWS Glue)</li> <li>Standardized APIs</li> <li>You can use AWS Glue Data Catalog or create a data catalog with Lambda, Dynamodb and Elasticsearch Service.</li> <li>Amazon Athena: interactive query serverless service that makes it easy for you to analyze data directly in Amazon S3, using standard SQL. Can process unstructured, semi-structured, and structured data sets. It integrates with Amazon QuickSight for easy visualization. It can also be used with third-party reporting and business intelligence tools by connecting these tools to Athena with a JDBC driver. Typically used for ad hoc data discovery and SQL querying.</li> <li>Amazon Redshift Spectrum: enables you to run Amazon Redshift SQL queries directly against data stored in an Amazon S3-based data lake. You can directly query a wide variety of data assets stored in the data lake, including CSV, TSV, Parquet, Sequence, and RCFile. Typically used for more complex queries and scenarios where a large number of data lake users want to run concurrent BI and reporting workloads.</li> <li>Amazon FSx for Lustre file systems can link to Amazon S3 buckets, allowing you to access and process data concurrently from a high-performance file system.</li> </ul>"},{"location":"cloud/aws/cert-sap/#analytics","title":"Analytics","text":"<ul> <li>S3 Storage Lens: Delivers organization-wide visibility into object storage usage and activity trends. Usage metrics describe the size, quantity, and characteristics of your storage. Provides recommendations. Includes drill-down options to generate insights at the organization, account, Region, bucket, or even prefix level.</li> <li>S3 Storage Class Analysis: Analyzes storage access patterns to help you determine when to transition less frequently accessed storage to a lower-cost storage class. You can configure by bucket, prefix and or tags.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-s3-block-public-access-settings","title":"Amazon S3 Block Public Access settings","text":"<ul> <li>Block all public access</li> <li>Block public access granted through new ACLs (any existing ACLs or policies granting public access permissions will not be affected and public access to those resources will remain intact)</li> <li>Block public access granted through any ACLs (you are not prevented from creating new ACLs that would normally grant public access but they are ignored)</li> <li>Block public access granted through new public bucket policies (recommended at account level)</li> <li>Block public and cross-account access granted through any public bucket policies (If you have any ACLs granting public access to buckets and objects will remain publicly accessible)</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-s3-object-ownership","title":"Amazon S3 Object Ownership","text":"<ul> <li>With Amazon S3 Object Ownership, the bucket owner, now has full control of the objects, and may own any new objects written by other accounts automatically.</li> <li>Object writer \u2013 The account that is writing the object owns the object.</li> <li>Bucket owner preferred \u2013 The bucket owner will own the object if uploaded with the bucket-owner-full-control canned ACL. Without this setting and canned ACL, the object is uploaded to the bucket but remains owned by the uploading account (add a bucket policy to require all Amazon S3 PUT operations to include the bucket-owner-full-control canned ACL).</li> <li>After you set the S3 Object Ownership to Bucket owner preferred, you can add a bucket policy to require all Amazon S3 PUT operations to include the bucket-owner-full-control canned ACL. If the uploader fails to meet the ACL requirement in their upload, the request fails. This setting enables bucket owners to enforce uniform object ownership across all newly uploaded objects in their buckets. </li> </ul>"},{"location":"cloud/aws/cert-sap/#access-policies","title":"Access policies","text":"<ul> <li>Bucket policy size limit of up 20 KB.</li> <li>Each bucket and object has an ACL attached to it as a subresource. The ACL defines which AWS accounts or groups are granted access and the type of access.</li> <li>For cross-account access, use ACLs to control which principals in another account can access a resource. You cannot use ACLs to control access for a principal in the same account.</li> <li>Amazon S3 also evaluates the corresponding resource policies (bucket policy, bucket ACL, and object ACL) at the same time.</li> <li>You can use a bucket policy to control access to the bucket from only specified VPC endpoints or specific VPCs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#access-points","title":"Access Points","text":"<ul> <li>Access points are named network endpoints that attach to S3 buckets. They enforce distinct permissions and network controls for any request made through the access point.</li> <li>Name must be unique by account and regions, dns compliant, between 3 and 50 characters, not have upper, periods or underscores.</li> <li>Can only be associated with one bucket.</li> <li>Can create up to 1000 per account and region.</li> <li>Policies limited to 20 KB.</li> <li>Only used to perform operations on objects.</li> <li>Don't work with all AWS services (can't be used as destination for S3 cross-region replication).</li> <li>You can find logging for Amazon S3 requests in CloudTrail logs. These include requests made through access points and requests made to the APIs that manage access points.</li> <li>Access point ARNs use the format:  arn:partition:service:region:account-id:accesspoint/resource</li> <li>Objects accessed through an access point use the ARN format: arn:partition:service:region:account-id:accesspoint/access-point-name/object/resource</li> <li>To configure a bucket in which all access is through access control policies, configure the bucket policy to allow full access. Then, create access point policies to restrict access based on your user or application needs.</li> <li>During access point creation, you can choose requests to originate from a specific VPC. Alternatively, you can choose to make the access point accessible from the internet. An access point that's accessible only from a specified VPC has a network origin of VPC. Amazon S3 rejects any request made to the access point that doesn't originate from that VPC. An access point that's accessible from the internet is said to have a network origin of Internet.</li> <li>Adding an S3 access point to a bucket doesn't change the bucket's behavior when accessed through the existing bucket name or ARN. All existing operations against the bucket continue to work as before. Restrictions that you include in an access point policy apply only to requests made through that access point.</li> <li>Amazon S3 Access Points support independent Block Public Access settings for each access point.</li> <li>For any request made through an access point, Amazon S3 evaluates the Block Public Access settings for that access point, the underlying bucket, and the bucket owner's account.</li> </ul>"},{"location":"cloud/aws/cert-sap/#presigned-urls","title":"Presigned URLs","text":"<ul> <li>Someone who has permission to perform the operation must create the presigned URL.</li> <li>Presigned url generated from IAM user have a maximum expiration time of 7 days (12 hours if in web console).</li> <li>If you created a presigned URL using a temporary token (Security Token Service), then the URL expires when the token expires (up to 36 hours), even if you created the URL with a later expiration time.</li> <li>If you create a presigned URL using an IAM instance profile, it is valid up to 6 hours, even if you specify a later expiration date.</li> <li>You can define a policy with the <code>s3:signatureAge</code> condition to deny any presigned url with signature more than X seconds old.</li> </ul>"},{"location":"cloud/aws/cert-sap/#online-data-transfer-services","title":"Online data transfer services","text":"<ul> <li>AWS DataSync: To move large amounts of data online between on-premises storage and Amazon S3.</li> <li>AWS Transfer Family: Provides fully managed support for file transfers directly into and out of Amazon S3.</li> <li>Amazon S3 Transfer Acceleration: Secure transfers of files over long distances. Takes advantage of Amazon CloudFFront.</li> <li>Amazon Kinesis Data Firehose: To stream data into Amazon S3, a fully managed streaming service. Because it captures and automatically loads streaming data in Amazon S3 and Amazon Redshift, you get near-real-time analytics with the business intelligence tools that you already use.</li> <li>Amazon Kinesis Data Streams (KDS): Enables you to build custom applications that process or analyze streaming data for specialized needs. You can also emit data to other AWS services, such as Amazon S3, Amazon Redshift, Amazon EMR, and AWS Lambda.</li> <li>Amazon Partner Network: Con can use third-party connectors.</li> </ul>"},{"location":"cloud/aws/cert-sap/#offline-data-transfer-services","title":"Offline data transfer services","text":"<ul> <li>AWS Snowcone</li> <li>AWS Snowball</li> <li>AWS Snowmobile</li> </ul>"},{"location":"cloud/aws/cert-sap/#hybrid-cloud-storage-services","title":"Hybrid cloud storage services","text":"<ul> <li>AWS Direct Connect: Dedicated network connection from your on-premise data center to AWS.</li> <li>AWS Storage Gateway: AWS Storage Gateway configured as a File Gateway enables you to connect your Amazon S3 bucket using either the Network File System (NFS) or Server Message Block (SMB) protocol with local caching.</li> </ul>"},{"location":"cloud/aws/cert-sap/#unmanaged-cloud-data-migration-tools","title":"Unmanaged cloud data migration tools","text":"<ul> <li>rsync and 3rd party tools</li> <li>Amazon S3 and the AWS CLI</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-snowcone","title":"AWS Snowcone","text":"<ul> <li>The smallest component.</li> <li>AWS DataSync comes pre-installed (you also can send data offline by shipping).</li> <li>Purpose-build for use outside of a traditional datacenter.</li> <li>Can run edge computing workloads that use AWS IoT Greengrass or Amazon Elastic Compute Cloud (Amazon EC2) instances.</li> <li>Formatted in NFS when import to S3 job type and in EBS with compute and storage job type.</li> <li>AWS Snowcone devices are not available in a cluster configuration.</li> </ul>"},{"location":"cloud/aws/cert-sap/#jobs","title":"Jobs","text":"<ul> <li>Local compute and storage job</li> <li>Import to Amazon S3 job</li> <li>Export from Amazon S3 job</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-snowball","title":"AWS Snowball","text":"<ul> <li>Edge computing, data migration, and edge storage device.</li> <li>You can it for data collection, ML processing and storage in environments with intermittent connectivity or in remote disconnected locations.</li> <li>Comes in three options, storage optimized (with and without compute) and compute optimized.</li> <li>Storage Optimized devices provide 24 vCPUs of compute capacity (if is with compute), coupled with 80 terabytes (TB) of usable block or Amazon S3-compatible object storage.</li> <li>Compute Optimized devices provide 52 vCPUs, 42 TB of usable block or object storage and an optional GPU.</li> </ul>"},{"location":"cloud/aws/cert-sap/#jobs_1","title":"Jobs","text":"<ul> <li>Local compute and storage job</li> <li>Import to Amazon S3 job</li> <li>Export from Amazon S3 job</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-snowmobile","title":"AWS Snowmobile","text":"<ul> <li>Exabyte-scale data transfer service.</li> <li>You can transfer up to 100 PB per Snowmobile.</li> </ul>"},{"location":"cloud/aws/cert-sap/#features","title":"Features","text":"<ul> <li>Fast transfer even at massive scale</li> <li>Strong encryption: you provide the keys (can manage in KMS)</li> <li>Rugged, durable and more secure</li> <li>Customized for your needs</li> <li>Massively scalable</li> <li>Easy data retrieval</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases_8","title":"Use cases","text":"<ul> <li>Cloud data migration</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-outpost","title":"AWS Outpost","text":"<ul> <li>On-premises cloud storage that includes EBS and S3 services.</li> <li>Offers the same functionality but on-premise.</li> <li>Ideal for workloads that require low latency to on-premise systems, local data processing and data residency or migration of applications with local system interdependencies.</li> <li>AWS compute, storage, database, and other services run locally on Outposts. You can access the full range of AWS services available in the Region to build, manage, and scale your on-premises applications using familiar AWS services and tools.</li> <li>You can create subnets.</li> <li>You can create resources like EC2, EBS, ECS, EKS, ElastiCache, EMR, S3, ALB, App Mesh Envoy proxy and RDS.</li> <li>Requires connectivity to an AWS Region. A service link is a network route that enables communication between your Outpost and its associated AWS Region. Each Outpost is an extension of an Availability Zone and its associated Region.</li> </ul>"},{"location":"cloud/aws/cert-sap/#compute-capabilities","title":"Compute capabilities","text":"<ul> <li>General purpose (M5/M5d)</li> <li>Compute optimized (C5/C5d)</li> <li>Memory-optimized (R5/R5d)</li> <li>Graphics optimized (G4dn)</li> <li>I/O optimized (I3en)</li> <li>Support for EC2 instances powered by Graviton processors, such as C6g, M6g, and R6g</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-ebs_1","title":"Amazon EBS","text":"<ul> <li>EBS gp2</li> <li>Offered in tiers of 11 TB, 33 TB, and 55 TB</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-s3","title":"Amazon S3","text":"<ul> <li>S3 on Outposts provides an Amazon S3 storage class named S3 Outposts, which uses the S3 APIs.</li> <li>You can add 26 TB, 48 TB, or 96 TB of Amazon S3 storage capacity to your Outposts.</li> <li>You can create up to 100 buckets on each Outpost.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-ebs-snapshots","title":"Amazon EBS snapshots","text":"<ul> <li>Snapshots of EBS volumes on your Outpost are stored on Amazon S3 in the same AWS Region.</li> <li>Local snapshots on Outposts require that you provision your Outpost with S3.</li> <li>You can use local snapshots on Outposts for disaster recovery and backup.</li> <li>You can migrate workloads from any source directly onto Outposts, or from one Outpost to another. You can perform the migration without requiring the snapshot data to go through the Region.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cloudendure","title":"CloudEndure","text":"<ul> <li>AWS offers CloudEndure Migration to migrate workloads onto AWS Outposts from physical, virtual, or cloud-based sources.</li> <li>CloudEndure Disaster Recovery offers scalable, cost-effective business continuity for physical, virtual, and cloud-based workloads onto AWS Outposts.</li> <li>CloudEndure Disaster Recovery improves resilience, enabling recovery point objectives (RPOs) of seconds and recovery time objectives (RTOs) of minutes.</li> </ul>"},{"location":"cloud/aws/cert-sap/#networking_1","title":"Networking","text":"<ul> <li>VPC extension: Extend an existing VPC to your Outpost in your on-premises location. You can create a subnet in your Regional VPC. You can associate it with an Outpost. Instances in Outpost subnets communicate with other instances in the AWS Region using private IP addresses, all within the same VPC.</li> <li>Local gateway: Each Outpost provides a new local gateway that you can use to connect your Outpost resources with your on-premises networks.</li> <li>Load balancer: You can provision an ALB to automatically distribute incoming HTTP and HTTPS traffic across multiple targets on your Outposts. Scales automatically.</li> <li>Private Connectivity: Establish a service link VPN connection from your Outposts to the AWS Region over AWS Direct Connect.</li> </ul>"},{"location":"cloud/aws/cert-sap/#security-and-compliance_2","title":"Security and compliance","text":"<ul> <li>Enhanced security with AWS Nitro: Continuously monitors, protects, and verifies your Outpost instance hardware and firmware. </li> <li>Security model: AWS is responsible for protecting Outposts infrastructure similar to how it secures infrastructure in the cloud. You are responsible for securing your applications and physical security.</li> <li>Securing data: Data at rest is encrypted, data in transit is encrypted through the Service Link and data is deleted when instances are terminated.</li> </ul>"},{"location":"cloud/aws/cert-sap/#access-regional-services","title":"Access Regional services","text":"<ul> <li>Is an extension of a Region.</li> <li>You can access all Regional AWS services in your private VPC environment through interface endpoints, gateway endpoints, or their Regional public endpoints.</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases_9","title":"Use cases","text":"<ul> <li>Low latency compute</li> <li>Local data processing</li> <li>Data residency</li> <li>Migration and modernization</li> </ul>"},{"location":"cloud/aws/cert-sap/#pricing_6","title":"Pricing","text":"<ul> <li>Pricing depends on the EC2 instance types and EBS volumes.</li> <li>Includes delivery, installation and maintenance of the equipment.</li> <li>You can add S3 for an additional fee.</li> <li>Purchase is for a 3-year term and you can pay all upfront, partial upfront and no upfront. If not all upfront, monthly charges apply.</li> <li>Any upfront charges apply 24 hours after your Outpost is installed and the compute and storage capacity is available for use.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-storage-gateway","title":"AWS Storage Gateway","text":"<ul> <li>Connects on-premises users and applications using a software appliance with cloud-based storage.</li> </ul>"},{"location":"cloud/aws/cert-sap/#types","title":"Types","text":""},{"location":"cloud/aws/cert-sap/#amazon-s3-file-gateway","title":"Amazon S3 File Gateway","text":"<ul> <li>Amazon S3 File Gateway provides a seamless way to connect to the cloud to store application data files and backup images as durable objects in Amazon S3.</li> <li>Amazon S3 File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching.</li> <li>POSIX-style metadata, including ownership, permissions, and timestamps, are durably stored in Amazon S3 in the user-metadata of the object associated with the file.</li> <li>When objects are transferred to S3, you can manage them as native S3 objects.</li> <li>You can use S3 File Gateway to back up on-premises file data as objects in Amazon S3.</li> <li>Integrate with SAP, SQL Server, Oracle, Hadoop Distributed File System (HDFS), and other applications.</li> <li>The S3 File Gateway appliance is configured with a file share. Each file share is paired with a single S3 bucket and uses the appliance's local cache. A specific S3 File Gateway appliance can have multiple NFS and SMB file shares.</li> <li>Files written to the file share become objects in the S3 bucket, with a one-to-one mapping between files and objects. Metadata, such as ownership and timestamps, are stored with the object. File paths become part of the object's key, and thus maintain consistent name space. Objects in Amazon S3 appear as files to the on-premises clients.</li> <li>You can access your data directly in Amazon S3.</li> <li>You can implement storage management capabilities, such as versioning, lifecycle management, and cross-Region replication.</li> <li>S3 File Gateway also publishes audit logs for SMB file share user operations to CloudWatch.</li> <li>A cache refresh will find objects in the S3 bucket that were added, removed, or replaced since the gateway last listed the bucket's contents and cached the results. It will then refresh the metadata and cached inventory in the gateway appliance.</li> <li>You can configure Storage Gateway for automated cache refresh based on a timer value between 5 minutes and 30 days.</li> <li>The Storage Gateway can also be refreshed using the RefreshCache API operation.</li> <li>You can refresh a cache at the bucket or prefix level.</li> <li>Each file share needs to be connected to an S3 bucket and given access to the bucket through an IAM role with a trust policy.</li> <li>When you create a new file share, there are some settings that you will not be able to change, such as the bucket or access point or the VPC endpoint settings.</li> <li>You can change other settings of the file share configuration after the file share has been created. For example, you can edit the storage class for your S3 bucket, edit the file share name, export as read-write or read-only, automatically refresh cache settings, and more. What you can change is specific to SMB and NFS.</li> <li>You can configure your gateway to notify you when a file has been fully uploaded to Amazon S3 by the file gateway. Storage Gateway can invoke Amazon EventBridge when your file operations are completed. This in turn, can send an event to a target such as SNS.</li> </ul>"},{"location":"cloud/aws/cert-sap/#planning-and-designing-the-deployment","title":"Planning and designing the deployment","text":"<ul> <li> <ol> <li>Deploy the gateway appliance. In on-premises choose from virtual or physical appliance. In AWS you can deploy as an AMI in EC2.</li> </ol> </li> <li> <ol> <li>Connectivity using public endpoint, VPC endpoint over Direct Connect of VPN or FIPS compliant endpoints.</li> </ol> </li> <li> <ol> <li>Configure local disks as cache. It is recommended to allocate at least 20 percent or 150 GiB. You can increase adding new disks. The maximum supported size of the local cache for a gateway running on a VM is 64 TiB. To optimize gateway performance, consider adding high-performance disks such as solid state drives (SSDs) or an NVMe controller, or attach a virtual disk to your VM directly. A high-performance disk generally results in better throughput and more input/output operations per second (IOPS).</li> </ol> </li> <li> <ol> <li>Add file shares to the gateway appliance. Each file share is associated with a unique S3 bucket or unique prefix on the same bucket. Currently file metadata, such as ownership, stored as S3 object metadata cannot be mapped across different protocols. A file gateway, however, can host one or more file shares of different types.</li> </ol> </li> <li> <ol> <li>Select the S3 storage class for the file share. It is recommended to use standard and use lifecycle policies.</li> </ol> </li> </ul>"},{"location":"cloud/aws/cert-sap/#reads-writes-and-updates","title":"Reads, Writes and Updates","text":"<ul> <li>On premises, the file share uses a local cache that provides low-latency access to recently accessed data and reduces outgoing data charges.</li> <li>The S3 File Gateway also stores a local inventory of the objects in the S3 bucket and the metadata. The inventory is used to provide low-latency access for file system operations; for example, listing an inventory.</li> <li>For read requests from the client, first check to see if the data is in the cache. If the data is not in the cache, it's fetched from the S3 bucket using byte-range gets to better use available bandwidth.</li> <li>The gateway performs several optimizations. For example, if you read the file in parts, the whole file is not necessarily pulled into the cache. The gateway tries to predict patterns and do some pre-fetching and read-aheads. If you are reading a video file, for example, and you are reading it serially front to back, the gateway will recognize the pattern and try to pre-fetch ahead of your operations. This is useful for avoiding buffering and avoids the latency of demand caching.</li> <li>When the client writes data to the gateway appliance, using either the NFS or SMB protocols, the gateway stores the data locally. The data is compressed asynchronously, and changed data is uploaded securely. Changes to the files asynchronously update the objects in the S3 bucket using optimized data transfers (such as multipart parallel uploads).</li> <li>To reduce data transfer overhead, the gateway uses multipart uploads and copy put, so only changed data in your files is uploaded to Amazon S3. Then, data that is already in the cloud is used to create a new version of the object (UploadPartCopy).</li> </ul>"},{"location":"cloud/aws/cert-sap/#file-share-health","title":"File share health","text":"Status Meaning AVAILABLE The file share is configured properly and is available to use. The normal running status. CREATING The file share is being created and is not ready for use. UPDATING The file share configuration is being updated. DELETING The file share is being deleted. Not deleted until all data is uploaded to AWS. FORCE_DELETING The file share is being deleted forcibly. Deleted immediately, uploading ceases. UNAVAILABLE The file share is in an unhealthy state."},{"location":"cloud/aws/cert-sap/#multi-writer-best-practices","title":"Multi-writer best practices","text":"<ul> <li>When multiple gateways or file shares write to the same S3 bucket, unpredictable results might occur.</li> <li>Configure your S3 bucket so that only one file share can write to it. You can create an S3 bucket policy that denies all roles, except the role used for the specific file share, to put or delete objects in the bucket and attach this policy to the S3 bucket. The best practice for secondary gateways is to either use an IAM role that is prevented from writing to the bucket or permit the export of file shares as read-only. These measures can prevent accidental writes to the bucket from the secondary clients at the Amazon S3 level and at Storage Gateway level.</li> <li>If you want to write to the same Amazon S3 bucket from multiple file shares, you must prevent the file shares from trying to write to the same objects simultaneously. To do this, you configure a separate, unique object prefix for each file share. This means that each file share will only write to objects with its corresponding prefix. It will not write to objects associated with the other file shares in your deployment. You configure the object prefix in the S3 prefix name field when you create a new file share.</li> </ul>"},{"location":"cloud/aws/cert-sap/#access-to-the-file-share","title":"Access to the file share","text":"<ul> <li>NFS: can limit access to specific NFS clients or networks by IP, permit read-only or read-write and activate user permission squashing.</li> <li>SMB: can limit access for AD users only or providing authenticated guests access to users, permit read-only or read-write, controlling file or directory access by POSIX or ACLs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cloudwatch-metrics","title":"CloudWatch Metrics","text":"Metric Description AvailabilityNotifications Number of availability-related health notifications generated by the gateway CacheHitPercent Percentage of application read operations from the file shares that are served from cache CachePercentDirty The file share's contribution to the overall percentage of the gateway's cache not persisted to AWS CachePercentUsed The file share's contribution to the overall percentage use of the gateway's cache storage CloudBytesUploaded Total number of bytes that the gateway uploaded to AWS during the reporting period CloudBytesDownloaded Total number of bytes that the gateway downloaded from AWS during the reporting period HealthNotifications Number of health notifications generated by this gateway in the reporting period IoWaitPercent Percentage of time that the gateway is waiting on a response from the local disk ReadBytes Total number of bytes read from your on-premises applications in the reporting period WriteBytes Total number of bytes written from your on-premises applications in the reporting period WriteTime Total number of milliseconds spent on write operations from your on-premises application <ul> <li>Analyze the CloudBytesDownloaded and CloudBytesUploaded metrics to understand throughput between your S3 File Gateway and the AWS Cloud. </li> <li>Calculate the percentage of read requests that are served from the cache by using the CacheHitPercent metric with the Average statistic. </li> <li>Use the WriteTime metric with the Average statistic to measure latency.</li> <li>Monitor performance of your Storage Gateway by monitoring metrics such as CachePercentDirty. The higher the percentage of dirty cache (data that has not been written to Amazon S3), the lower the space available for low-latency data.</li> <li>For each activated gateway, we recommend that you create the following CloudWatch alarms:</li> <li>High IO wait: IoWaitpercent &gt;= 20 for 3 datapoints in 15 minutes</li> <li>Cache percent dirty: CachePercentDirty &gt; 80 for 4 datapoints within 20 minutes</li> <li>Availability notifications: AvailabilityNotifications &gt;= 1 for 1 datapoints within 5 minutes</li> <li>Health notifications: HealthNotifications &gt;= 1 for 1 datapoints within 5 minutes</li> <li>Storage Gateway generates events that EventBridge uses.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-fsx-file-gateway","title":"Amazon FSx File Gateway","text":"<ul> <li>Amazon FSx File Gateway optimizes on-premises access to fully managed, highly reliable file shares in Amazon FSx for Windows File Server.</li> <li>A local cache of frequently used data that you can access is stored, providing faster performance and reduced data transfer traffic.</li> <li>FSx File Gateway stores your data natively as files rather than as objects.</li> <li>Customers with unstructured or file data, whether from SMB-based group shares or business applications, might require on-premises access to meet low-latency requirements.</li> <li>FSx File Gateway is a solution for replacing on-premises NAS.</li> <li>It facilitates user or team file shares and file-based application migration shares in Amazon FSx for Windows File Server, using the SMB protocol.</li> <li>Files written through FSx File Gateway can be directly accessed in FSx for Windows File Server.</li> </ul>"},{"location":"cloud/aws/cert-sap/#tape-gateway","title":"Tape Gateway","text":"<ul> <li>Tape Gateway is used to replace physical tapes on premises with virtual tapes in AWS without changing existing backup workflows.</li> <li>Tape Gateway supports all leading backup applications and caches virtual tapes on premises for low-latency data access.</li> <li>Tape Gateway presents a virtual tape library (VTL) to your backup application using storage open standard iSCSI protocol.</li> <li>Stores your virtual tapes in service-managed S3 buckets.</li> <li>Move the data to an archive tier to further reduce storage costs (Amazon S3 Glacier or Amazon S3 Glacier Deep Archive).</li> </ul>"},{"location":"cloud/aws/cert-sap/#volume-gateway","title":"Volume gateway","text":"<ul> <li>Volume Gateway presents cloud-backed iSCSI block storage volumes to your on-premises applications. </li> <li>Volume Gateway stores and manages on-premises data in Amazon S3 on your behalf and operates in cache mode or stored mode.</li> <li>In the cached mode, your primary data is written to Amazon S3, while retaining your frequently accessed data locally in a cache for low-latency access.</li> <li>In the stored mode, your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS.</li> <li>In either mode, you can take point-in-time copies of your volumes, which are stored as Amazon EBS snapshots in AWS. With this feature, you can make space-efficient versioned copies of your volumes for data protection, recovery, migration, and various other copy data needs.</li> <li>Because Volume Gateway integrates with AWS Backup, you can use the AWS Backup service to protect on-premises applications that use Storage Gateway volumes.</li> <li>The Volume Gateway cached mode is deployed into your on-premises or AWS Cloud environment as a VM running on VMware ESXi, KVM, Microsoft Hyper-V hypervisor, Amazon EC2, or a physical gateway hardware appliance. Stored mode is only available with on-premises host platform options.</li> <li>To prepare data for upload to Amazon S3, your gateway also stores incoming data in a staging area, referred to as an upload buffer. You can use on-premises DAS or SAN disks for working storage.</li> <li>To recover a backup of your data, you can restore an Amazon EBS snapshot to an on-premises gateway storage volume. You can also use the snapshot as a starting point for a new Amazon EBS volume, which you can then attach to an Amazon EC2 instance for processing in the cloud.</li> <li>Volume resizing for the gateway is not supported. To decrease the storage capacity, you will need to create a new gateway and migrate your data to the new gateway. To increase storage capacity, you add new disks to the gateway instead of expanding disks previously allocated.</li> <li>If you ofter use processes that require reading all of the data on the entire volume, use stored mode instead of cached mode.</li> <li>EBS snapshots are stored in an S3 service bucket instead of a customer bucket.</li> <li>The gateway software running as a virtual machine (VM) or on the hardware appliance is stateless, so you can easily create and manage new instances of your gateway as your storage needs evolve.</li> <li>The host platform can be a VMware ESXi, Microsoft Hyper-V, Linux KVM, Amazon EC2 and hardware appliance.</li> <li>AWS Storage Gateway provides public, VPC, and Federal Information Processing Standards (FIPS) service endpoints.</li> <li>You can connect a gateway to the service either using public internet or through Direct Connect.</li> <li>Storage Gateway supports authentication between your gateway and iSCSI initiators by using Challenge-Handshake Authentication Protocol (CHAP). Data is transferred over the internet, secured by Secure Sockets Layer (SSL)/Transport Layer Security (TLS), from the gateway appliance to AWS.</li> <li>CHAP provides protection against man-in-the-middle and playback attacks by periodically verifying the identity of an iSCSI initiator as authenticated to access a storage volume target. For each volume target, you can define one or more CHAP credentials.</li> <li>Assure your Storage Gateway and the gateway appliance are supported in your desired Regions.</li> <li>When deploying to VMware, Microsoft Hyper-V, and Linux KVM, you must synchronize the virtual machine's time with the host time before you can successfully activate your gateway. Make sure that your host clock is set to the correct time and synchronize it with an NTP server.</li> <li>You should choose a CloudWatch log group option for monitoring the health of your gateway.</li> <li>By default, stored volumes are assigned a snapshot schedule of once a day. This schedule can be edited by specifying either the time the snapshot occurs each day or the frequency (every 1, 2, 4, 8, 12, or 24 hours), or both.</li> <li>Storage Gateway doesn't create a default snapshot schedule for cached volumes. A default schedule is not needed because your data is durably stored in Amazon S3. You can create a snapshot schedule for a cached volume at any time if you want.</li> <li>You can't remove the default snapshot schedule for stored volumes, as they require at least one snapshot schedule.</li> <li>By default, an activated gateway has no rate limits on upload or download. You can limit (or throttle) the upload throughput from the gateway to AWS or the download throughput from AWS to your gateway. The minimum rate for download is 100 Kilobits (Kib) per second and the minimum rate for upload is 50 Kib per second. You can also assign bandwidth rate limits on a schedule.</li> <li>You can optimize iSCSI settings on your iSCSI initiator to achieve higher I/O performance. We recommend choosing 256 KiB for MaxReceiveDataSegmentLength and FirstBurstLength, and 1 MiB for MaxBurstLength.</li> <li>When you provision gateway disks, it is strongly recommended that you don't provision local disks for the upload buffer and cache storage that use the same underlying physical storage disk.</li> <li>If you find that adding more volumes to a gateway reduces the throughput to the gateway, consider adding the volumes to a separate gateway. In particular, if a volume is used for a high-throughput application, consider creating a separate gateway for the high-throughput application. However, as a general rule, you should not use one gateway for all of your high-throughput applications and another gateway for all of your low-throughput applications. To measure your volume throughput, use the ReadBytes and WriteBytes metrics.</li> </ul>"},{"location":"cloud/aws/cert-sap/#planning-and-designing-the-deployment_1","title":"Planning and designing the deployment","text":"<ul> <li> <ol> <li>Choose the right configuration. Cached for custom file shares, migrating application data. Stored for block storage backups, migrations, cloud-based disaster recovery.</li> </ol> </li> <li> <ol> <li>Deploy the gateway appliance. In on-premises choose from virtual or physical appliance. In AWS you can deploy as an AMI in EC2 (only for cached volume) and minimum recommended size of m5.xlarge, the main use cases for this scenario are proof of concept, disaster recovery and data mirroring.</li> </ol> </li> <li> <ol> <li>Gateway appliance sizing. Determine the number of total volumes and capacity, cached mode supports up to 32 volumes of 32 TiB and stored mode supports up to 32 volumes of 16 TiB. Estimate the application and workload volume, minimum of 150 GiB for cache storage (only for cached mode) and upload buffer storage, best practice for performance is to allocate multiple local disc for cache storage. Also deploy additional appliances to increase overall throughput if required. In VM if you have more than one data store, is recommended to use one for the cache storage and another for the upload buffer.</li> </ol> </li> <li> <ol> <li>Connectivity using public endpoint, VPC endpoint over Direct Connect of VPN or FIPS compliant endpoints.</li> </ol> </li> <li> <ol> <li>Adding volumes. After creation and activation, for a cached volume provision storage volumes backed by S3 and then mount these volumes to your on-premises application servers as iSCSI devices. For a stored volume, map them to on-premises direct-attached storage (DAS) or storage area network (SAN) disk, you can start with new disck or already holding data, then mount these storage volumes to your on-premises application as iSCSI devices.</li> </ol> </li> </ul> From To Protocol Port Usage Storage Gateway VM AWS TCP 443 (HTTPS) Communicate from a Storage Gateway outbound VM to an AWS service endpoint. Web Browser Storage Gateway VM TCP 80 (HTTP) Used by local systems to obtain the Storage Gateway activation key. Only used during activation. A Storage Gateway VM doesn't require port 80 to be publicly accessible. Storage Gateway VM DNS server UDP 53 (DNS) For communication between a Storage Gateway VM and the DNS server. Storage Gateway VM AWS TCP 22 (Support) Allows AWS Support to access your gateway for troubleshooting. Not needed for normal operation. Storage Gateway VM NTP server UDP 123 (NTP) Local systems use this protocol to synchronize VM time to the host time. iSCSI initiators Storage Gateway VM TCP 3260 (iSCSI) Used by local systems to connect to iSCSI targets exposed by a gateway."},{"location":"cloud/aws/cert-sap/#reads-writes-and-updates_1","title":"Reads, Writes and Updates","text":"<ul> <li>In cached volume the reads check first the cache, if not available then retrieves the data compressed from the S3, decompresses it, stores it in the local cache and returns it. This is know as read-through cache.</li> <li>In stored volume the reads are locally.</li> <li>In cached volume the writes are stored in the cache volume first and then compresses and encrypts the data ais ot moves the data from the cache into the upload buffer. This is known as write-back.</li> <li>In stored volume the writes happen directly to the local disk. When you create a snapshot of one of your volumes, the data will then be moved to AWS. Data is compressed and encrypted as it is moved to the upload buffer. From there, data is securely transferred to Amazon S3 in AWS.</li> <li>iSCSI The initiator is the client component of an iSCSI network. The initiator sends requests to the iSCSI target. Initiators can be implemented in software or hardware. Storage Gateway only supports software initiators.</li> <li>The iSCSI target is the server component of the iSCSI network that receives and responds to requests from initiators. Each of your volumes is exposed as an iSCSI target. Connect only one iSCSI initiator to each iSCSI target.</li> </ul>"},{"location":"cloud/aws/cert-sap/#recovery-options","title":"Recovery options","text":"<ul> <li>Volume clone: Instant clones offer rapid recovery of data to a gateway on premises. Cloning from an existing volume is faster and more cost effective than creating an Amazon EBS snapshot. Cloning does a byte-to-byte copy of your data from the source volume to the new volume, using the most recent recovery point from the source volume. You can create a new volume from any existing cached volume in the same AWS Region. The new volume is created from the most recent recovery point of the selected volume.</li> <li>EBS snapshot: Snapshots represent a point-in-time copy of the volume at the time the snapshot is requested. Data written to the volume by your application prior to taking the snapshot, but not yet uploaded to AWS, will be included in the snapshot.</li> </ul>"},{"location":"cloud/aws/cert-sap/#volume-status","title":"Volume status","text":"Status Meaning AVAILABLE The volume is available for use. The Available status is the normal running status for a volume. BOOTSTRAPPING The gateway is synchronizing data locally with a copy of the data stored in AWS. You typically don't need to take action for this status, because the storage volume automatically sees the Available status in most cases. CREATING The volume is currently being created and is not ready for use. The Creating status is transitional. No action is required. DELETING The volume is currently being deleted. The Deleting status is transitional. No action is required. IRRECOVERABLE An error occurred from which the volume cannot recover. PASS THROUGH Local data is out of sync with data stored in AWS. Data written to a volume while the volume is in Pass Through status remains in the cache until the volume status is Bootstrapping. This data starts to upload to AWS when Bootstrapping status begins. RESTORING The volume is being restored from an existing snapshot. This status applies only for stored volumes. RESTORING PASS THROUGH The volume is being restored from an existing snapshot and has encountered an upload buffer issue. This status applies only for stored volumes. UPLOAD BUFFER NOT CONFIGURED The volume cannot be created or used. No upload buffer is configured. ###### Attachment status Status Meaning ATTACHED The volume is attached to a gateway. DETACHED The volume is detached from a gateway. DETACHING The volume is being detached from a gateway. When you are detaching a volume and the volume doesn't have data on it, you might not see this status."},{"location":"cloud/aws/cert-sap/#cloudwatch-metrics_1","title":"CloudWatch Metrics","text":"Metric Description AvailabilityNotifications Number of availability-related notifications sent by the volume. CacheHitPercent Percent of application read operations from the volume that are served from cache. CachePercentDirty The volume's contribution to the overall percentage of the gateway's cache that isn't persisted to AWS. CachePercentUsed The volume's contribution to the overall percent use of the gateway's cache storage. CloudBytesUploaded The total number of bytes that the gateway uploaded to AWS during the reporting period. CloudBytesDownloaded The total number of bytes that the gateway downloaded from AWS during the reporting period. HealthNotifications The number of health notifications sent by the volume. IoWaitPercent Percent of time that the gateway is waiting on a response from the local disk. ReadBytes The total number of bytes read from your on-premises applications in the reporting period. WriteBytes The total number of bytes written to your on-premises applications in the reporting period. <ul> <li>Analyze the CloudBytesDownloaded and CloudBytesUploaded metrics to understand throughput between your gateway and the AWS Cloud.</li> <li>Calculate the percent of read requests that are served from the cache by using the CacheHitPercent metric with the Average statistic. The Average statistic of CloudWatch will sum all the metric values over a period of time and divide them by the number of samples. </li> <li>Use the ReadTime and WriteTime metrics with the Average statistic to measure latency.</li> <li>Monitor performance of your Storage Gateway by monitoring metrics like CachePercentDirty. The higher the percentage of dirty cache (overall percentage of the gateway's cache that has not persisted to AWS), the lower the space available for low-latency data.</li> <li>Use the ReadBytes and WriteBytes metrics with the Sum statistic to measure throughput from your on-premises applications coming into the gateway.</li> <li>For each activated gateway, it is recommended to create the following CloudWatch alarms:<ul> <li>High IO wait: IoWaitPercent &gt;= 20 for 3 datapoints in 15 minutes</li> <li>Cache percent dirty: CachePercentDirty &gt; 80 for 4 datapoints within 20 minutes</li> <li>Availability notifications: AvailabilityNotifications &gt;= 1 for 1 datapoints within 5 minutes</li> <li>Health notifications: HealthNotifications &gt;= 1 for 1 datapoints within 5 minutes</li> </ul> </li> </ul>"},{"location":"cloud/aws/cert-sap/#features_1","title":"Features","text":"<ul> <li>Standard storage protocols</li> <li>Fully managed cache</li> <li>Optimized and secure data transfer</li> <li>Native AWS integrated service</li> <li>High availability on VMware</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases_10","title":"Use cases","text":"<ul> <li>Low-latency access for on-premises applications to the cloud</li> <li>Use on-premise file shares backed by cloud storage</li> <li>Moving backups to the cloud</li> <li>Data protection and disaster recovery</li> </ul>"},{"location":"cloud/aws/cert-sap/#pricing_7","title":"Pricing","text":"<ul> <li>Charged based on the type and amount of storage you use, requests and data transferred out of AWS.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-transfer","title":"AWS Transfer","text":"<ul> <li>Provides fully managed support for file transfers directly into and out of Amazon S3 or Amazon EFS.</li> <li>Support for Secure File Transfer Protocol (SFTP), File Transfer Protocol over SSL (FTPS), and File Transfer Protocol (FTP).</li> <li>Integrates with the specified authentication system and provides DNS routing with Route 53.</li> <li>You can use Microsoft AD and LDAP for authentication or store amd manager users directly in the service.</li> <li>You can use IAM for security and identity management, CloudWatch for monitoring, KMS, S3 server-side encryption or customer managed keys to control encryption at rest. CloudTrail help meet compliance requirements.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-datasync","title":"AWS DataSync","text":"<ul> <li>Online data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS Storage services and between AWS Storage services.</li> <li>Used to migrate active datasets, archive data, replicate data or transfer data for analysis and processing.</li> <li>Can copy data between NFS, SMB, self-managed object storage, Snowcone, S3, EFS and FSx for Windows File Server file systems.</li> <li>Automatically recovers from network connectivity failures.</li> <li>A single DataSync task can fully use 10 Gbps over a network link between your on-premises environment and AWS.</li> <li>AWS DataSync supports asynchronous or one-direction at a time transfers between on-premises file systems to supported AWS Storage services in the AWS Cloud.</li> <li>DataSync also supports asynchronous data transfers between supported AWS Storage resources within the AWS Cloud.</li> <li>AWS DataSync can be scheduled or run on demand.</li> </ul>"},{"location":"cloud/aws/cert-sap/#features_2","title":"Features","text":"<ul> <li>Automatic infrastructure management.</li> <li>Data encryption and validation.</li> <li>Data transfer scheduling.</li> <li>File system integration and metadata preservation.</li> <li>Integration with WS infrastructure and management services: Supports VPC endpoints powered by AWS PrivateLink.</li> <li>Monitoring and auditing with Amazon CloudWatch and AWS CloudTrail.</li> </ul>"},{"location":"cloud/aws/cert-sap/#use-cases_11","title":"Use cases","text":"<ul> <li>Data migration.</li> <li>Data protection.</li> <li>Archiving cold data.</li> <li>Data processing for edge and hybrid workloads.</li> </ul>"},{"location":"cloud/aws/cert-sap/#pricing_8","title":"Pricing","text":"<ul> <li>You pay for only the amount of data that you copy.</li> </ul>"},{"location":"cloud/aws/cert-sap/#application-migration-service-aws-mgn","title":"Application Migration Service (AWS MGN)","text":"<ul> <li>Includes CloudEndure Migration, is a highly automated lift-and-shift (rehost) solution.</li> <li>You can use AWS MGN or CloudEndure Migration by itself to quickly lift-and-shift physical, virtual, or cloud servers without compatibility issues, performance impact, or long cutover windows.</li> <li>Continuously replicates your source servers to your AWS account. When you're ready to migrate, it automatically converts and launches your servers on AWS.</li> <li>AWS MGN provides similar capabilities as CloudEndure Migration, but it is available on the AWS Management Console.</li> <li>Replication Agent on your source servers continuously replicates the data to staging EBS. When you launch test or cutover instances, converts your source servers to run natively on AWS.</li> <li>For each source server you have a free period of 2.160 hours (90 days). Then your are charged per hour.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-backup","title":"AWS Backup","text":"<ul> <li>Global service.</li> <li>You can centralize and automate data protection across AWS services.</li> <li>When you combine AWS Organizations with AWS Backup, you can deploy data protection policies centrally.</li> <li>Automate backup scheduling.</li> <li>Automate retention management.</li> <li>Lifecycle management policies.</li> <li>Incremental backups (except DynamoDB or Aurora).</li> <li>Cross-Region backup.</li> <li>AWS Backup can seamlessly integrate with AWS Organizations so the administrative team can manage and monitor all of their backups from a single management account.</li> <li>AWS Backup provides unified IAM policy control over which accounts and roles can access backup resources. Using unified IAM policy control decreases the number of individual per-service roles and service-specific, individualized permissions to be maintained.</li> <li>AWS Backup is in scope of the many AWS compliance programs, including FedRAMP High, General Data Protection Regulation (GDPR), SOC 1, SOC 2, SOC 3, payment card industry (PCI), Health Insurance Portability and Accountability Act of 1996 (HIPAA).</li> <li>AWS Backup storage pricing is based on the amount of storage space your backup data consumes. For the first backup of an AWS resource, a full copy of your data is saved. For each incremental backup, only the changed part of your AWS resource is saved.</li> <li>AWS Backup and native snapshots are stored in AWS managed Amazon S3 buckets.</li> <li>Solve the operational issues of compliance requirements, data retention requirements and data durability requirements.</li> <li>A recovery point objective (RPO) is how far back in time you can go to recover the data. How much data loss can you tolerate?</li> <li>A recovery time objective (RTO) defines the maximum amount of time that your business can be down or offline without affecting the business.</li> <li>Backup and recovery processes should have the appropriate level of granularity to meet the RTO and RPO objectives for the workload and any supporting business processes.</li> <li>AWS Backup Audit Manager is used to audit the compliance of their AWS Backup policies against defined controls (schedules, retention policies, and the like).</li> </ul>"},{"location":"cloud/aws/cert-sap/#supported-services","title":"Supported services","text":"<ul> <li>Compute - EC2 at instance level</li> <li>Snapshot of the root EBS volume, launch configurations (instance type, security groups and VPC, monitoring configuration, tags), all associated EBS, AMI including all launch configurations and also Volume Shadow Copy Service (VSS) enabled Microsoft Windows applications.</li> <li>Stored in S3.</li> <li>Not backed up configuration of the Elastic Inteface accelerator and user data used when the instance was launched.</li> <li>Storage - EBS</li> <li>Storage - EFS</li> <li>Can use AWS Backup or EFS Replication</li> <li>You can backup all the data in the EFS and restore entire file system or restore specific indivudual files and directories.</li> <li>Storage - S3</li> <li>Can create nearly continuous point-in-time and periodic backups.</li> <li>You must activate S3 Versioning on your bucket before AWS Backup can back it up.</li> <li>Storage - FSX for Windows File Server</li> <li>Backups stored in S3</li> <li>Storage - FSX for Luste</li> <li>Data - RDS</li> <li>Backus the entire instance</li> <li>If necessary you can restore at any point in time</li> <li>Data - Aurora clusters</li> <li>Data - Neptune</li> <li>If necessary you can restore at any point in time</li> <li>Lets you specify a backup retention period, from 1 to 35 days, when you create or modify a DB cluster</li> <li>Data - DynamoDB tables</li> <li>Data - Amazon DocumentDB</li> <li>Hybrid cloud - VMWare</li> <li>Provides built-in controls for VMware backups so you can track backup and restore operations and generate auditor-ready reports.</li> <li>Provides a single-click restore experience so you can restore VMware backups on-premises and in VMware Cloud on AWS.</li> <li>Hybrid cloud - AWS Storage Gateway</li> <li>Supports backup and restore of both cached and stored volumes</li> </ul>"},{"location":"cloud/aws/cert-sap/#backup-plan","title":"Backup plan","text":"<ul> <li>Defines a set of backup rules.</li> <li>Then assign resources, select between all or specific types and resources and also can filter by tags (multiple mean and condition).</li> <li>The following quotas apply to a single resource assignment in AWS Backup: 500 ARNs without wildcards, 30 ARNs with wildcard expressions, 30 conditions and 30 tags per resource assignment.</li> </ul>"},{"location":"cloud/aws/cert-sap/#backup-rule-sections","title":"Backup rule sections","text":"<ul> <li>Vault: where backup data is stored. Can have up to 100 per region. AWS Backup Vault Lock enforces a write-once, read-many (WORM) setting for all the backups you store and create in a backup vault. A best practice is to create different vaults based on what is stored within the vault.</li> <li>Backup frecuency (supports specific time spans and cron expression)</li> <li>Backup window: the time that the backup window begins and the duration of the window in hours. The default backup window is set to start at 5 AM UTC and lasts 8 hours.</li> <li>Transition to cold storage: when available, use this feature to reduce storage costs. The recovery points that AWS Backup transitions to cold storage must remain there for at least 90 days. Therefore, your retention period setting must be at least 90 days after your transition to cold storage setting.</li> <li>Retention period: snapshots can be retained between 1 day and 100 years (or indefinitely, if you do not enter a retention period), and continuous backups between 1 and 35 days.  </li> <li>Cross-region and cross-account copy</li> <li>Tags added to recovery points</li> <li>Advanced backup settings: Backup and restore your Volume Shadow Copy Service (VSS) enabled Microsoft Windows applications, including Windows Server, Microsoft SQL Server, Exchange Server, and SharePoint running on EC2 instances.</li> </ul>"},{"location":"cloud/aws/cert-sap/#security_3","title":"Security","text":"<ul> <li>By default, AWS Backup creates an AWS KMS key with the alias aws/backup. You can choose to use this key or choose any other AWS KMS key in your account. This is powerful because the AWS KMS key can have a policy that allows AWS operators to encrypt the backup, but you can limit decryption to a completely different principal. </li> <li>The encryption key specified in the vault applies to backups of resource types (for example, supported AWS services) that support full AWS Backup management.</li> <li>If a resource type doesn't currently support full AWS Backup management, then that resource type (service) is backed up using the key that is used to encrypt the source resource.</li> <li>After you create a backup vault and set the AWS KMS encryption key, you can no longer edit the key for that backup vault.</li> <li>Your account always has a default backup vault. If you require different encryption keys or access policies for different groups of backups, you can create multiple backup vaults.</li> <li>AWS Backup Vault Lock is a feature that helps fortify compliance requirements by protecting your backups and lifecycles against intentional or accidental actions, such as deletions. AWS Backup Vault Lock uses a WORM model.</li> <li>With AWS Backup Vault Lock, no users\u2014including root, administrators, or bad actors\u2014can delete your backups or change their lifecycle settings such as retention periods and transition to cold storage.</li> <li>If you are using AWS Organizations to manage multiple AWS accounts, you can turn on organization-wide backup protection and monitoring using AWS Backup.</li> </ul>"},{"location":"cloud/aws/cert-sap/#monitoring_2","title":"Monitoring","text":"<ul> <li>You can use CloudWatch to monitor AWS Backup metrics by using the aws/backup namespace.</li> </ul> Category Metrics Example Dimensions Example Use Case Jobs Number of backup, restore, and copy jobs across each state Resource type, vault name Monitor the number of failed backup jobs within one or more specific backup vaults. When there are more than 5 failed jobs within 1 hour, send an email or SMS using Amazon SNS or open a ticket to the engineering team to investigate. Recovery points Number of warm and cold recovery points across each state Resource type, vault name Track the number of deleted recovery points for your Amazon EBS volumes and separately track the number of warm and cold recovery points in each backup vault. <ul> <li>You can configure Amazon SNS to notify you of AWS Backup events, like failed backup jobs or job completion and expiration.</li> </ul> Job Type Event Backup job BACKUP_JOB_STARTED, BACKUP_JOB_COMPLETED Copy job COPY_JOB_STARTED, COPY_JOB_SUCCESSFUL or COPY_JOB_FAILED Restore job RESTORE_JOB_STARTED, RESTORE_JOB_COMPLETED Recovery point RECOVERY_POINT_MODIFIED <ul> <li>AWS Backup sends events to EventBridge in a best-effort manner every 5 minutes.</li> <li>Although you can use the AWS Backup notification API to track AWS Backup events with Amazon SNS, EventBridge tracks more changes than the notification API, including changes to backup vaults, copy job state, Region settings, and the number of cold or warm recovery points.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-backup-audit-manager","title":"AWS Backup Audit Manager","text":"<ul> <li>AWS Backup Audit Manager provides built-in compliance controls. You can customize those controls to define your data protection policies. The service automatically detects violations of your defined data protection policies and prompts you to take corrective actions.</li> <li>You can continuously evaluate backup activity and generate audit reports to help demonstrate that you are compliant with regulatory qualifications.</li> <li>AWS Backup Audit Manager framework is a collection of controls that can be managed as a single entity. If you must comply with different internal or regulatory standards, such as NIST, HIPAA, or SOC, you can create multiple frameworks. With multiple frameworks, you can separately track the compliance of these different standards.</li> </ul>"},{"location":"cloud/aws/cert-sap/#available-controles","title":"Available controles","text":"<ul> <li>Backup resources protected by backup plan control: you can select all supported resources, or those identified by a tag, by type, or a particular resource. This control helps identify gaps in your backup coverage.</li> <li>Backup plan minimum frequency and minimum retention control: this control has parameters governing how frequently the backup plan should be taking backups and for how long recovery points should be maintained. The default settings require backups to occur every hour and recovery points should be retained for a month, but you can customize the settings to meet your business compliance requirements.</li> <li>Backup prevent recovery point manual deletion control: you can add up to five IAM roles allowed to manually delete recovery points if there are exceptions.</li> <li>Backup recovery point encrypted control: evaluates if the backup recovery points are encrypted. You can evaluate all or by specfic tags.</li> <li>Backup recovery point minimum retention control: you have the option to specify parameters ensuring that selected resources have valid recovery points in your backup vault and that the recovery points are retained for at least the specified backup recovery point minimum retention period.</li> </ul>"},{"location":"cloud/aws/cert-sap/#reporting","title":"Reporting","text":"<ul> <li>AWS Backup Audit Manager delivers a daily report in CSV, JSON, or both formats to your Amazon S3 bucket.</li> <li>You can also run an on-demand report anytime.</li> <li>You can have a maximum of 20 report plans per AWS account.</li> <li>Similar to a backup plan, you create a report plan to automate the creation of your reports and define their destination Amazon S3 bucket.</li> <li>A report plan requires that you have an S3 bucket to receive your reports.</li> <li>A report template defines the information you want included in your report.</li> <li>When you automate your reports using a report plan, AWS Backup Audit Manager provides reports for the previous 24 hours.</li> <li>AWS Backup Audit Manager creates these reports between the hours of 1:00 and 5:00 AM UTC.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cloudendure-disaster-recovery","title":"CloudEndure Disaster Recovery","text":"<ul> <li>CloudEndure Disaster Recovery uses Amazon EBS to copy the operating system, application files, and data to AWS. The on-premises block data is replicated to the EBS volumes.</li> <li>Provides a cost-effective disaster recovery option for your on-premises servers and applications, reducing your disaster recovery total cost of ownership TCO.</li> <li>You can use CloudEndure Disaster Recovery to protect your most critical databases, including Oracle, MySQL, and SQL Server, and enterprise applications such as SAP.</li> <li>Because CloudEndure Disaster Recovery replicates data at the block level, you can use it for all applications and databases that run on supported versions of Windows and Linux OS.</li> <li>Continuously replicates your machines into a low-cost staging area in your target AWS account and preferred Region.</li> <li>CloudEndure Disaster Recovery provides continuous, asynchronous, block-level replication.</li> <li>In the case of a disaster, you can instruct CloudEndure Disaster Recovery to automatically launch thousands of your machines in their fully provisioned state in minutes (Recovery Time Objectives RTO of minutes).</li> <li>After your on-premises systems are restored to an operational ready state, CloudEndure Disaster Recovery updates those systems with current information and performs a managed failback operation.</li> <li>You can conduct disaster recovery drills without disrupting your source environment or risking data loss. During drills, CloudEndure Disaster Recovery starts machines in your target AWS Region in complete isolation to avoid network conflicts and performance impact.</li> <li>CloudEndure Disaster Recovery is billed hourly per source server registered, irrespective of provisioned storage capacity. In addition you pay for the low-cost staging resources that creates during continuous replication.</li> </ul>"},{"location":"cloud/aws/cert-sap/#databases","title":"Databases","text":""},{"location":"cloud/aws/cert-sap/#server-based-vs-serverless","title":"Server based vs serverless","text":""},{"location":"cloud/aws/cert-sap/#server-based","title":"Server based","text":"<ul> <li>Amazon RDS: AWS manages the entire process of database configuration, management and maintenance.</li> <li>EC2: Full control.</li> </ul>"},{"location":"cloud/aws/cert-sap/#serverless","title":"Serverless","text":"<ul> <li>Amazon DynamoDB</li> <li>Amazon Aurora Serverless</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-rds","title":"Amazon RDS","text":"<ul> <li>Relational databases.</li> <li>Supports MySql, PostgreSQL, MariaDB, SqlServer, Oracle</li> <li>Offers On-Demand and Reserved instance types.</li> <li>Multi-AZ deployment is recommended.</li> <li>Multi-AZ provides automatic failover and protect DB performance by backing up from the standby instance.</li> <li>When you create a DB instance, a Domain Name System (DNS) name is provided. AWS uses that DNS name to fail over to the standby database.</li> <li>DB instance standard (m) provide balance.</li> <li>DB instance memory (r/x) optimized accelerate performance for workloads that process large datasets in memory.</li> <li>DB instance burstable (t) provide baseline level of CPU with the ability to burst above the baseline.</li> <li>Can enable storage autoscaling to increase the allocated storage of the EBS.</li> <li>The subnet that will be designated to your DB should not have access to internet.</li> <li>Use Amazon RDS encryption to secure your DB instances and snapshots at rest.</li> <li>Can scale components independently (memory, processor size, allocated storage, IOPS)</li> <li>Online transaction processing (OLTP) databases focus on recording Update, Insertion, and Deletion data transactions. OLTP queries are simple and short, which requires less time and space to process.</li> <li>The best practice to restrict access to your database is by placing it inside of a VPC.</li> <li>Security groups are used to control access to a database instance. Amazon RDS can use three types of security groups: database, VPC, and EC2.</li> <li>Data in transit is protected by SSL.</li> <li>Amazon RDS uses the industry-standard AES-256 bit encryption algorithm to encrypt the data while at rest.</li> </ul>"},{"location":"cloud/aws/cert-sap/#backup","title":"Backup","text":"<ul> <li>Automated backups are retained between 0 and 35 days.</li> <li>Setting automated backups to 0 days stops them and it will also delete all existing automated backups.</li> <li>Point-in-time recovery provides more granularity by restoring the full backup an rolling back transactions up to the specified time range.</li> <li>Manual snapshots can retain backups longe than 35 days. When restored, it creates a new DB instance using the data from the snapshot.</li> </ul>"},{"location":"cloud/aws/cert-sap/#disaster-recovery-architecture","title":"Disaster recovery architecture","text":"<ul> <li>RDS or a lambda generate a snapshot and store it to S3</li> <li>S3 storing generates an SNS event</li> <li>The SNS event triggers a lambda that copies the snapshot to another bucket in a different AZ.</li> </ul>"},{"location":"cloud/aws/cert-sap/#real-time-data-analytics-architecture","title":"Real-time data analytics architecture","text":"<ul> <li>The RDS can use stored procedures to integrate with lambda functions (for example trigger when a record is inserted).</li> <li>The lambda function gathers the data and passes it to Kinesis Data Firehose.</li> <li>Kinesis Data Firehose stores the data in S3.</li> <li>Athena is used to query the records in S3 in real time.</li> <li>QuickSight uses the results of Athena to build reports and dashboards (can refresh and load all new records in real time).</li> </ul>"},{"location":"cloud/aws/cert-sap/#aurora","title":"Aurora","text":"<ul> <li>Scales up to 128 TiB</li> <li>Supports PostgreSQL and MySQL</li> <li>Offers On-Demand, Reserved and serverless pricing methods.</li> <li>Maintains 6 copies of data in 3 AZ and will automatically attempt to recover the database in a healthy AZ with no data loss.</li> <li>You can create up to 15 read replicas that can serve read-only traffic as well as failover.</li> <li>You can get five times the throughput of standard MySQL and three times the throughput of standard PostgreSQL.</li> <li>Amazon Aurora Serverless is an on-demand, auto scaling configuration. It was designed to enable databases to run in the cloud without managing individual database instances.</li> <li>Three ways that you can pay for your instance. On-Demand Instance pricing lets you pay for compute by the hour. Reserved Instance pricing lets you secure a one- or three-year contract in exchange for discounts over the On-Demand rates. Serverless pricing is based on capacity, because there are no instances to manage.</li> <li>You pay for the storage and I/O, consumed by your database. Storage is billed per gigabyte per month, and the I/O is billed per million requests. There is no additional charge for the built-in backups. User-initiated backups, however, are billed per GB per month.</li> <li>There is a charge for data transferred out to the internet and other AWS Regions. You never pay for data transfers between AWS services in the same Region.</li> <li>Aurora supports two types of instances: memory-optimized and burstable performance. Memory-optimized instances are suitable for most Aurora databases. Burstable performance instances are best when your database may experience short-lived bursts of high activity.</li> <li>The Amazon Aurora Global Database is a feature available for Aurora MySQL that allows a single Aurora database to span multiple AWS Regions.</li> <li>The best practice to restrict access to your database is by placing it inside of a VPC.</li> <li>Security groups are used to control access to a database instance. Can use three types of security groups: database, VPC, and EC2.</li> <li>Data in transit is protected by SSL.</li> <li>Amazon RDS uses the industry-standard AES-256 bit encryption algorithm to encrypt the data while at rest.</li> </ul>"},{"location":"cloud/aws/cert-sap/#public-source-data-ingestion-architecture","title":"Public source data ingestion architecture","text":"<ul> <li>Data is gathered from a website and sent to Kinesis Data Firehose.</li> <li>Lambda takes the data from the data stream and transforms it before storing it in S3.</li> <li>AWS DMS takes the data from the S3 and loads it into Aurora.</li> </ul>"},{"location":"cloud/aws/cert-sap/#log-analytics-architecture","title":"Log analytics architecture","text":"<ul> <li>Aurora generate logs in CloudWatch.</li> <li>Amazon Elasticsearch gathers data from CloudWatch and catalogs it, allowing to be visualized by Quicksight.</li> <li>You can store the logs from CloudWatch to S3 and use Athena to query the data.</li> </ul>"},{"location":"cloud/aws/cert-sap/#redshift","title":"Redshift","text":"<ul> <li>Enterprise-level, petabyte scale, fully managed data warehousing service.</li> <li>Online analytical processing (OLAP) databases store historical data that has been input by OLTP. OLAP databases allow users to view different summaries of multidimensional data. Using OLAP, you can extract information from a large database and analyze it for decision-making.</li> <li>Run queries across petabytes of data in your Amazon Redshift data warehouse and exabytes of data directly from your data lake built on Amazon Simple Storage Service (Amazon S3) with Amazon Redshift Spectrum.</li> <li>Data is indexed using columnar indexing.</li> <li>With Concurrency Scaling, you can support virtually unlimited concurrent users and concurrent queries. When enabled, Amazon Redshift automatically adds additional cluster capacity when you need it to process an increase in concurrent read queries. When the demand decreases, the additional capacity is removed.</li> <li>Internally Amazon Redshift is broken down into nodes. There is a single leader node and several compute nodes. Clients access Amazon Redshift via a SQL endpoint on the leader node. The client sends a query to the endpoint. You can use any application that uses an industry standard JDBC or ODBC driver for PostgreSQL.</li> <li>The leader node creates jobs based on the query logic and sends them in parallel to the compute nodes. The compute nodes contain the actual data the queries need. The compute nodes find the required data, perform operations, and return results to the leader node. The leader node then aggregates the results from all of the compute nodes and sends a report back to the client.</li> <li>Compute nodes partition the job into slices. Each slice is allocated a portion of the node's memory and disk space. It is in these slices where the node processes its assigned portion of the job.</li> <li>You start by choosing the cluster node types that meet your needs. Each cluster node includes memory, storage, and I/O. The node type is billed per hour. There are four pricing types.</li> <li>On-Demand pricing has no upfront costs. You simply pay an hourly rate based on the type and number of nodes in your cluster.</li> <li>With Concurrency Scaling pricing, you simply pay a per-second on-demand rate for usage that exceeds the free daily credits. Each cluster earns up to one hour of free Concurrency Scaling credits per day, which is sufficient for most customers.</li> <li>Reserved Instance pricing enables you to save up to 75 percent over On-Demand rates by committing to using Amazon Redshift for a 1- or 3-year term.</li> <li>Amazon Redshift Spectrum pricing is applied when you begin using this feature. In addition to the cluster pricing, you pay for the number of bytes scanned on Amazon S3.</li> <li>There is no charge for data transferred between Amazon Redshift and Amazon S3 within the same AWS Region for backup, restore, load, and unload operations. For all other data transfers, you are billed using the standard AWS data transfer rates.</li> </ul>"},{"location":"cloud/aws/cert-sap/#dynamodb_1","title":"DynamoDB","text":"<ul> <li>NoSQL that combines the types of document store and key-value database.</li> <li>A database that can easily scale over time to meet the changing demands of online activity. One that can support ACID compliance, encryption at rest, and point-in-time recovery.</li> <li>It's a fully managed, serverless service that only requires you to provide the capacity your application requires.</li> <li>The service stores data in the form of tables, items, and attributes. When you create a table, you define a partition key attribute to uniquely identify each item in that table. This way, no two items can have the same key. You can also assign other attributes, like a sort key attribute.</li> <li>DynamoDB charges for reading, writing, and storing data, along with any optional features you choose to enable.</li> <li>In on-demand capacity mode, you are billed for each read and write that your application performs. Best for new tables with unknown workloads or unpredictable traffic.</li> <li>In provisioned capacity mode (default and eligible for free tier), you specify the number of reads and writes per second that you expect your application to require. You then use auto scaling to tell DynamoDB to automatically adjust your table's capacity within those limits. Best for predictable application traffic or consistent traffic that ramps up or down gradually. </li> <li>For high-scale applications and serverless applications.</li> <li>Work for nearly all online transaction processing (OLTP).</li> <li>Scale up/down the tables throughput without downtime or performance degradation.</li> <li>Monitor resource usage and performance metrics.</li> <li>All the data is automatically replicated across multiple AZ in a region.</li> <li>You can replicate tables across multiple AWS Regions.</li> <li>When activity occurs, is recorded in a CloudTrail event. For an ongoing record of events you can create a trail to deliver log files to S3.</li> <li>DynamoDB uses partition keys to find each item in the database. Data is distributed on physical storage nodes. DynamoDB uses the partition key to determine which of those nodes the item is located on.</li> <li>DynamoDB items can have an optional sort key to store related attributes in a sorted order. This allows multiple items to be queried as a collection, which simplifies access patterns.</li> <li>Each table has a primary key, which represents the table's key or keys. If there is no sort key, the primary and partition keys are the same. If there is a sort key, the primary key is a combination of the partition and sort keys called a composite primary key.</li> <li>You can have a table without an index.</li> <li>DynamoDB has two types of secondary indexes: local and global.</li> <li>A local secondary index uses the table\u2019s partition key with a unique sort key. You are allowed five per table. Local indexes must be created when you create the table.</li> <li>A global secondary index uses a partition key and sort key that can be different from those on the table. This allows you to model very complex data access patterns that differ from the original table. You are allowed up to 20 global indexes per table. Global indexes can be created and edited at any time.</li> <li>IAM allows you to control access at the table and item levels.</li> <li>DynamoDB provides end-to-end enterprise-grade encryption for data that is both in transit and at rest. All DynamoDB tables have encryption at rest enabled by default. This provides enhanced security by encrypting all your data using encryption keys stored in the AWS Key Management System.</li> <li>You can use Amazon DynamoDB Accelerator (DAX), which is an in-memory store for DynamoDB, without the need to modify application logic.</li> <li>DynamoDB Streams (Kinesis Data Streams) is an optional feature that captures data modification events in DynamoDB tables. Events are generated when a new item is added, and item is updated (the stream captures the before and after image of any attribute that were modified) or an item is deleted (the stream captures an image of the entire item before deletion). Each stream record also contains the name of the table, event timestamp, and other metadata. Each event has a 24h lifetime.</li> <li>One read capacity unit (RCU) represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. Transactional read requests require two RCUs to perform one read per second for items up to 4 KB. If you need to read an item that is larger than 4 KB, DynamoDB must consume additional RCUs.</li> <li>One write capacity unit (WCU) represents one write per second for an item up to 1 KB in size. If you need to write an item that is larger than 1 KB, DynamoDB must consume additional WCUs. Transactional write requests require two WCUs to perform one write per second for items up to 1 KB.</li> <li>Throttling is the action of limiting the number of requests that a client can submit to a given operation in a given amount of time.</li> <li>The size of the table is not updated in real time and can take up to 6 hours for the value to update.</li> <li>Table class is either DynamoDB Standard or DynamoDB Standard-IA.</li> <li>Using Time to Live (TTL), you can define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput.</li> <li>CloudTrail captures all API calls for DynamoDB as events (if you create a trail, they are stored in S3, if not you can view the last ones).</li> </ul>"},{"location":"cloud/aws/cert-sap/#monitoring-cloudwatch-events","title":"Monitoring Cloudwatch events","text":"Account Metric Unit Description <code>AccountMaxReads</code> Count The maximum number of read capacity units that can be used by an account <code>AccountMaxWrites</code> Count The maximum number of write capacity units that can be used by an account <code>AccountMaxTableLevelReads</code> Count The maximum number of read capacity units that can be used by a table or global secondary index <code>AccountMaxTableLevelWrites</code> Count The maximum number of write capacity units that can be used by a table or global secondary index <code>AccountProvisionedReadCapacityUtilization</code> Percent The percentage of provisioned read capacity units used by an account <code>AccountProvisionedWriteCapacityUtilization</code> Percent The percentage of provisioned write capacity units used by an account <code>MaxProvisionedTableReadCapacityUtilization</code> Percent The percentage of provisioned read capacity units used by the highest provisioned read table or GSI <code>MaxProvisionedTableWriteCapacityUtilization</code> Percent The percentage of provisioned write capacity units used by the highest provisioned write table or GSI <code>UserErrors</code> Count Requests to DynamoDB or Streams that return an HTTP 400 error during the specified time period Table Metric Unit Description <code>ConsumedReadCapacityUnits</code> Count The number of read capacity units consumed over the specified time period <code>ConsumedWriteCapacityUnits</code> Count The number of write capacity units consumed over the specified time period <code>ProvisionedReadCapacityUnits</code> Count The number of provisioned read capacity units for a table or a global secondary index <code>ProvisionedWriteCapacityUnits</code> Count The number of provisioned write capacity units for a table or a global secondary index Table Operation Metric Unit Description <code>ReturnedItemCount</code> Count The number of items returned by <code>Query</code>, <code>Scan</code>, or <code>ExecuteStatement (select)</code> operations during the time period <code>SuccessfulRequestLatency</code> Milliseconds The elapsed time for successful requests to DynamoDB or DynamoDB Streams during the specified time period <ul> <li>The DynamoDB dashboard provides a list of the custom alarms created.</li> <li>When you turn on auto scaling for a table, DynamoDB automatically creates several alarms that can launch auto scaling actions.</li> <li>If you set up CloudWatch Contributor Insights for DynamoDB on a table or global secondary index, you can determine the most accessed and throttled items in those resources. It creates by default 4 rules: most accessed items (partition key and partition and sort keys) and most throttled keys (partition key and partition and sort keys).</li> </ul>"},{"location":"cloud/aws/cert-sap/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cloud/aws/cert-sap/#troubleshooting-dynamodb-tables-that-are-throttled","title":"Troubleshooting DynamoDB Tables That Are Throttled","text":""},{"location":"cloud/aws/cert-sap/#common-throttling-issues","title":"Common throttling issues","text":"<ul> <li>Your DynamoDB table has adequate provisioned capacity, but most of the requests are being throttled.</li> <li>You activated AWS Application Auto Scaling for DynamoDB, but your DynamoDB table is being throttled.</li> <li>Your DynamoDB table is in on-demand capacity mode, but the table is being throttled.</li> <li>You have a hot partition in your table.</li> </ul>"},{"location":"cloud/aws/cert-sap/#metrics-to-examine","title":"Metrics to examine","text":"<ul> <li>OnlineIndexThrottleEvents</li> <li>ReadThrottleEvents</li> <li>ThrottledPutRecordCount</li> <li>ThrottledRequests</li> <li>WriteThrottleEvents</li> </ul>"},{"location":"cloud/aws/cert-sap/#cause-1-table-has-enough-provisioned-capacity-but-most-requests-are-throttled","title":"Cause 1: Table has enough provisioned capacity, but most requests are throttled","text":"<ul> <li>DynamoDB reports minute-level metrics to CloudWatch. The metrics are calculated as the sum for a minute, and then are averaged. However, the DynamoDB rate limits are applied per second. If all the workload falls within a couple of seconds, then the requests might be throttled.</li> <li>Solution: Add jitter and exponential backoff to your API calls.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cause-2-application-auto-scaling-is-set-up-but-your-table-is-still-throttled","title":"Cause 2: Application Auto Scaling is set up, but your table is still throttled","text":"<ul> <li>Application Auto Scaling is not a suitable solution to address sudden spikes in traffic with DynamoDB tables. It only initiates a scale-up when two consecutive data points for consumed capacity units exceed the configured target utilization value in a 1-minute span.</li> <li>Solution: If you use DynamoDB for a service that receives requests with several peak times and abrupt workload spikes, you too can benefit from switching the capacity mode from provisioned to on demand.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cause-3-your-table-uses-the-on-demand-capacity-mode-but-is-still-throttled","title":"Cause 3: Your table uses the on-demand capacity mode but is still throttled","text":"<ul> <li>When the table uses the on-demand capacity mode, the table doesn't throttle as long as the following conditions are true: the access pattern is distributed evenly across partitions to avoid issues related to a hot partition and the table doesn't exceed double its previous peak traffic within a span of 30 minutes.</li> <li>For new on-demand tables, you can immediately drive up to 4,000 write request units or 12,000 read request units, or any linear combination of the two. For an existing table that you switched to on-demand capacity mode, the previous peak is half the previous provisioned throughput for the table\u2014or the settings for a newly created table with on-demand capacity mode, whichever is higher.</li> <li>Solution: Apply a strategy to avoid creating hot partitions such as distributing the read and write operations as evenly as possible across your table.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cause-4-you-have-a-hot-partition-in-your-table","title":"Cause 4: You have a hot partition in your table","text":"<ul> <li>A partition key that doesn't have a high cardinality can result in many requests targeting only a few partitions and resulting in a hot partition. A hot partition can cause throttling if the partition limits of 3000 read capacity units (RCU) or 1000 write capacity units (WCU) (or a combination of both) per second are exceeded.</li> <li>Solution: To avoid this poor performance, distribute the read and write operations as evenly as possible across your table.</li> </ul>"},{"location":"cloud/aws/cert-sap/#troubleshooting-dynamodb-latency","title":"Troubleshooting DynamoDB Latency","text":""},{"location":"cloud/aws/cert-sap/#metrics-to-examine_1","title":"Metrics to examine","text":"<ul> <li>SuccessfulRequestLatency</li> </ul>"},{"location":"cloud/aws/cert-sap/#solutions","title":"Solutions","text":"<ul> <li>Reduce the request timeout settings to fail much faster using the SDK. This causes the client to abandon high-latency requests after the specified time period and then send a second request that usually completes much faster than the first.</li> <li>Reduce distance between client and DynamoDB endpoint.</li> <li>Use caching: If your traffic is read heavy, consider using a caching service such as Amazon DynamoDB Accelerator (DAX). DAX is a fully managed, highly available in-memory cache for DynamoDB that can help improve performance from milliseconds to microseconds, even at millions of requests per second.</li> <li>Send constant traffic or reuse connections: When you are not making requests, consider having the client send dummy traffic to a DynamoDB table. You can also reuse client connections or try connection pooling. All of these techniques keep internal caches warm, which can help keep latency low.</li> <li>Use eventually consistent reads: If your application doesn't require strongly consistent reads, consider using eventually consistent reads. Eventually consistent reads are cheaper and are less likely to experience high latency.</li> </ul>"},{"location":"cloud/aws/cert-sap/#troubleshooting-issues-with-dynamodb-auto-scaling-not-working-properly","title":"Troubleshooting Issues with DynamoDB Auto Scaling not working properly","text":"<ul> <li>Don't delete CloudWatch alarms</li> <li>DynamoDB might not handle short activity spikes</li> <li>Set the billing mode to PAY_PER_REQUEST if the traffic is frequently unpredictable</li> </ul>"},{"location":"cloud/aws/cert-sap/#mobile-application-backend-architecture","title":"Mobile application backend architecture","text":"<ul> <li>Mobile app send requests to api gateway that is linked to a lambda function.</li> <li>Lambda uses DynamoDB to search friend list of users and send notifications via SNS.</li> </ul>"},{"location":"cloud/aws/cert-sap/#iot-sensor-data-capture-architecture","title":"IoT sensor data capture architecture","text":"<ul> <li>Lambda function reads events from SQS queue and stores them to DynamoDB.</li> <li>Amazon EMR gathers data from DynamoDB and stores it in S3.</li> <li>Amazon Athena is used to query the data in S3.</li> </ul>"},{"location":"cloud/aws/cert-sap/#elasticache","title":"ElastiCache","text":"<ul> <li>Caching solution.</li> <li>Supports Valkey, Redis and Memcached.</li> <li>Two common approaches to caching are lazy loading and write-through. Lazy loading is reactive. Data is put into the cache the first time it is requested. Write-through is proactive. Data is put into the cache at the same time it is put into the database.</li> <li>Redis supports complex data types, data replication, and high availability. This makes it ideal for session caching, full-page caching, message queue applications, leaderboards, and much more.</li> <li>Memcached is for data that is relatively small and static. For example, a static HTML page or JavaScript and CSS files.</li> <li>ElastiCache is a popular choice for gaming, advertising technology (ad tech), financial service, healthcare, and Internet of Things (IoT) apps.</li> <li>On-Demand Nodes let you pay for memory capacity by the hour with no long-term commitments. Pricing is per node-hour consumed, from the time you launch a node until you terminate it.</li> <li>Reserved Nodes enable you to save up to 75 percent over On-Demand rates by committing to using ElastiCache for a 1- or 3-year term.</li> <li>ElastiCache provides storage for one database snapshot at no charge. Each additional snapshot is charged per gigabyte per month.</li> <li>You are only charged for the data transfer in or out of the Amazon EC2 instance. There is no ElastiCache data transfer charge for traffic in or out of the ElastiCache node itself.</li> <li>On-premises servers can use ElastiCache provided that there is connectivity between your VPC and data center through either a VPN or AWS Direct Connect.</li> <li>ElastiCache for Redis supports encryption at rest and in transit. Using the Redis AUTH feature, ElastiCache can also authenticate clients. This version of ElastiCache has versions that are compliant with HIPAA, FedRAMP, and PCI DSS.</li> <li>Amazon ElastiCache offers a fast, in-memory data store to power live streaming use cases or distributed cached sessions.</li> </ul>"},{"location":"cloud/aws/cert-sap/#elasticsearch-service-eds","title":"Elasticsearch Service (EDS)","text":"<ul> <li>Elasticsearch like service offering full-text search.</li> </ul>"},{"location":"cloud/aws/cert-sap/#memorydb","title":"MemoryDB","text":"<ul> <li>Redis-compatible, durable, in-memory database.</li> <li>Microseconds read latency.</li> <li>Single-digit milliseconds write latency.</li> </ul>"},{"location":"cloud/aws/cert-sap/#documentdb","title":"DocumentDB","text":"<ul> <li>Document database.</li> <li>Fully-managed database service that provides high throughput and low latency for document queries.</li> <li>Compatible with MongoDB.</li> <li>On-Demand Instances let you pay by the second. Pricing is per instance-hour or per partial instance-hour consumed from the time you launch an instance until you delete it. Storage auto scales from 10GB up to 64TB with no interaction necessary. You only pay for what you consume, and your cluster is billed in per gigabyte per month increments.</li> <li>Automatic, continuous, and incremental backups and point-in-time restore. There is no additional charge for backup storage of up to 100% of your total Amazon DocumentDB cluster storage for a Region. Additional backup storage is billed in per GB-months.</li> <li>Also billed for IOPS (Input/Output Operations Per Second) are pay as you go, and IOPS consumed are billed in per million request increments. There is no charge for data transferred into your Amazon DocumentDB database. Data transferred out of the database is charged per gigabyte per month.</li> <li>The cluster's data is stored in the cluster volume, which stores six copies of your data across three different Availability Zones.</li> <li>For use cases like content management systems, profile management, web and mobile applications.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-keyspaces-for-apache-cassandra","title":"Amazon Keyspaces (for Apache Cassandra)","text":"<ul> <li>Compatible with Cassandra Query Language (CQL).</li> </ul>"},{"location":"cloud/aws/cert-sap/#neptune","title":"Neptune","text":"<ul> <li>Graph database.</li> <li>Often used for recommendation engines, social network, fraud detection and knowledge graphs.</li> <li>Supports Gremlin (Apache TinkerPop), openCypher and RDF/SPARQL.</li> <li>Neptune uses database instances. Of these, the primary database instance supports read and write operations and performs all the data modifications to the cluster volume. There can only be one primary database instance.</li> <li>Neptune also uses replicas. A Neptune replica connects to the same storage volume as the primary database instance and only supports read-only operations. There can be up to 15 of these replicas.</li> <li>Neptune uses a cluster volume. The cluster volume is where Neptune data is stored. It is designed for reliability and high availability. The cluster volume consists of copies of the data across multiple Availability Zones in a single Region.</li> <li>A cluster endpoint connects to the current primary database instance for the database cluster. There is only one cluster endpoint.</li> <li>A reader endpoint connects to one of the available Neptune replicas. Each replica has its own endpoint.</li> <li>An instance endpoint connects to a specific database instance. Each database instance in a database cluster has its own unique instance endpoint. The instance endpoint provides direct control over connections to the DB cluster.</li> <li>Your data at rest in the database is encrypted using the industry standard AES-256 bit encryption algorithm on the server that hosts your Neptune instance. Keys can also be used, which are managed through AWS Key Management Service (AWS KMS).</li> <li>Can be configured to use IAM DB authentication (manage database user credentials through IAM users and roles).</li> <li>You can load data in it from S3 (calling the loader in the api).</li> <li>You pay for the instance hosting the databases, known as an On-Demand Instance. You pay for your database by the hour with no long-term commitments or upfront fees.</li> <li>You pay for the storage consumed by your database. This is billed per gigabyte per month, and the first 50 gigabytes of backup storage is offered at no cost.</li> <li>You pay for the number of requests to the database.</li> <li>You pay for the amount of data transferred out of the database. You never pay for moving data into your database.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-timestream","title":"Amazon Timestream","text":"<ul> <li>Time series database for IoT.</li> <li>Used for measuring events that change over time, such as stock prices over time or temperature measurements.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-quantum-ledger-database-qldb","title":"Amazon Quantum Ledger Database (QLDB)","text":"<ul> <li>Immutable ledger.</li> <li>Provides a complete and cryptographically verifiable history of all changes.</li> <li>The journal provides immutability and verifiability by storing a log of each transaction in a sequenced way, using cryptographic techniques. Once the data is written to the journal, it is materialized into your tables, allowing for faster and more efficient queries. Uses a cryptographic hash function--in this case SHA-256.</li> <li>Serverless, you don\u2019t have to worry about provisioning capacity or configuring read and write limits.</li> <li>Amazon QLDB supports PartiQL, a new open standard query language. With PartiQL, you can easily query, manage, and access the entire data and change history using familiar SQL operators.</li> <li>Use cases: finance and insurance.</li> <li>You are billed for read/write input and output (IO) requests, data transfer, journal storage, and indexed storage.</li> <li>Storage consumed by your Amazon QLDB ledger is billed per GB per month, and IOs consumed are billed per million requests.</li> <li>Amazon QLDB's ledger is deployed across multiple Availability Zones with multiple copies per Availability Zone. AWS maintains redundancy within the Region and ensures full recoverability from Availability Zone failures. A write is acknowledged only after being written to multiple Availability Zones, and hence, Amazon QLDB is strongly durable.</li> <li>A ledger consists of a set of tables and a journal that maintains the complete, immutable history of changes to the tables.</li> <li>Tables exist within a ledger and contain a collection of document revisions. This can include a record of the deletion of a document.</li> <li>Documents exist within tables and must be in Amazon Ion form. Ion is a superset of JSON that adds additional data types, type annotations, and comments. Amazon QLDB supports documents that contain nested JSON elements and gives you the ability to write queries that reference and include these elements.</li> <li>The journal consists of a sequence of blocks, each cryptographically chained to the previous block so that changes can be verified. Blocks, in turn, contain the actual changes that were made to the tables, indexed for efficient retrieval. The journal represents a complete and immutable history of all the changes to your ledger data.</li> <li>The user view shows the latest revision, or version, of your data.</li> <li>The committed view includes everything from the user view plus access to system-generated metadata (blockAddress, hash, metadata (document id, document version, transaction time, transaction id)).</li> <li>The history view is a little different from the others. It's generated by running a history function in the query. The view returned is a combination of all recorded revisions of the document based on the committed view. This means you'll have access to the history as well as the document\u2019s metadata.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-dms-database-migration-service","title":"AWS DMS (Database Migration Service)","text":"<ul> <li>Helps you migrate databases to AWS efficiently and securely.</li> <li>The source database can remain fully operational during the migration, minimizing downtime to applications.</li> <li>At its most basic level, AWS DMS is an instance in the AWS Cloud that runs replication software.</li> <li>You detect the source and target connections, and create the EC2 instance, then execute the job.</li> <li>You can migrate the schema or only the data.</li> <li>Supports homogeneous migrations such as Oracle to Oracle as well as heterogeneous migrations between different database engines, such as Oracle to MySQL.</li> <li>Where you migrate between different database engines, require the use of the AWS Schema Conversion Tool (AWS SCT) to first translate your database schema to the new platform. You can then use AWS DMS to migrate the data. It is important to understand that AWS DMS and SCT are two different tools that serve different needs.</li> <li>DMS: Loads the tables with data without any foreign keys or constraints.</li> <li>SCT: Schema conversion, generate target schema including foreign keys and constraints, and converts code like procedures and views.</li> </ul>"},{"location":"cloud/aws/cert-sap/#migration-steps","title":"Migration steps","text":""},{"location":"cloud/aws/cert-sap/#1-envisioning-and-assessment","title":"1 - Envisioning and assessment","text":"<ul> <li>Evaluate the current situation and decide the target database engine.</li> <li>Use SCT to generate a report of the migration of the tables and code objects.</li> <li>Estimate effort based on the report. Simple actions usually require less than an hour, medium actions can be completed in 1-4 hours, and complex actions would typically take over 4 hours.</li> </ul>"},{"location":"cloud/aws/cert-sap/#2-database-schema-conversion","title":"2 - Database schema conversion","text":"<ul> <li>Use SCT to migrate the schema and objects and perform the required manual changes detected in the report.</li> <li>The Database Migration Playbooks are a series of guides focused on best practices for creating successful blueprints for heterogeneous database migration.</li> <li>The AWS SCT Extension Pack is an add-on module that emulates functions present in the source database that are required when converting objects to the target database. Before you can install the AWS SCT Extension Pack, you need to convert your database schema.</li> </ul>"},{"location":"cloud/aws/cert-sap/#3-application-conversion-and-remediation","title":"3 - Application conversion and remediation","text":"<ul> <li>You can use AWS SCT to extract SQL statements that are embedded in your application code.</li> </ul>"},{"location":"cloud/aws/cert-sap/#4-scripts-conversion","title":"4 - Scripts conversion","text":"<ul> <li>Use AWS SCT to convert Oracle, Microsoft, and Teradata scripts to run on PostgreSQL-derived databases, including Amazon Aurora with PostgreSQL compatibility and Amazon Redshift. As with other conversion features in AWS SCT, if the code cannot be converted for any reason, the tool will highlight the problem for manual intervention.</li> </ul>"},{"location":"cloud/aws/cert-sap/#5-integration-with-third-party-applications","title":"5 - Integration with third-party applications","text":"<ul> <li>This process may involve upgrading the third-party tools, or changing adapters or APIs to connect to your new databases.</li> <li>Other third-party applications might be tightly coupled to a third-party database. In that case, consider whether you will maintain a legacy database for these applications, or whether you want to migrate from them.</li> </ul>"},{"location":"cloud/aws/cert-sap/#6-data-migration","title":"6 - Data migration","text":"<ul> <li>AWS DMS is a web service that helps you migrate data from a source data store to a target data store, as long as either the source or target databases reside within AWS.</li> <li>You can migrate between source and target endpoints that use the same database engine.</li> <li>You can also migrate between source and target endpoints that use different database engines. The only requirement to use AWS DMS is that one endpoint is on an AWS service. You cannot use AWS DMS to migrate from an on-premises database to another on-premises database.</li> <li>You can use AWS DMS to perform a one-time copy of the source data to the target.</li> <li>You can also use AWS DMS to keep your source and target synced by migrating ongoing transactions as they occur on the source.</li> <li>If you are migrating to a new target database, you can continue replicating until you are ready to switch over your applications. If your use case requires ongoing data replication, then you can use AWS DMS to keep a source and target in sync indefinitely.</li> <li>You can select which schemas or tables to include in the migration, filter out unwanted data records, and transform names to conform to your particular naming conventions. As your migration progresses, you can monitor the migration process and check the health of your migration resources via the AWS DMS console.</li> <li>A migration task runs on a replication instance (EC2) that has been configured with the AWS DMS software. The task migrates the data between your source and target endpoints. You can see the progress of the task in the console, and review detailed task logs if needed.</li> <li>For relational migrations, AWS DMS can validate the migrated data, so you can be confident that the source and target databases match. Data validation is an option that you can choose to add to your replication task. Data validation tracks the progress of the migration and incrementally validates new data as it is written to the target.</li> </ul>"},{"location":"cloud/aws/cert-sap/#7-functional-testing-of-the-entire-system","title":"7 - Functional testing of the entire system","text":"<ul> <li>Ensure that all applications interacting with the database perform as before, from a functional perspective.</li> </ul>"},{"location":"cloud/aws/cert-sap/#8-performance-testing","title":"8 - Performance testing","text":"<ul> <li>Tune performance when an issue is discovered or to meet required criterias.</li> </ul>"},{"location":"cloud/aws/cert-sap/#9-integration-and-deployment","title":"9 - Integration and deployment","text":"<ul> <li>Integration and deployment is the process of cutting over to your new database system.</li> </ul>"},{"location":"cloud/aws/cert-sap/#10-training-and-knlowedge-transfer","title":"10 - Training and knlowedge transfer","text":"<ul> <li>Until all of the team understands the new technologies that you are moving to, consider adding training time on the database engine, AWS, and AWS RDS (depending on what you use).</li> </ul>"},{"location":"cloud/aws/cert-sap/#11-documentation-and-version-control","title":"11 - Documentation and version control","text":"<ul> <li>Document all changes that have been made to the system, and how the new system operates.</li> <li>Use infrastructure as code.</li> <li>You also keep and manage your database schemas as source code.</li> </ul>"},{"location":"cloud/aws/cert-sap/#12-post-production-support","title":"12 - Post production support","text":"<ul> <li>Plan for support your application might need.</li> <li>Ensure that automated tasks are occasionally checked, and that you have personnel for tasks that are not automated.</li> </ul>"},{"location":"cloud/aws/cert-sap/#monitoring_3","title":"Monitoring","text":""},{"location":"cloud/aws/cert-sap/#cloudwatch_1","title":"CloudWatch","text":"<ul> <li>Different resources create different types of metrics.</li> <li>Many AWS services automatically send metrics to CloudWatch for free at a rate of 1 data point per metric per 5-minute interval. This is called basic monitoring, and it gives you visibility into your systems without any extra cost. For many applications, basic monitoring is adequate.</li> <li>For applications running on EC2 instances, you can get more granularity by posting metrics every minute instead of every 5-minutes using a feature like detailed monitoring. Detailed monitoring incurs a fee.</li> <li>You can use external or custom tools to ingest and analyze CloudWatch metrics using the GetMetricData API.</li> <li>Dashboard is customizable to show desired metrics.</li> <li>You can use custom metrics (you can send 1 data point per second per custom metric).</li> <li>Alarms allow to create alarms for thresholds is a certain period of time.</li> <li>Alar can be in state OK, ALARM or INSUFFICIENT_DATA.</li> <li>Alarms can trigger SNS notification and/or send an email. With the SNS notification you can invoke a lambda to call AWS API to perform actions like scale, reboot, etc.</li> <li>CloudWatch Logs is centralized place for logs to be stored and analyzed.</li> <li>A log group is composed of log streams that all share the same retention and permissions settings.</li> <li>Log events are grouped into log streams, which are sequences of log events that all belong to the same resource being monitored.</li> <li>A log event is a record of activity recorded by the application or resource being monitored.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cloudtrail_1","title":"CloudTrail","text":"<ul> <li>IAM and AWS STS are integrated with AWS CloudTrail, a service that provides a record of actions taken by an IAM user or role.</li> <li>If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket.</li> <li>AWS services like Amazon Athena can then be used to query the logs directly from the S3 bucket.</li> <li>If you don't configure a trail, you can still view the most recent events in the CloudTrail console in Event history.</li> </ul>"},{"location":"cloud/aws/cert-sap/#principal-fields","title":"Principal fields","text":"<ul> <li>eventTime: UTC</li> <li>eventType: AwsConsoleSigning, AwsServiceEvent, AwsApiCall</li> <li>eventSource: the service that the request was made of</li> <li>eventName: the requested action</li> <li>sourceIpAddress: IP address where the request came from, if one AWS service calls another, the DNS name of the calling service is used</li> <li>userAgent: tool or application that made the request</li> <li>errorMessage: any error message returned by the requested service</li> <li>requestParameters</li> <li>resources: arn, account number or resource type</li> <li>userIdentity: collection of fields that describe the user or service that made the call</li> </ul>"},{"location":"cloud/aws/cert-sap/#other","title":"Other","text":""},{"location":"cloud/aws/cert-sap/#cloudformation","title":"CloudFormation","text":"<ul> <li>You can use CloudFormation to treat your infrastructure as code.</li> <li>You define your AWS resources in a structured text format, either YAML or JSON, called a CloudFormation template.</li> <li>Then you can create a CloudFormation stack in AWS, which contains the resources created. You can then manage these resources by updating the template.</li> <li>One important issue is how to create the CloudFormation stacks. You can create them manually on the console, but a better approach is to create an integration pipeline. You can create this so that merging changes to your templates into the main branch creates or modifies the stacks, then use your standard code review practices to manage the templates.</li> <li>Free service.</li> <li>AWS CloudFormation StackSets can deploy an IAM role across multiple accounts with a single operation.</li> </ul>"},{"location":"cloud/aws/cert-sap/#basic-concepts","title":"Basic concepts","text":"<ul> <li>Resources: any of the things you can create within AWS, which includes things like S3 buckets, EC2 instances, or SQS queues. Templates: text-based (JSON or YAML) descriptions of CloudFormation stacks that you can use to define all of your resources, including which resources depend on each other.  Stack: a collection of AWS resources that you can manage as a single unit.  StackSet: a named set of stacks that use the same template, but applied across different accounts and Regions. You can create, update, or delete stacks across multiple accounts and Regions with a single operation.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-q-developer","title":"Amazon Q Developer","text":"<ul> <li>AI coding companion.</li> <li>Generates code suggestions based on comments and existing code.</li> <li>Offers real-support for code vulnerabilities.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-wa-tool","title":"AWS WA Tool","text":"<ul> <li>To use the AWS WA Tool, define your workload, apply one of the AWS Well-Architected lenses or your own custom lens, and begin your review. The tool generates an improvement plan and provides a mechanism to track and measure your progress.</li> </ul>"},{"location":"cloud/aws/cert-sap/#exam-preparation-notes","title":"Exam preparation notes","text":""},{"location":"cloud/aws/cert-sap/#analytics_1","title":"Analytics","text":""},{"location":"cloud/aws/cert-sap/#amazon-athena","title":"Amazon Athena","text":"<ul> <li>Regional service.</li> <li>Amazon Athena is a interactive query serverless service that makes it easy for you to analyze data directly in Amazon S3, using standard SQL. Can process unstructured, semi-structured, and structured data sets. It integrates with Amazon QuickSight for easy visualization. It can also be used with third-party reporting and business intelligence tools by connecting these tools to Athena with a JDBC driver. Typically used for ad hoc data discovery and SQL querying.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-data-exchange","title":"AWS Data Exchange","text":"<ul> <li>Global service.</li> <li>AWS Data Exchange is a service that enables customers to find, subscribe to, and use third-party data sets in the AWS Cloud.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-data-firehose","title":"Amazon Data Firehose","text":"<ul> <li>Regional service.</li> <li>A fully managed service for delivering real-time streaming data to destinations such as Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk. It can automatically scale to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</li> <li>Supports data transformation through AWS Lambda.</li> <li>Useful to buffer data.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-emr","title":"Amazon EMR","text":"<ul> <li>Regional service. Support single-AZ or multi-AZ.</li> <li>S3DistCp tool is used to copy large amounts of data from Amazon S3 into HDFS.</li> <li>Master node: A node that manages the cluster by running software components to coordinate the distribution of data and tasks among other nodes for processing. The master node tracks the status of tasks and monitors the health of the cluster. Every cluster has a master node, and it's possible to create a single-node cluster with only the master node.</li> <li>Core node: A node with software components that run tasks and store data in the Hadoop Distributed File System (HDFS) on your cluster. Multi-node clusters have at least one core node.</li> <li>Task node: A node with software components that only runs tasks and does not store data in HDFS. Task nodes are optional.</li> <li>To optimize cost and performance:</li> <li>Master node: Unless your cluster is very short-lived and the runs are cost-driven, avoid running your Master node on a Spot Instance. A Spot interruption on the Master node terminates the entire cluster. Alternatively to On-Demand, you can set up the Master node on a Spot Block. Setting the defined duration of the node and failing over to On-Demand if the Spot Block capacity is unavailable. </li> <li>Core nodes: Avoid using Spot Instances for Core nodes if the jobs on the cluster use HDFS. That prevents a situation where Spot interruptions cause data loss for data that was written to the HDFS volumes on the instances.</li> <li>Task nodes: Use Spot Instances for your task nodes by selecting up to five instance types that match your hardware requirement. Amazon EMR fulfills the most suitable capacity by price and capacity availability.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-glue","title":"AWS Glue","text":"<ul> <li>Regional service.</li> <li>AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams. AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python or Scala code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so there\u2019s no infrastructure to set up or manage.</li> <li>You can use a crawler to populate the AWS Glue Data Catalog with tables. This is the primary method used by most AWS Glue users. A crawler can crawl multiple data stores in a single run. Upon completion, the crawler creates or updates one or more tables in your Data Catalog. A classifier reads the data in a data store. If it recognizes the format of the data, it generates a schema. The classifier also returns a \"certainty\" number to indicate how certain the format recognition was.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-kinesis-data-streams","title":"Amazon Kinesis Data Streams","text":"<ul> <li>Regional service.</li> <li>In an IoT scenario you can group the requests from API Gateway by streaming the data into an Amazon Kinesis data stream and processing the data in batches using a lambda function.</li> <li>The amount of data per shard is 1 MB/s.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-lake-formation","title":"AWS Lake Formation","text":"<ul> <li>Regional service.</li> <li>AWS Lake Formation is a fully managed service that simplifies the creation, security, and management of data lakes.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-managed-service-for-apache-flink","title":"Amazon Managed Service for Apache Flink","text":"<ul> <li>Regional service.</li> <li>Amazon Managed Service for Apache Flink (M) is a fully managed service that simplifies building and running Apache Flink applications for real-time data processing.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-managed-streaming-for-apache-kafka-amazon-msk","title":"Amazon Managed Streaming for Apache Kafka (Amazon MSK)","text":"<ul> <li>Regional service.</li> <li>Amazon Managed Streaming for Apache Kafka (MSK) is a fully managed service by Amazon Web Services (AWS) that simplifies the setup, operation, and management of Apache Kafka clusters.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-opensearch-service","title":"Amazon OpenSearch Service","text":"<ul> <li>Regional service.</li> <li>Amazon OpenSearch Service is a managed service that makes it easy to deploy, operate, and scale OpenSearch clusters in the AWS Cloud. Amazon OpenSearch Service supports OpenSearch and legacy Elasticsearch OSS (up to 7.10, the final open-source version of the software). When you create a cluster, you have the option of which search engine to use.</li> <li>OpenSearch Dashboards is an open-source visualization tool designed to work with OpenSearch. Amazon OpenSearch Service provides an installation of OpenSearch Dashboards with every OpenSearch Service domain. You can find a link to Dashboards on your domain dashboard on the OpenSearch Service console.</li> <li>OpenSearch Service offers three storage tiers:</li> <li>Hot - provides the fastest possible performance for indexing and searching new data. Standard data nodes use hot storage, which takes the form of instance store or Amazon EBS volumes attached to each node.</li> <li>UltraWarm - uses Amazon S3 with related caching solutions to improve performance. UltraWarm offers significantly lower costs per GiB for read-only data that you query less frequently and don\u2019t need the same performance as hot storage. Data stored in UltraWarm is immutable.</li> <li>Cold - optimized to store infrequently accessed or historical data at a cheaper price per GB per month. When you use cold storage, you detach your indexes from the UltraWarm tier, making them inaccessible. You can reattach these indexes when you need to query that data.</li> <li>Use Index State Management (ISM) policies to transition data to cold storage.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-quicksight","title":"Amazon QuickSight","text":"<ul> <li>Regional service in standard and global service in enterprise.</li> <li>Amazon QuickSight is a fast, cloud-powered business intelligence (BI) service that simplifies data analysis and visualization. It allows users to create interactive dashboards, conduct ad-hoc analysis, and derive insights from various data sources.</li> </ul>"},{"location":"cloud/aws/cert-sap/#application-integration","title":"Application Integration","text":""},{"location":"cloud/aws/cert-sap/#amazon-appflow","title":"Amazon AppFlow","text":"<ul> <li>Regional service.</li> <li>Amazon AppFlow is used to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, SAP, Zendesk, Slack, and ServiceNow, and AWS services.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-appsync","title":"AWS AppSync","text":"<ul> <li>Regional service.</li> <li>AWS AppSync is a fully managed service that makes it easy to develop GraphQL APIs by handling the heavy lifting of securely connecting to data sources like Amazon DynamoDB, Lambda, and more. Adding caches to improve performance, subscriptions to support real-time updates, and client-side data stores that keep offline clients in sync are just as easy. Once deployed, AWS AppSync automatically scales your GraphQL API execution engine up and down to meet API request volumes.</li> <li>With managed GraphQL subscriptions, AWS AppSync can push real-time data updates over Websockets to millions of clients. For mobile and web applications, AppSync also provides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online.</li> <li>AppSync supports real-time chat applications. You can build conversational mobile or web applications that support multiple private chat rooms, offer access to conversation history, and queue outbound messages, even when a device is offline.</li> <li>AppSync can also be used for real-time collaboration. You can broadcast data from the backend to all connected clients (one-to-many) or between clients (many-to-many), such as in a second screen scenario where you broadcast the same data to all clients, who can then reply.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-eventbridge_1","title":"Amazon EventBridge","text":"<ul> <li>Regional service.</li> <li>Amazon EventBridge is a serverless event bus service that simplifies building event-driven applications and integrating services.</li> <li>You can configure rules to react to events.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-mq","title":"Amazon MQ","text":"<ul> <li>Regional service. Can be single-AZ or multi-AZ.</li> <li>Amazon MQ is a managed message broker service that provides compatibility with many popular message brokers. AWS recommends Amazon MQ for migrating applications from existing message brokers that rely on compatibility with APIs such as JMS or protocols such as AMQP, MQTT, OpenWire, and STOMP.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-simple-notification-service-amazon-sns","title":"Amazon Simple Notification Service (Amazon SNS)","text":"<ul> <li>Regional service.</li> <li>With Amazon SNS Mobile Push Notifications, you have the ability to send push notification messages directly to apps on mobile devices. Push notification messages sent to a mobile endpoint can appear in the mobile app as message alerts, badge updates, or even sound alerts.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-simple-queue-service-amazon-sqs","title":"Amazon Simple Queue Service (Amazon SQS)","text":"<ul> <li>Regional service.</li> <li>Amazon SQS supports dead-letter queues (DLQ), which other queues (source queues) can target for messages that can't be processed (consumed) successfully.</li> <li>Occasionally, producers and consumers might fail to interpret aspects of the protocol that they use to communicate, causing message corruption or loss. Also, the consumer's hardware errors might corrupt message payload. If a message can't be consumed successfully, you can send it to a dead-letter queue (DLQ). Dead-letter queues let you isolate problematic messages to determine why they are failing.</li> <li>The Maximum receives value determines when a message will be sent to the DLQ. If the ReceiveCount for a message exceeds the maximum receive count for the queue, Amazon SQS moves the message to the associated DLQ (with its original message ID).</li> <li>The <code>maxReceiveCount</code> is the number of times a consumer tries receiving a message from a queue without deleting it before being moved to the dead-letter queue. Setting the maxReceiveCount to a low value, such as 1, would result in any failure to receive a message to cause the message to be moved to the dead-letter queue. Such failures include network errors and client dependency errors.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-step-functions_1","title":"AWS Step Functions","text":"<ul> <li>Regional service.</li> <li>You can use to coordinate the components of distributed applications and microservices using visual workflows.</li> <li>Standard workflows for long-running, durable and auditable workflows.</li> <li>Express workflows for high-volume, event-processing workloads such as IoT data ingestion, streaming data processing and transformation, and mobile application backends. </li> </ul>"},{"location":"cloud/aws/cert-sap/#blockchain","title":"Blockchain","text":""},{"location":"cloud/aws/cert-sap/#amazon-managed-blockchain","title":"Amazon Managed Blockchain","text":"<ul> <li>Amazon Managed Blockchain is a fully managed service that helps build and manage Web3 applications with blockchain frameworks. With Amazon Managed Blockchain, you can securely transact and share data on a distributed and immutable ledger.</li> <li>For private data, you store it in other store and publish the hah in the blockchain.</li> </ul>"},{"location":"cloud/aws/cert-sap/#business-applications","title":"Business Applications","text":""},{"location":"cloud/aws/cert-sap/#amazon-simple-email-service-amazon-ses","title":"Amazon Simple Email Service (Amazon SES)","text":"<ul> <li>Amazon Simple Email Service (Amazon SES) is a scalable and secure cloud-based email-sending service designed to help businesses send marketing, transactional, and other types of professional emails.</li> <li>The Amazon SES SMTP endpoint requires that all connections be encrypted using Transport Layer Security (TLS). Amazon SES supports two mechanisms for establishing a TLS-encrypted connection: STARTTLS and TLS Wrapper.</li> <li>STARTTLS is a means of upgrading an unencrypted connection to an encrypted connection. To set up a STARTTLS connection, the SMTP client connects to the Amazon SES SMTP endpoint on port 25, 587, or 2587, issues an EHLO command, and waits for the server to announce that it supports the STARTTLS SMTP extension. The client then issues the STARTTLS command, initiating TLS negotiation. When negotiation is complete, the client issues an EHLO command over the new encrypted connection, and the SMTP session proceeds normally.</li> <li>Amazon SES supports sending emails through two main interfaces: the Simple Mail Transfer Protocol (SMTP) and the SES API.</li> </ul>"},{"location":"cloud/aws/cert-sap/#cloud-financial-management","title":"Cloud Financial Management","text":""},{"location":"cloud/aws/cert-sap/#aws-budgets","title":"AWS Budgets","text":"<ul> <li>AWS Budgets allows you to set custom budgets to track your AWS spending, and it can alert you via email or SNS notifications if your costs exceed or are projected to exceed your budget. AWS Cost Explorer is a tool that provides detailed information about your AWS costs and usage, enabling you to visualize and understand your spending patterns.</li> <li>Using AWS Budgets in the management account (when using AWS Organziations) offers a centralized way to manage and monitor spending across multiple departmental accounts. You can create budget alerts specifically for each member account using budget filters.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-cost-and-usage-report","title":"AWS Cost and Usage Report","text":"<ul> <li>You can use tags to organize your resources and cost allocation tags, you need to activate them in the Billing and Cost Management console.</li> <li>AWS generates a cost allocation report as a comma-separated value (CSV file) with your usage and costs grouped by your active tags.</li> <li>If you use AWS Organizations, the management account, with its organization-wide visibility, is best suited. By enabling the cost allocation tag at this level, all analytics-related resources across accounts can be tracked.</li> <li>Only a management account in an organization and single accounts that aren't members of an organization have access to the cost allocation tags manager in the Billing and Cost Management console.</li> <li>The AWS Cost and Usage Reports (AWS CUR) contains the most comprehensive set of cost and usage data available. You can use Cost and Usage Reports to publish your AWS billing reports to an Amazon Simple Storage Service (Amazon S3) bucket that you own.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-cost-explorer","title":"AWS Cost Explorer","text":"<ul> <li>UI-based spending analysis and visualziation for quick visual reviews.</li> </ul>"},{"location":"cloud/aws/cert-sap/#savings-plans","title":"Savings Plans","text":"<ul> <li>Compute saving plans: up to 66% applied to any instance family and can cover usage across different services like EC2, Fargate, and Lambda. Terms of 1 or 3 years.</li> <li>EC2 Instance saving plans: up to 72% applied to a specific instance within a chosen region. Terms of 1 or 3 years.</li> <li>Convertible Reserved Instances (RI): up to 66% fixed instance family, any size regional only. Terms of 1 or 3 years.</li> <li>Standard Reserved Instances (RI): up to 72% fixed instance family, any size regional only. Terms of 1 or 3 years.</li> </ul>"},{"location":"cloud/aws/cert-sap/#compute_1","title":"Compute","text":""},{"location":"cloud/aws/cert-sap/#aws-app-runner","title":"AWS App Runner","text":"<ul> <li>Regional service.</li> <li>AWS App Runner builds and deploys web applications automatically, load balances traffic with encryption, scales to meet your traffic needs, and allows for the configuration of how services are accessed and communicate with other AWS applications in a private Amazon VPC.</li> <li>Requires a container image.</li> <li>It is a regional service. If you require to do a multiple region deployment, you need to set up cross-region replication to the second region for the ECR images, deploy App Runner to the second Region and set up a Route 53 routing policy and use a global database like Aurora global database with write forwarding or DynamoDB global table.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-auto-scaling","title":"AWS Auto Scaling","text":"<ul> <li>AWS Auto Scaling, offers a centralized place to manage configurations for a wider range of scalable resources, like EC2, ECS, Amazon DynamoDB tables or Amazon Relational Database Aurora read replicas.</li> <li>AWS Auto Scaling introduced the concept of scaling plans, which use scaling strategies in order to manage resource utilization. Application owners can select a target utilization, such as CPU utilization at 50%, and AWS Auto Scaling will add or remove capacity to achieve that target.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-batch_1","title":"AWS Batch","text":"<ul> <li>Regional service. Single-AZ or multi-AZ depending on compute resources (EC2 or Fargate).</li> <li>AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. With AWS Batch, there is no need to install and manage batch computing software or server clusters that you use to run your jobs, allowing you to focus on analyzing results and solving problems.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-elastic-beanstalk","title":"AWS Elastic Beanstalk","text":"<ul> <li>Regional service. Can deploy Single-AZ or Multi-AZ.</li> <li>AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.</li> <li>The <code>.ebextensions</code> configuration files are used to customize the software that runs on the EC2 instances of your Elastic Beanstalk environment. You can use <code>.ebextensions</code> configuration files to mount the EFS onto the EC2 instances.</li> <li>It has different deployment policies:</li> <li>All at once \u2013 The quickest deployment method. Suitable if you can accept a short loss of service and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance.</li> <li>Rolling \u2013 Avoids downtime and minimizes reduced availability at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service. With this method, your application is deployed to your environment one batch of instances at a time.</li> <li>Rolling with additional batch \u2013 Avoids any reduced availability at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment.</li> <li>Immutable \u2013 A slower deployment method that ensures your new application version is always deployed to new instances instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails.</li> <li>Traffic splitting \u2013 A canary testing deployment method. Suitable if you want to test the health of your new application version using a portion of incoming traffic while keeping the rest of the traffic served by the old application version.</li> <li>You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. With this method, you can have two independent environments and you can quickly switch between the version by swapping the URLs.</li> <li>You can use platform hooks to run custom scripts: prebuild, predeploy and postdeploy.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-elastic-compute-cloud-amazon-ec2","title":"Amazon Elastic Compute Cloud (Amazon EC2)","text":"<ul> <li>Regional service where each instance is in a single AZ, AMIs are regional. Cluster placement groups are single-AZ.</li> <li>Setting up a diversified allocation strategy for your Spot Fleet is a best practice to increase the chances that a spot request can be fulfilled by EC2 capacity in the event of an outage in one of the Availability Zones. You can include each AZ available to you in the launch specification. And instead of using the same subnet each time, use three unique subnets (each mapping to a different AZ).</li> <li>Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. </li> <li>You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload.</li> <li>Cluster \u2013 packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.</li> <li>Partition \u2013 spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.</li> <li>Spread \u2013 strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.</li> <li>A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that are launched based on criteria that you specify. The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet.</li> <li>EC2Rescue can help you diagnose and troubleshoot problems on Amazon EC2 Linux and Windows Server instances. You can run the tool manually, or you can run the tool automatically by using Systems Manager Automation and the AWSSupport-ExecuteEC2Rescue document. The AWSSupport-ExecuteEC2Rescue document is designed to perform a combination of Systems Manager actions, AWS CloudFormation actions, and Lambda functions that automate the steps normally required to use EC2Rescue.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-ec2-auto-scaling","title":"Amazon EC2 Auto Scaling","text":"<ul> <li>Regional service.</li> <li>EC2 Auto Scaling provides several methods:</li> <li>Manual Scaling: You can manually adjust the desired capacity of your Auto -Scaling group. This method is useful when you need to make immediate changes to your group's capacity.</li> <li>Scheduled Scaling: This method allows you to set up scheduled actions to scale your group at specific times. It's ideal for predictable load changes that occur at fixed times.</li> <li>Dynamic Scaling: This approach automatically adjusts capacity in response to changing demand. There are three types of dynamic scaling policies: a. Target Tracking Scaling: Adjusts capacity to maintain a specific metric at a target value. b. Step Scaling: Uses step adjustments to scale based on the size of the alarm breach. c. Simple Scaling: Adjusts capacity based on a single scaling adjustment.</li> <li>Predictive Scaling: This method uses machine learning to forecast future traffic and automatically provisions the right number of EC2 instances in advance. It's useful for handling cyclical traffic patterns.</li> <li>You can suspend the EC2 termination if you need to investigate the ASG unhealty EC2.</li> <li>When Amazon EC2 Auto Scaling responds to a scale out event, it launches one or more instances. These instances start in the <code>Pending</code> state. If you added an <code>autoscaling:EC2_INSTANCE_LAUNCHING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Pending</code> state to the <code>Pending:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Pending:Proceed</code> state. When the instances are fully configured, they are attached to the Auto Scaling group and they enter the InService state.</li> <li>When Amazon EC2 Auto Scaling responds to a scale in event, it terminates one or more instances. These instances are detached from the Auto Scaling group and enter the <code>Terminating</code> state. If you added an <code>autoscaling:EC2_INSTANCE_TERMINATING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Terminating:Proceed</code> state. When the instances are fully terminated, they enter the <code>Terminated</code> state.</li> <li>If you want to keep the last logs of an EC2 from a terminated instance in Auto Scaling Group, you can dd a lifecycle hook to the Auto Scaling Group for the <code>autoscaling:EC2_INSTANCE_TERMINATING</code> event and set the default result to <code>CONTINUE</code>. Implement a script using an AWS Systems Manager Automation document to backup log data to an Amazon S3 bucket. Create an Amazon EventBridge rule that invokes a Lambda function when an instance is in the <code>Terminating:Wait</code> state. Configure the function to call the <code>SendCommand</code> API to run the automation document.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-fargate","title":"AWS Fargate","text":"<ul> <li>Regional service.</li> <li>AWS Fargate only supports the \"awsvpc\" network mode. Each task is allocated its own elastic network interface (ENI) that is used for communication inside the VPC.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-lambda","title":"AWS Lambda","text":"<ul> <li>Regional service.</li> <li>Can run up to 15 min.</li> <li>In scenarios where you need more than 15 min for async tasks maybe you can solve the problem using ECS task triggered from lambda.</li> <li>Lambda function URLs are HTTP(S) endpoints dedicated to your Lambda function. You can easily create and set up a function URL using the Lambda console or API. Once created, Lambda generates a unique URL endpoint for your use. Function URLs are dual stack-enabled, supporting IPv4 and IPv6. After you configure a function URL for your function, you can invoke your function through its HTTP(S) endpoint via a web browser, curl, Postman, or any HTTP client.</li> <li>CloudWatch Lambda Insights is primarily designed for monitoring and troubleshooting Lambda functions. It provides deeper visibility into the execution environment of your Lambda functions, allowing you to diagnose and resolve issues more efficiently.</li> <li>You can use a lambda function to update an ASG capacity limit and promote a RDS read replica to primary for DR, it will be triggered with SNS linkes to Route 53 health check.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-lightsail","title":"Amazon Lightsail","text":"<ul> <li>Regional service with Single-AZ.</li> <li>VPS provider and is a useful way to get started with AWS for users who need a solution to build and host their applications on AWS Cloud.</li> <li>Provides low-cost, pre-configured cloud resources for simple workloads just starting on AWS.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-outposts","title":"AWS Outposts","text":"<ul> <li>On-premises cloud storage that includes EBS and S3 services.</li> <li>Offers the same functionality but on-premise.</li> <li>You can create resources like EC2, EBS, ECS, EKS, ElastiCache, EMR, S3, ALB, App Mesh Envoy proxy and RDS.</li> <li>Requires connectivity to an AWS Region. A service link is a network route that enables communication between your Outpost and its associated AWS Region. Each Outpost is an extension of an Availability Zone and its associated Region.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-wavelength","title":"AWS Wavelength","text":"<ul> <li>Regional service.</li> <li>AWS Wavelength helps you build and deploy applications that meet your data residency, security, and low-latency requirements leveraging AWS services and APIs for digital transformation and using familiar tools for automation, deployments, security, and operational consistency enabling you to support telecom, finance, public sector, healthcare, and gaming use cases.</li> </ul>"},{"location":"cloud/aws/cert-sap/#containers","title":"Containers","text":""},{"location":"cloud/aws/cert-sap/#amazon-elastic-container-registry-amazon-ecr","title":"Amazon Elastic Container Registry (Amazon ECR)","text":"<ul> <li>Regional service.</li> <li>Container registry.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-elastic-container-service-amazon-ecs","title":"Amazon Elastic Container Service (Amazon ECS)","text":"<ul> <li>Regional service.</li> <li>ECS task execution role is for the ECS agent (pull images from ECS, using the awslogs, etc) and the ECS task role is for the task itself (services that the code needs, like S3 for example). </li> <li>If Amazon ECS Spot Instance draining is enabled on the instance, ECS receives the Spot Instance interruption notice and places the instance in DRAINING status. When a container instance is set to DRAINING, Amazon ECS prevents new tasks from being scheduled for placement on the container instance. Service tasks on the draining container instance that are in the PENDING state are stopped immediately. If there are container instances in the cluster that are available, replacement service tasks are started on them. Spot Instance draining is disabled by default and must be manually enabled by adding the line <code>ECS_ENABLE_SPOT_INSTANCE_DRAINING=true</code> on your <code>/etc/ecs/ecs.config file</code>.</li> <li>Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.</li> <li>You can configure various Docker networking modes that will be used by containers in your ECS task. The valid values are <code>none</code>, <code>bridge</code>, <code>awsvpc</code>, and <code>host</code>. The default Docker network mode is <code>bridge</code>.</li> <li>If the network mode is set to <code>none</code>, the task's containers do not have external connectivity, and port mappings can't be specified in the container definition.</li> <li>If the network mode is <code>bridge</code>, the task utilizes Docker's built-in virtual network which runs inside each container instance.</li> <li>If the network mode is <code>host</code>, the task bypasses Docker's built-in virtual network and maps container ports directly to the EC2 instance's network interface directly. In this mode, you can't run multiple instantiations of the same task on a single container instance when port mappings are used.</li> <li>If the network mode is <code>awsvpc</code>, the task is allocated an elastic network interface, and you must specify a <code>NetworkConfiguration</code> when you create a service or run a task with the task definition. When you use this network mode in your task definitions, every task that is launched from that task definition gets its own elastic network interface (ENI) and a primary private IP address. The task networking feature simplifies container networking and gives you more control over how containerized applications communicate with each other and other services within your VPCs. Task networking provides greater security for your containers by allowing you to use security groups and network monitoring tools at a more granular level within your tasks. Because each task gets its own ENI, you can also take advantage of other Amazon EC2 networking features like VPC Flow Logs so that you can monitor traffic to and from your tasks. Additionally, containers that belong to the same task can communicate over the localhost interface. A task can only have one ENI associated with it at a given time.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-ecs-anywhere","title":"Amazon ECS Anywhere","text":"<ul> <li>ECS Anywhere is a feature of Amazon ECS that lets you run and manage container workloads on your infrastructure. This feature helps you meet compliance requirements and scale your business without sacrificing your on-premises investments.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-elastic-kubernetes-service-amazon-eks","title":"Amazon Elastic Kubernetes Service (Amazon EKS)","text":"<ul> <li>Regional service.</li> <li>A solution to use when needs Kubernetes comatibility.</li> <li>Main solution to use if the requiremens mention multi-cloud configuration that runs additional clusters on other cloud service providers to further improve the site\u2019s performance.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-eks-anywhere","title":"Amazon EKS Anywhere","text":"<ul> <li>Amazon EKS Anywhere is built on the Kubernetes sub-project Cluster API (CAPI) and supports a range of infrastructure including VMware vSphere, bare metal, Nutanix, Apache CloudStack, and AWS Snow. Amazon EKS Anywhere can be run in air-gapped environments and offers optional integrations with regional AWS services for observability and identity management. </li> <li>For customers operating Kubernetes clusters on-premises.</li> <li>EKS Connector is a software agent that runs on a Kubernetes cluster and enables the cluster to register with the Amazon EKS console.</li> <li>Amazon EKS Anywhere is not supported on AWS Outposts. If you plan to run Kubernetes on AWS Outposts, we strongly recommend using Amazon EKS on AWS Outposts.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-eks-distro","title":"Amazon EKS Distro","text":"<ul> <li>Amazon EKS Distro is a Kubernetes distribution of the same open source components and dependencies deployed by Amazon EKS. Amazon EKS Distro includes binaries and containers of open source Kubernetes, etcd, networking, and storage plugins, all tested for compatibility. Amazon EKS Distro provides extended support for Kubernetes versions after community support expires. You can securely access Amazon EKS Distro releases from GitHub or within AWS via Amazon S3 and Amazon ECR for a common source of releases and updates.</li> </ul>"},{"location":"cloud/aws/cert-sap/#database","title":"Database","text":""},{"location":"cloud/aws/cert-sap/#amazon-aurora","title":"Amazon Aurora","text":"<ul> <li>Regional service.</li> <li>Offers high availability by default.</li> <li>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. This provides your application with an effective Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan.</li> <li>You can set auto scaling for replica database but not for master.</li> <li>You can switch your existing database clusters once every 30 days to Aurora I/O-Optimized. You can switch back to Aurora Standard at any time.</li> <li>Encrypting an existing unencrypted Aurora instance is not supported. To use Amazon Aurora encryption for an existing unencrypted database, create a new DB Instance with encryption enabled and migrate your data into it.</li> <li>Aurora Auto Scaling uses a scaling policy to adjust the number of Aurora Read Replicas in an Aurora DB cluster.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-aurora-serverless","title":"Amazon Aurora Serverless","text":"<ul> <li>It is recommended to use Aurora Serverless for lightly-used applications, with peaks of 30 minutes to several hours a few times each day or several times per year, such as human resources, budgeting, or operational reporting application.</li> <li>In Aurora Serverless, database capacity is measured in Aurora Capacity Units (ACUs). You pay a flat rate per second of ACU usage.</li> <li>1 ACU has approximately 2 GiB of memory with corresponding CPU and networking.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-aurora-postgresql-limitless-database-horizontal-scaling","title":"Amazon Aurora PostgreSQL Limitless Database (Horizontal Scaling)","text":"<ul> <li>For applications that need to scale horizontally and require more write throughput or data storage capacity than a single Aurora database instance supports.</li> <li>Supports three types of tables:</li> <li>Sharded tables: These tables are distributed across multiple shards. Data is split among the shards based on the values of designated columns in the table, called shard keys. They are useful for scaling the largest, most I/O-intensive tables in your application.</li> <li>Reference tables: These tables copy data in full on every shard so that join queries can work faster by removing unnecessary data movement. They are commonly used for infrequently modified reference data, such as product catalogs and zip codes.</li> <li>Standard tables: These tables are like regular Aurora PostgreSQL tables. Standard tables are all placed together on a single shard so join queries can work faster by removing unnecessary data movement. You can create sharded and reference tables from standard tables.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-documentdb-with-mongodb-compatibility","title":"Amazon DocumentDB (with MongoDB compatibility)","text":"<ul> <li>Regional service.</li> <li>Amazon DocumentDB interacts with the Apache 2.0 open source MongoDB 3.6, 4.0, and 5.0 APIs.</li> <li>You can switch your existing database clusters once every 30 days to Amazon DocumentDB I/O-Optimized. You can switch back to Amazon DocumentDB standard storage configurations at any time.</li> <li>Amazon DocumentDB Elastic Clusters enables you to elastically scale your document database to handle millions of writes and reads, with petabytes of storage capacity.</li> <li>Choosing an optimal shard key for Elastic Clusters is no different than other databases. A great shard key has two characteristics - high frequency and high cardinality. For example, if your application stores user_orders in DocumentDB, then generally you have to retrieve the data by the user.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-dynamodb","title":"Amazon DynamoDB","text":"<ul> <li>Regional service.</li> <li>Amazon DynamoDB global tables provide you with a fully managed, multi-region and multi-active database.</li> <li>DynamoDB Accelerator is used for caching requests if you need response times in microseconds.</li> <li>Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. TTL is provided at no extra cost as a means to reduce stored data volumes by retaining only the items that remain current for your workload\u2019s needs.</li> <li>You can purchase reserved capacity in advance to lower the costs of running your DynamoDB instance.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-elasticache","title":"Amazon ElastiCache","text":"<ul> <li>Regional service. Single-AZ only for Memcache.</li> <li>ElastiCache offers fully managed Valkey, Memcached, and Redis OSS for your most demanding applications that require submillisecond response times.</li> <li>ElastiCache Serverless continuously monitors your cache\u2019s memory, compute, and network utilization to instantly scale.</li> <li>Global Datastore is a feature of ElastiCache that provides fully managed, fast, reliable and security-focused cross-Region replication. With Global Datastore, you can write to your cache in one Region and have the data available for read in up to two other cross-Region replica clusters, thereby enabling low-latency reads and disaster recovery across Regions.</li> <li>Use Memcached for simple data types and multi-thread.</li> <li>Redis and Valkey are Multi-AZ and support pub/sub, sorted sets and geospatial indexing.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-keyspaces-for-apache-cassandra_1","title":"Amazon Keyspaces (for Apache Cassandra)","text":"<ul> <li>Regional service.</li> <li>Amazon Keyspaces is designed to be compatible with Apache Cassandra databases.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-neptune","title":"Amazon Neptune","text":"<ul> <li>Regional service.</li> <li>Graph database.</li> <li>Supports Gremlin (Apache TinkerPop), openCypher and RDF/SPARQL.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-relational-database-service-amazon-rds","title":"Amazon Relational Database Service (Amazon RDS)","text":"<ul> <li>Regional service. Supports Single-AZ and Multi-AZ.</li> <li>Used for OLTP scenarios.</li> <li>Amazon RDS does not support certain features in Oracle such as Multitenant Database, Real Application Clusters (RAC), RMAN, Unified Auditing, Database Vault, and many more.</li> <li>Amazon RDS Proxy is a fully managed, highly available database proxy for Amazon Relational Database Service (RDS) that makes applications more scalable, more resilient to database failures, and more. Amazon RDS Proxy sits between your application and your relational database to efficiently manage connections to the database and improve the scalability of the application. Amazon RDS Proxy can be enabled for most applications with no code changes.</li> <li>Database sharding is the process of storing a large database across multiple machines. A single machine, or database server, can store and process only a limited amount of data. Database sharding overcomes this limitation by splitting data into smaller chunks, called shards, and storing them across several database servers. All database servers usually have the same underlying technologies, and they work together to store and process large volumes of data.</li> <li>You can set up replication between an Amazon RDS MySQL (or MariaDB DB instance) that is running in AWS and a MySQL (or MariaDB instance) to your on-premises data center.</li> <li>RDS Optimized Writes feature provides you with up to 2x improvement in write transaction throughput on RDS for MySQL by writing only once while protecting you from data loss.</li> <li>RDS Optimized Reads feature is commonly used to achieve faster query processing by placing temporary tables generated by MySQL on NVMe-based SSD block storage that is physically connected to the host server.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-redshift","title":"Amazon Redshift","text":"<ul> <li>Regional service.</li> <li>WS KMS keys are specific to a region. If you want to enable cross-region snapshot copy for an AWS KMS-encrypted cluster, you must configure a <code>snapshot copy grant</code> for a master key in the destination region so that Amazon Redshift can perform encryption operations in the destination region.</li> <li>Used for OLAP scenarios.</li> <li>Redshift is configured with automatic snapshot by default but you need to enable cross-region snapshot if you require it.</li> <li>Amazon Redshift only has cross-region backup feature (using snapshots), not Cross-Region Replication.</li> <li>Redshif Spectrum can be used to analyze data stored in S3.</li> <li>Apache Parquet is an open-source file format that is optimized for use with big data processing frameworks. It stores data in a columnar format, which means it organizes the data by columns rather than by rows. This can lead to significant performance improvements when executing analytical queries.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-timestream_1","title":"Amazon Timestream","text":"<ul> <li>Regional service.</li> <li>Amazon Timestream offers fully managed InfluxDB, one of the most popular open source time-series databases in the market, and LiveAnalytics, a serverless time-series database built for scale.</li> <li>Amazon Timestream for InfluxDB should be used for use cases that require near real-time time-series queries and when you need InfluxDB features or open source APIs. The existing Timestream engine, Amazon Timestream for LiveAnalytics, should be used when you need to ingest more than tens of gigabytes of time-series data per minute and run SQL queries on terabytes of time-series data in seconds.</li> </ul>"},{"location":"cloud/aws/cert-sap/#developer-tools","title":"Developer Tools","text":""},{"location":"cloud/aws/cert-sap/#aws-codeartifact","title":"AWS CodeArtifact","text":"<ul> <li>AWS CodeArtifact is used to automatically fetch software packages and dependencies from public artifact repositories.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-codebuild","title":"AWS CodeBuild","text":"<ul> <li>AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. AWS CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-codedeploy","title":"AWS CodeDeploy","text":"<ul> <li>AWS CodeDeploy helps to deploy the changes in your desired environment.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-codeguru","title":"Amazon CodeGuru","text":"<ul> <li>CodeGuru Security is a machine learning (ML) and program analysis-based tool that finds security vulnerabilities in your application code. CodeGuru Security also scans for hardcoded credentials. CodeGuru Profiler optimizes performance for applications running in production and identifies the most expensive lines of code, reducing operational costs significantly.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-codepipeline","title":"AWS CodePipeline","text":"<ul> <li>AWS CodePipeline is a continuous delivery service for fast and reliable application and infrastructure updates. CodePipeline builds, tests, and deploys your code every time there is a code change based on the release process models you define. Supports executions from third-party Git sources.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-x-ray","title":"AWS X-Ray","text":"<ul> <li>With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application\u2019s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.</li> </ul>"},{"location":"cloud/aws/cert-sap/#end-user-computing","title":"End User Computing","text":""},{"location":"cloud/aws/cert-sap/#amazon-appstream-20","title":"Amazon AppStream 2.0","text":"<ul> <li>Regional service.</li> <li>Amazon AppStream 2.0 is a fully managed application streaming service. You centrally manage your desktop applications on AppStream 2.0 and securely deliver them to any computer. You can easily scale to any number of users across the globe without acquiring, provisioning, and operating hardware or infrastructure.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-workspaces","title":"Amazon WorkSpaces","text":"<ul> <li>Regional service.</li> <li>Amazon WorkSpaces allows you to control which IP addresses your WorkSpaces can be accessed from. By using IP address-based control groups, you can define and manage groups of trusted IP addresses and only allow users to access their WorkSpaces when connected to a trusted network.</li> <li>An IP access control group acts as a virtual firewall that controls the IP addresses from which users are allowed to access their WorkSpaces. To specify the CIDR address ranges, add rules to your IP access control group and then associate the group with your directory. You can associate each IP access control group with one or more directories. You can create up to 100 IP access control groups per Region per AWS account. However, you can only associate up to 25 IP access control groups with a single directory.</li> </ul>"},{"location":"cloud/aws/cert-sap/#frontend-web-and-mobile","title":"Frontend Web and Mobile","text":""},{"location":"cloud/aws/cert-sap/#aws-amplify","title":"AWS Amplify","text":"<ul> <li>AWS Amplify consists of a set of tools (open source framework, visual development environment, console) and services (web app and static website hosting) to accelerate the development of mobile and web applications on AWS.</li> <li>Amplify's open source framework includes an opinionated set of libraries, UI components, and a command line interface (CLI) to build an app backend and integrate it with your iOS, Android, Web, and React Native apps. The framework leverages a core set of AWS Cloud Services to offer capabilities including offline data, authentication, analytics, push notifications, and bots at high scale.</li> <li>AWS Amplify web hosting is priced for two features \u2013 build &amp; deploy, and web hosting.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-api-gateway_1","title":"Amazon API Gateway","text":"<ul> <li>Regional service.</li> <li>Caching the API requests should be done on the API Gateway. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.</li> <li>Switching the existing Amazon API Gateway from a Regional endpoint to an Edge-Optimized endpoint would enhance API call performance for users worldwide.</li> <li>API Gateway supports multiple mechanisms for controlling and managing access:</li> <li>Resource policies let you create resource-based policies to allow or deny access to your APIs and methods from specified source IP addresses or VPC endpoints.</li> <li>Standard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs, as well as who can invoke them.</li> <li>IAM tags can be used together with IAM policies to control access.</li> <li>Endpoint policies for interface VPC endpoints allow you to attach IAM resource policies to interface VPC endpoints to improve the security of your private APIs.</li> <li>Lambda authorizers are Lambda functions that control access to REST API methods using bearer token authentication\u2014as well as information described by headers, paths, query strings, stage variables, or context variables request parameters. Lambda authorizers are used to control who can invoke REST API methods.</li> <li>Amazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-device-farm","title":"AWS Device Farm","text":"<ul> <li>AWS Device Farm is an app testing service. It lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real-time.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-pinpoint","title":"Amazon Pinpoint","text":"<ul> <li>Amazon Pinpoint is a versatile AWS service designed for engaging with customers through various messaging channels. It allows you to send push notifications, emails, SMS text messages, and voice messages. Beyond basic messaging, Pinpoint helps build and execute targeted marketing campaigns, segment audiences, and analyze campaign performance across different channels.</li> </ul>"},{"location":"cloud/aws/cert-sap/#internet-of-things-iot","title":"Internet of Things (IoT)","text":""},{"location":"cloud/aws/cert-sap/#aws-iot-core","title":"AWS IoT Core","text":"<ul> <li>Regional service.</li> <li>AWS IoT Core is the central component of AWS IoT that provides the communication infrastructure for connecting IoT devices to the AWS cloud. Supports MQTT, MQTT over WSS, HTTPS and LoRaWAN. The data plane endpoints are specific to each AWS account and AWS Region (the Data-ATS). </li> <li>AWS IoT Core Basic Ingest is specifically designed for high-volume data ingestion, allowing devices to publish messages directly to the AWS IoT Rules Engine without maintaining a persistent connection, which helps reduce overhead.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-iot-device-defender","title":"AWS IoT Device Defender","text":"<ul> <li>AWS IoT Defender is primarily used for security audit, alerting, and mitigation of IoT resources.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-iot-device-management","title":"AWS IoT Device Management","text":"<ul> <li>AWS IoT Device Management is a service that makes it easy to securely register, organize, monitor, and remotely manage IoT devices at scale throughout their lifecycle. You can use IoT Device Management to upload and view device information and configuration, organize your device inventory, monitor your fleet of devices, troubleshoot individual devices, and remotely manage devices deployed across many locations including updating device software over-the-air (OTA).</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-iot-events","title":"AWS IoT Events","text":"<ul> <li>AWS IoT Events is a new IoT service that helps companies continuously monitor their equipment and fleets of devices for failure or changes in operation and trigger alerts to respond when events occur.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-iot-greengrass","title":"AWS IoT Greengrass","text":"<ul> <li>AWS IoT Greengrass is an Internet of Things (IoT) open source edge runtime and cloud service that helps you build, deploy, and manage device software. Customers use AWS IoT Greengrass for their IoT applications on millions of devices in homes, factories, vehicles, and businesses. You can program your devices to act locally on the data they generate, execute predictions based on machine learning models, filter and aggregate device data, and only transmit necessary information to the cloud.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-iot-sitewise","title":"AWS IoT SiteWise","text":"<ul> <li>AWS IoT SiteWise is used for collecting and gaining insights to equipment data for industrial operations management.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-iot-things-graph","title":"AWS IoT Things Graph","text":"<ul> <li>You can represent your business logic in a flow composed of devices and services.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-iot-1-click","title":"AWS IoT 1-Click","text":"<ul> <li>Simple devices that are ready to execute a Lambda function when activated.</li> </ul>"},{"location":"cloud/aws/cert-sap/#machine-learning","title":"Machine Learning","text":""},{"location":"cloud/aws/cert-sap/#amazon-comprehend","title":"Amazon Comprehend","text":"<ul> <li>Regional service.</li> <li>You can use Amazon Comprehend to identify the language of the text, extract key phrases, places, people, brands, or events, understand sentiment about products or services, and identify the main topics from a library of documents.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-fraud-detector","title":"Amazon Fraud Detector","text":"<ul> <li>Regional service.</li> <li>To cover use cases like suspicious payments, new account frauds, trial and loyalty program abuse and account takeover.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-kendra","title":"Amazon Kendra","text":"<ul> <li>Regional service.</li> <li>Amazon Kendra is a highly accurate and easy-to-use enterprise search service that\u2019s powered by machine learning (ML). It allows developers to add search capabilities to their applications so their end users can discover information stored within the vast amount of content spread across their company.</li> <li>Multiple data origins likeS3, Microsoft SharePoint, Salesforce, ServiceNow, RDS databases, or Microsoft OneDrive.</li> <li>Amazon Kendra does not support questions where the answers require cross-document passage aggregation or calculations.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-lex","title":"Amazon Lex","text":"<ul> <li>Regional service.</li> <li>Amazon Lex uses AWS Lambda functions to query your business applications and make updates as requested.</li> <li>Amazon Lex chatbots also maintain context and manage the dialogue, dynamically adjusting responses based on the conversation.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-personalize","title":"Amazon Personalize","text":"<ul> <li>Regional service.</li> <li>Amazon Personalize is a fully managed machine learning (ML) service that uses your data to generate product and content recommendations for your users. You provide data about your end-users (e.g., age, location, device type), items in your catalog (e.g., genre, price) and interactions between users and items (e.g., clicks, purchases). Personalize uses this data to train custom, private models that generate recommendations which can be surfaced via an API.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-polly","title":"Amazon Polly","text":"<ul> <li>Regional service.</li> <li>Used to convert text to audio.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-rekognition","title":"Amazon Rekognition","text":"<ul> <li>Regional service.</li> <li>Supports the IndexFaces operation. To store facial information, you must first create (CreateCollection) a face collection in one of the AWS Regions in your account. You specify this face collection when you call the IndexFaces operation. After you create a face collection and store facial feature information for all faces, you can search the collection for face matches. To search for faces in an image, call SearchFacesByImage. To search for faces in a stored video, call StartFaceSearch. To search for faces in a streaming video, call CreateStreamProcessor.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-sagemaker-ai-previously-known-as-amazon-sagemaker","title":"Amazon SageMaker AI (previously known as Amazon SageMaker)","text":"<ul> <li>Regional service.</li> <li>Used to train and refine your ML models.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-textract","title":"Amazon Textract","text":"<ul> <li>Regional service.</li> <li>Used to extract text from documents.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-transcribe","title":"Amazon Transcribe","text":"<ul> <li>Regional service.</li> <li>Used to convert audio to text.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-translate","title":"Amazon Translate","text":"<ul> <li>Regional service.</li> <li>Used to translate text between languages.</li> </ul>"},{"location":"cloud/aws/cert-sap/#management-and-governance","title":"Management and Governance","text":""},{"location":"cloud/aws/cert-sap/#aws-cloudformation","title":"AWS CloudFormation","text":"<ul> <li>With Amazon CloudFormation StackSets you can define an AWS resource configuration in a CloudFormation template and then roll it out across multiple AWS accounts and/or Regions with a couple of clicks.</li> <li>In order for StackSets to have the necessary permissions to perform operations in AWS Organizations' accounts, it must be enabled as a trusted service. To ensure the template is automatically deployed to all accounts (including newly joined ones), a stack set should be created in the management account with service-managed permissions. The AWS Organizations should be set as the deployment target, and automatic deployments should be enabled.</li> <li>On CloudFormation you can set the resources DeletionPolicy as <code>Retain</code> which keeps the resource without deleting it or its contents when its stack is deleted.</li> <li>On CloudFormation you can set the resources DeletionPolicy as <code>Snapshot</code> which keeps the resource backup data when deleting it or its contents when its stack is deleted.</li> <li>The <code>AWS::AutoScaling::AutoScalingGroup</code> resource defines an Amazon EC2 Auto Scaling group. You can add an <code>UpdatePolicy</code> attribute to your stack to perform rolling updates (or replace the group) when a change has been made to the group. Alternatively, you can force a rolling update on your instances at any time after updating the stack by starting an instance refresh. To specify how AWS CloudFormation handles rolling updates for an Auto Scaling group, use the <code>AutoScalingRollingUpdate</code> policy. Rolling updates enable you to specify whether AWS CloudFormation updates instances that are in an Auto Scaling group in batches or all at once. For example, suppose you have updated the <code>MaxBatchSize</code> in your stack template's <code>UpdatePolicy</code> from 1 to 10. This allows you to perform updates without causing downtime to your currently running application.</li> <li>You can get enable data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the Organizations master account to deploy stacks to all accounts in your organization or in specific organizational units (OUs).</li> <li>Instance profiles are used to pass an IAM role to an EC2 instance, in the template you reference the IAM Role as a property inside the <code>AWS::IAM::InstanceProfile</code> of the application instance.</li> <li>AWS Cloud Development Kit (AWS CDK) is a software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-cloudtrail","title":"AWS CloudTrail","text":"<ul> <li>Can be used to monitor management events (operations in aws, the control plane) and data events (operations in a service like S3, SQS, DynamoDB, Lambda, the data plane). Can optionally perform anomaly detection with insights events.</li> <li>You can use CloudTrail to montior calls to AWS Organizations and EventBridge and SNS to raise events when certain actions occur.</li> <li>An organizational trail in AWS CloudTrail is a type of trail that logs and monitors activity across all AWS accounts within an AWS Organization. The management account or delegated admin can create an organization trail that automatically enables logging in all current and future member accounts of the organization.</li> <li>You can create an AWS CloudTrail train in a new (to avoid current permissions causing a confidentiality breach) S3 bucket to persist logs about the actions performed to the resources. With <code>--is-multi-region-trail</code> you enable all the region services, and with <code>--include-global-service-events</code> you include the global services. This logs can be encrypted using KMS. Also you can enable MFA for S3 removal and restrict access configuring bucket policies.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-cloudwatch","title":"Amazon CloudWatch","text":"<ul> <li>A CloudWatch alarm to detect service quota near it's limits can be created using the expression <code>metricid/SERVICE_QUOTA(metricid)*100</code> from the metrics in the <code>AWS/Usage</code> namespace.</li> <li>Use CloudWatch Application Signals to automatically instrument your applications on AWS so that you can monitor current application health and track long-term application performance against your business objectives. Creating SLOs is very important for getting the most benefit from CloudWatch Application Signals.</li> <li>Transaction Search is an interactive analytics experience you can use to get complete visibility of your application transaction spans.</li> <li>You can use Amazon CloudWatch Synthetics to create canaries, configurable scripts that run on a schedule, to monitor your endpoints and APIs. Offers different blueprints:</li> <li>Heartbeat monitor: load specified URL, store screenshot and HAR file.</li> <li>API canary: rest calls.</li> <li>Broken link checker: collects all the links in a url (including itself) to the configured limit.</li> <li>Visual monitoring: compares screenshots taken during a canary run with a baseline canary run. Uses a threshold to determine differences and fail.</li> <li>Canary recorder: you use Syntethics Recorder to record your click and type actions.</li> <li>GUI Workflow: you define steps to click, input text and verify selector and text.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-cloudwatch-logs","title":"Amazon CloudWatch Logs","text":"<ul> <li>Regional service.</li> <li>To collect logs from your Amazon EC2 instances and on-premises servers into CloudWatch Logs, AWS offers the unified CloudWatch agent.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-command-line-interface-aws-cli","title":"AWS Command Line Interface (AWS CLI)","text":"<ul> <li>CLI.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-compute-optimizer","title":"AWS Compute Optimizer","text":"<ul> <li>AWS Compute Optimizer helps you choose the optimal AWS Compute resources for your workloads. Compute Optimizer delivers intuitive and actionable recommendations to help you identify the optimal AWS Compute resources.AWS Compute Optimizer provides recommendations for optimal AWS resource utilization based on historical usage data.</li> <li>After you opt-in, Compute Optimizer starts to scan your AWS infrastructure and generates recommendations. It may take up to 12 hours for Compute Optimizer to deliver recommendations for all supported AWS Compute resources.</li> <li>AWS Compute Optimizer provides a set of APIs and a console experience to help you reduce costs and increase workload performance by recommending the optimal AWS resources for your AWS workloads.</li> <li>The <code>ExportLambdaFunctionRecommendations</code> API gives recommended configurations based on historical performance lf your Lambda functions.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-config","title":"AWS Config","text":"<ul> <li>Regional service.</li> <li>AWS Config provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices.</li> <li>You can use AWS Config data aggegator (resource type that collects AWS Config data from multiple source accounts and regions) to montior the compliance of your AWS organizations, IAM, etc, and EventBridge and SNS to send alerts when changes are detected.</li> <li>With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-control-tower","title":"AWS Control Tower","text":"<ul> <li>Regional service.</li> <li>Automates setup of a secure multi-account AWS environment (landing zone).</li> <li>Uses AWS Organizations to manage organizational units (OUs) and accounts.</li> <li>Creates baseline accounts: Management, Log Archive, and Audit.</li> <li>Applies preventive and detective guardrails (SCPs and AWS Config rules).</li> <li>Provides a dashboard to monitor account status and compliance.</li> <li>Includes Account Factory to create and configure new accounts with best practices.</li> <li>Supports drift detection to identify configuration changes outside of Control Tower.</li> <li>Integrates with AWS IAM Identity Center (SSO) for centralized access management.</li> <li>Works with AWS CloudTrail, AWS Config, and AWS Service Catalog.</li> <li>Enables extension and customization through APIs and lifecycle hooks.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-health-dashboard","title":"AWS Health Dashboard","text":"<ul> <li>To view account-specific health information or receive AWS Health event updates using Amazon EventBridge.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-license-manager","title":"AWS License Manager","text":"<ul> <li>Regional service.</li> <li>AWS License Manager is used to create customized licensing rules that emulate the terms of their licensing agreements and then enforce these rules.</li> <li>It is not used for storing software licenses.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-managed-grafana","title":"Amazon Managed Grafana","text":"<ul> <li>Regional service.</li> <li>Build, package, and deploy workspaces that are provisioned, set up, scaled, and maintained for you.</li> <li>Migrate from your self-managed Grafana environment, so there\u2019s no need to start from scratch.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-managed-service-for-prometheus","title":"Amazon Managed Service for Prometheus","text":"<ul> <li>Regional service.</li> <li>Amazon Managed Service for Prometheus is a serverless monitoring service for metrics compatible with open source Prometheus, making it easier for you to securely monitor and alert on container environments.</li> <li>The Amazon Managed Service for Prometheus collector is an agentless scraper that enables customers to automatically discover and monitor their Amazon EKS applications and infrastructure by removing the undifferentiated heavy lifting of managing Prometheus agents to collect Prometheus metrics.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-management-console","title":"AWS Management Console","text":"<ul> <li>Web.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-organizations","title":"AWS Organizations","text":"<ul> <li>Global service.</li> <li>You can use organizational units (OUs) to group accounts together to administer as a single unit.</li> <li>Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs do not grant permissions to the IAM users and IAM roles in your organization. No permissions are granted by an SCP. An SCP defines a permission guardrail, or sets limits, on the actions that the IAM users and IAM roles in your organization can perform.</li> <li>You attach SCPs to the root, OUs, or directly to accounts.</li> <li>It is not recommended to attach the SCPs to the root of the organization.</li> <li>By default, an SCP named <code>FullAWSAccess</code> is attached to every root, OU, and account.</li> <li>SCPs do not affect any service-linked role. Service-linked roles enable other AWS services to integrate with AWS Organizations and can't be restricted by SCPs.</li> <li>You can use trusted access to enable an AWS service that you specify, called the trusted service, to perform tasks in your organization and its accounts on your behalf. This involves granting permissions to the trusted service but does not otherwise affect the permissions for IAM users or roles. When you enable access, the trusted service can create an IAM role called a service-linked role in every account in your organization. That role has a permissions policy that allows the trusted service to do the tasks that are described in that service's documentation. This enables you to specify settings and configuration details that you would like the trusted service to maintain in your organization's accounts on your behalf.</li> <li>For billing purposes, the consolidated billing feature of AWS Organizations treats all the accounts in the organization as one account. This means that all accounts in the organization can receive the hourly cost-benefit of Reserved Instances (RI) that are purchased by any other account. In the master account, you can turn off Reserved Instance discount sharing on the Preferences page on the Billing and Cost Management console for the desired accounts.</li> <li>After you create an Organization and verify that you own the email address associated with the master account, you can invite existing AWS accounts to join your organization. When you invite an account, AWS Organizations sends an invitation to the account owner, who decides whether to accept or decline the invitation. You can use the AWS Organizations console to initiate and manage invitations that you send to other accounts. You can send an invitation to another account only from the master account of your organization. When an invited account joins your organization, you do not automatically have full administrator control over the account, unlike created accounts. If you want the master account to have full administrative control over an invited member account, you must create the  <code>OrganizationAccountAccessRole</code> IAM role in the member account and grant permission to the master account to assume the role.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-proton","title":"AWS Proton","text":"<ul> <li>Regional service.</li> <li>AWS Proton is a deployment workflow tool for modern applications. It can be used to manage Infrastructure as Code (IaC) templates build using tools like CloudFormation or Terraform.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-service-catalog","title":"AWS Service Catalog","text":"<ul> <li>Regional service.</li> <li>Allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures. AWS Service Catalog allows you to centrally manage deployed IT services and your applications, resources, and metadata.</li> <li>With AWS Service Catalog, you define your own catalog of AWS services and AWS Marketplace software and make them available for your organization. Then, end users can quickly discover and deploy IT services using a self-service portal.</li> <li>AWS Service Catalog enables a self-service capability for users, allowing them to provision the services they need while also helping you to maintain consistent governance \u2013 including the application of required tags and tag values.</li> </ul>"},{"location":"cloud/aws/cert-sap/#service-quotas","title":"Service Quotas","text":"<ul> <li>A DynamoDB RCU is up to 4 KB for a 1 strong or 2 eventually consisten reads per second.</li> <li>A DynamoDB WCU is up to 1 KB for writes per second.</li> <li>Lambda can run up to 15 minutes.</li> <li>In Kinesis Data Streams the max amount of data per shard is 1 MB/s or 1000 RPS.</li> <li>In Kinesis Data Streams retention period is from 24h to 1 week.</li> <li>Max standard SQS delay is 15 minutes.</li> <li>Max SQS message payload is 256 Kb.</li> <li>Max SQS retention is 14 days.</li> <li>Api gateway max payload size is 10 Mb.</li> <li>Api gateway max request timeout is 29s.</li> <li>Aurora minimum storage is 10 GiB, automatically grows by 10 GiB increments up to 128 TiB.</li> <li>Max number of Aurora replicas are 15.</li> <li>DocumentDB minimum storage is 10 GiB, automatically grows by 10 GiB increments up to 128 TiB.</li> <li>Max number of DocumentDB replicas are 15.</li> <li>Smallest VPC range is /28 and larges /16.</li> <li>AWS reserves 5 ips in each subnet (first 4 for network, local router, dns, and nothing and last for broadcast).</li> <li>Up to 10 VPN connections can terminate to the same virtual private gateway.</li> <li>Maximum 50 VIFs (virtual interfaces) per Direct Connect connection.</li> <li>Direct connect gatewat can associate up to 10 virtual private gateways globally and cross account.</li> <li>The default limit for shared VPC subnets is 100.</li> <li>The default limit of VPC peering for each VPC is 50.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-systems-manager","title":"AWS Systems Manager","text":"<ul> <li>Regional service.</li> <li>Systems Manager Run Command service enables you to automate common administrative tasks and perform ad hoc configuration changes at scale.</li> <li>The AWS Systems Manager Maintenance Windows feature lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. Each Maintenance Window has a schedule, a maximum duration, a set of registered targets (the instances that are acted upon), and a set of registered tasks.</li> <li>AWS Systems Manager Patch Manager automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type.</li> <li>Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. For each auto-approval rule that you create, you can specify an auto-approval delay. This delay is the number of days of wait after the patch was released, before the patch is automatically approved for patching.</li> <li>A patch group is an optional means of organizing instances for patching. Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested. You create a patch group by using Amazon EC2 tags.</li> <li>Patch Manager provides a convenient view of patch status in the console and lets you export this data as a CSV file for analysis.</li> <li>AWS Systems Manager Automation provides several runbooks with pre-defined steps that you can use to perform common tasks like restarting one or more EC2 instances or creating an Amazon Machine Image (AMI). A Systems Manager Automation runbook defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation runs. A runbook contains one or more steps that run in sequential order. Each step is built around a single action. Output from one step can be used as input in a later step.</li> <li>AWS Systems Manager State Manager is primarily used as a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define.</li> <li>Session Manager is a fully managed AWS Systems Manager capability. With Session Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, and on-premises servers and virtual machines (VMs). You can use either an interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.</li> <li>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. If you plan to use Systems Manager to manage on-premises servers and virtual machines (VMs) in what is called a hybrid environment, you must create an IAM role for those resources to communicate with the Systems Manager service.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-trusted-advisor","title":"AWS Trusted Advisor","text":"<ul> <li>AWS Trusted Advisor is primarily used to check if your cloud infrastructure is in compliance with the best practices and recommendations across five categories: cost optimization, security, fault tolerance, performance, and service limits.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-well-architected-tool","title":"AWS Well-Architected Tool","text":"<ul> <li>To use the AWS WA Tool, define your workload, apply one of the AWS Well-Architected lenses or your own custom lens, and begin your review. The tool generates an improvement plan and provides a mechanism to track and measure your progress.</li> </ul>"},{"location":"cloud/aws/cert-sap/#media-services","title":"Media Services","text":""},{"location":"cloud/aws/cert-sap/#amazon-elastic-transcoder","title":"Amazon Elastic Transcoder","text":"<ul> <li>Regional service.</li> <li>Amazon Elastic Transcoder uses pipelines to manage transcoding jobs. When you create a job, you specify the pipeline that you want to submit the job to. Pipelines are closely tied to an S3 bucket that you specify. Queues in AWS Elemental MediaConvert are similar to pipelines. The number of concurrent jobs processed in a given queue is determined based on the number of queues in your account. However, one of the key differences is that queues are not tied to a specific S3 bucket. This means that you can submit jobs that reference input files in different S3 buckets to the same queue. In Elastic Transcoder, you select the playlist version upon job creation. In MediaConvert, the HLS version will change as you enable features that require a specific version.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-kinesis-video-streams","title":"Amazon Kinesis Video Streams","text":"<ul> <li>Regional service.</li> <li>Amazon Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), playback, and other processing. Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices.</li> <li>Kinesis Video Streams also supports WebRTC, an open-source project that enables real-time media streaming and interaction between web browsers, mobile applications, and connected devices via simple APIs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#migration-and-transfer","title":"Migration and Transfer","text":""},{"location":"cloud/aws/cert-sap/#migration-options","title":"Migration options","text":"<ul> <li>Rehosting\u200a\u2014\u200aOtherwise known as \u201clift-and-shift\u201d. Many early cloud projects gravitate toward net new development using cloud-native capabilities, but in a large legacy migration scenario where the organization is looking to scale its migration quickly to meet a business case, applications can be rehosted.</li> <li>Replatforming\u200a\u2014\u200aSometimes, this is called \u201clift-tinker-and-shift.\u201d Here you might make a few cloud (or other) optimizations in order to achieve some tangible benefit, but you aren\u2019t otherwise changing the core architecture of the application. You may be looking to reduce the amount of time you spend managing database instances by migrating to a database-as-a-service platform like Amazon Relational Database Service (Amazon RDS) or migrating your application to a fully managed platform like Amazon Elastic Beanstalk.</li> <li>Repurchasing\u200a\u2014\u200aMoving to a different product. Repurchasing is a move to a SaaS platform. Moving a CRM to Salesforce.com, an HR system to Workday, a CMS to Drupal, etc.</li> <li>Refactoring / Re-architecting\u200a\u2014\u200aRe-imagining how the application is architected and developed, typically using cloud-native features. This is typically driven by a strong business need to add features, scale, or performance that would otherwise be difficult to achieve in the application\u2019s existing environment. For example, migrating from a monolithic architecture to a service-oriented (or server-less) architecture to boost agility.</li> <li>Retire\u200a\u2014\u200aThis strategy basically means: \"Get rid of.\" Once you\u2019ve discovered everything in your environment, you might ask each functional area who owns each application and see that some of the applications are no longer used. You can save costs by retiring these applications.</li> <li>Retain\u200a\u2014\u200aUsually this means \u201crevisit\u201d or do nothing (for now). Maybe you aren\u2019t ready to prioritize an application that was recently upgraded or are otherwise not inclined to migrate some applications. You can retain these applications and revisit your migration strategy.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-cloud-adoption-readiness-tool-cart","title":"AWS Cloud Adoption Readiness Tool (CART)","text":"<ul> <li>Helps organizations of all sizes develop efficient and effective plans for cloud adoption and enterprise cloud migrations. This 16-question online survey and assessment report detail your cloud migration readiness across six perspectives, including business, people, process, platform, operations, and security. Once you complete a CART survey, you can provide your contact details to download a customized cloud migration assessment that charts your readiness and what you can do to improve it. This tool is designed to help organizations assess their progress with cloud adoption and identify gaps in organizational skills and processes.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-application-discovery-service","title":"AWS Application Discovery Service","text":"<ul> <li>The AWS Application Discovery Service simply helps you to plan migration projects by gathering information about your on-premises data centers, but is not a migration service. All discovered data are stored in your AWS Migration Hub.</li> <li>Application Discovery Service offers two ways of performing discovery and collecting data about your on-premises servers:</li> <li>Agentless discovery can be performed by deploying the AWS Agentless Discovery Connector (OVA file) through your VMware Center.</li> <li>Agent-based discovery can be performed by deploying the AWS Application Discovery Agent on each of your VMs and physical servers.</li> <li>The collected data is retained in encrypted format in an AWS Application Discovery Service data store. You can export this data as a CSV file and use it to estimate the Total Cost of Ownership (TCO) of running on AWS and to plan your migration to AWS. In addition, this data is also available in AWS Migration Hub, where you can migrate the discovered servers and track their progress as they get migrated to AWS.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-application-migration-service","title":"AWS Application Migration Service","text":"<ul> <li>The AWS Application Migration Service (MGN) is primarily used to migrate virtual machines only, which can be from VMware vSphere and Windows Hyper-V to your AWS cloud. The first setup step for Application Migration Service is creating the Replication Settings template. Add source servers to Application Migration Service by installing the AWS Replication Agent (also referred to as \"the Agent\") on them. The Agent can be installed on both Linux and Windows servers. After you have added all of your source servers and configured their launch settings, you are ready to launch a Test instance. Once you have finalized the testing of all of your source servers, you are ready for cutover. The cutover will migrate your source servers to the Cutover instances on AWS.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-database-migration-service-aws-dms","title":"AWS Database Migration Service (AWS DMS)","text":"<ul> <li>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.</li> <li>You can configure a local task and AWS DMS task to replicate the ongoing updates in your local database to the AWS database service.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-datasync_1","title":"AWS DataSync","text":"<ul> <li>AWS DataSync is an online data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services. DataSync can copy data between Network File System (NFS) shares, Server Message Block (SMB) shares, self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, and Amazon FSx for Windows File Server file systems.</li> <li>Main use cases for AWS DataSync:</li> <li>Data migration \u2013 Move active datasets rapidly over the network into Amazon S3, Amazon EFS, or FSx for Windows File Server.</li> <li>Archiving cold data \u2013 Move cold data stored in on-premises storage directly to durable and secure long-term storage such as Amazon S3 Glacier or S3 Glacier Deep Archive.</li> <li>Data protection \u2013 Move data into any Amazon S3 storage class, choosing the most cost-effective storage class for your needs.</li> <li>Data movement for timely in-cloud processing \u2013 Move data into or out of AWS for processing when working with systems that generate data on-premises. This approach can speed up critical hybrid cloud workflows across many industries.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-migration-hub","title":"AWS Migration Hub","text":"<ul> <li>AWS Migration Hub (Migration Hub) provides a single place to discover your existing servers, plan migrations, and track the status of each application migration. The Migration Hub provides visibility into your application portfolio and streamlines planning and tracking. You can visualize the connections and the status of the servers and databases that make up each of the applications you are migrating, regardless of which migration tool you are using. Migration Hub gives you the choice to start migrating right away and group servers while migration is underway or to first discover servers and then group them into applications.</li> <li>You can generate EC2 migration right size recommendations.</li> <li>Offers data exploration features integrated with Amazon Athena. Enabling Data exploration in AWS Migration Hub allows data from on-premises servers to be automatically stored in an Amazon S3 bucket at regular intervals, making it readily available for analysis, using services like Amazon Athena.</li> <li>Use the <code>put-resource-attributes</code> CLI command to send the collected data to AWS Migration Hub.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-schema-conversion-tool-aws-sct","title":"AWS Schema Conversion Tool (AWS SCT)","text":"<ul> <li>You can use an AWS SCT agent to extract data from your on-premises data warehouse and migrate it to AWS database service. The agent extracts your data and uploads the data to either Amazon S3 or, for large-scale migrations, an AWS Snowball Edge device. You can then use AWS SCT to copy the data to AWS database service.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-snow-family","title":"AWS Snow Family","text":""},{"location":"cloud/aws/cert-sap/#snowball-edge","title":"Snowball Edge","text":"<ul> <li>Although some revisions of USB 3.0 or USB 3.1 can support up to 5 Gbps to 10 Gbps speeds, the network interface on the Snowball Edge supports up to 100 Gbps. </li> <li>Actions to positive impact the speed transfer:</li> <li>Perform multiple write operations at one time \u2013 To do this, run each command from multiple terminal windows on a computer with a network connection to a single AWS Snowball Edge device.</li> <li>Transfer small files in batches \u2013 Each copy operation has some overhead because of encryption. To speed up the process, batch files together in a single archive. When you batch files together, they can be auto-extracted when they are imported into Amazon S3.</li> <li>Write from multiple computers \u2013 A single AWS Snowball Edge device can be connected to many computers on a network. Each computer can connect to any of the three network interfaces at once.</li> <li>Don't perform other operations on files during transfer \u2013 Renaming files during transfer, changing their metadata, or writing data to the files during a copy operation has a negative impact on transfer performance. AWS recommends that your files remain in a static state while you transfer them.</li> <li>Reduce local network use \u2013 Your AWS Snowball Edge device communicates across your local network. So you can improve data transfer speeds by reducing other local network traffic between the AWS Snowball Edge device, the switch it's connected to, and the computer that hosts your data source.</li> <li>Eliminate unnecessary hops \u2013 AWS recommends that you set up your AWS Snowball Edge device, your data source, and the computer running the terminal connection between them so that they're the only machines communicating across a single switch. Doing so can improve data transfer speeds.</li> <li>For transferring small files, AWS also recommends transferring in batches. Each copy operation has some overhead because of encryption. To speed up the process of transferring small files to your AWS Snowball Edge device, you can batch them together in a single archive. When you batch files together, they can be auto-extracted when they are imported into Amazon S3, if they were batched in one of the supported archive formats. Typically, files that are 1 MB or smaller should be included in batches. There's no hard limit on the number of files you can have in a batch, though AWS recommends that you limit your batches to about 10,000 files. Having more than 100,000 files in a batch can affect how quickly those files import into Amazon S3 after you return the device. AWS recommends that the total size of each batch be no larger than 100 GB. Batching files is a manual process, which you have to manage.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-transfer-family","title":"AWS Transfer Family","text":"<ul> <li>Regional service. Each server endpoint exists in a Single-AZ.</li> <li>Used to create FTP, FTPS and SFTP linked to S3.</li> </ul>"},{"location":"cloud/aws/cert-sap/#networking-and-content-delivery","title":"Networking and Content Delivery","text":""},{"location":"cloud/aws/cert-sap/#amazon-cloudfront","title":"Amazon CloudFront","text":"<ul> <li>Global service.</li> <li>In CloudFront you can configure your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code> to increase your cache hit ratio.</li> <li>CloudFront supports Server Name Indication (SNI) for custom SSL certificates, along with the ability to take incoming HTTP requests and redirect them to secure HTTPS requests to ensure that clients are always directed to the secure version of your website.</li> <li>If you configure CloudFront to serve HTTPS requests using SNI, CloudFront associates your alternate domain name with an IP address for each edge location. The IP address to your domain name is determined during the SSL/TLS handshake negotiation and isn\u2019t dedicated to your distribution.</li> <li>To use field-level encryption, you configure your CloudFront distribution to specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request.</li> <li>Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer.</li> <li>You can use geo restriction - also known as geoblocking - to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront web distribution.</li> <li>Origin Access Control (OAC) enables CloudFront customers to easily secure their S3 origins by permitting only designated CloudFront distributions to access their S3 buckets. Customers can now enable AWS Signature Version 4 (SigV4) on CloudFront requests to S3 buckets with the ability to set when and if CloudFront should sign requests. Additionally, customers can now use AWS KMS keys SSE-KMS when performing uploads and downloads through CloudFront.</li> <li>You can customize error responses to return a custom error page, it's configured at distribution level.</li> <li>Amazon CloudFront only accepts well-formed connections to prevent many common DDoS attacks like SYN floods and UDP reflection attacks from reaching your origin.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-direct-connect_1","title":"AWS Direct Connect","text":"<ul> <li>Global service.</li> <li>AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS.</li> <li>This dedicated connection can be partitioned into multiple virtual interfaces.</li> <li>You can use an AWS Direct Connect gateway to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in your account that are located in the same or different Regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC. Then, create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway.</li> <li>With Direct Connect Gateway, you no longer need to establish multiple BGP sessions for each VPC; this reduces your administrative workload as well as the load on your network devices.</li> <li>Public virtual interfaces are used to connect to AWS resources reachable by public IP.</li> <li>Private virtual interfaces are used to connect to your resources hosted in one VPC usin private IP.</li> <li>If you want to encrypt the traffic flowing through Direct Connect, you will need to use the public virtual interface of DX to create a VPN connection that will allow access to AWS services such as S3, EC2, and other services, with a private interface this will not work.</li> <li>Transit virtual interfaces are used to connect to your resources hosted in multiple VPCs usin private IP, for hub and spoke or multi-vpc architectures.</li> <li>The prefix list, which includes IPv4 and IPv6 addresses, is a filter that permits the same or a smaller range of CIDRs to be promoted to the Direct Connect gateway. When setting up a Direct Connect gateway, it\u2019s important to set the allowed prefixes to a range that is the same or wider than the VPC CIDR block. This ensures that all IP addresses within the VPC can be reached from the local servers.</li> </ul>"},{"location":"cloud/aws/cert-sap/#elastic-load-balancing-elb","title":"Elastic Load Balancing (ELB)","text":"<ul> <li>Regional service.</li> <li>You can either terminate the SSL on the ELB side or on the EC2 instance. If you choose the former, the X.509 certificate will only be present in the ELB and if you choose the latter, the X.509 certificate will be stored inside the EC2 instance.</li> <li>In ALB, Client Port Preservation simply preserves the source client port, allowing backend applications to see the original client-side source port.</li> <li>In ALB, Sticky sessions enable the load balancer to bind a user's session to a specific instance, ensuring that all requests from the user during the session are sent to the same instance.</li> <li>Use a NLB to create VPC Endpoint Services.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-global-accelerator_1","title":"AWS Global Accelerator","text":"<ul> <li>Global service.</li> <li>You create accelerators to improve the performance of your applications for local and global users.</li> <li>With a standard accelerator, you can improve the availability of your internet applications that are used by a global audience. With a standard accelerator, Global Accelerator directs traffic over the AWS global network to endpoints in the nearest Region to the client.</li> <li>With a custom routing accelerator, you can map one or more users to a specific destination among many destinations.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-privatelink_1","title":"AWS PrivateLink","text":"<ul> <li>Regional service. Each endpoint exists in one AZ.</li> <li>Enables private connectivity between VPCs and AWS services without exposing traffic to internet.</li> <li>The VPC endpoints can be interface nedpoints (powered by private link with a private ENI) or gateway endpoints (for services like S3 and DynamoDB) where it uses route table entries instead of ENIs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-route-53","title":"Amazon Route 53","text":"<ul> <li>Global service.</li> <li>Amazon allows you to enable Domain Name System Security Extensions (DNSSEC) signing for all existing and new public hosted zones, and enable DNSSEC validation for Amazon Route 53 Resolver.</li> <li>A Route 53 Resolver Endpoint is a customer-managed resolver consisting of one or more Elastic Network Interfaces (ENIs) deployed on your VPC. Resolver Endpoints are classified into two types inboudn endpoint and outbound endpoint.</li> <li>Inbound endpoint allows DNS queries to your VPC from your on-premises network or another VPC.</li> <li>Outbound endpoint allows DNS queries from your VPC to your on-premises network or another VPC.</li> <li>To use your custom domain name with Global Accelerator when you use Route 53 as your DNS service, you create an alias record that points your custom domain name to the DNS name assigned to your accelerator.</li> <li>Route 53 is designed to propagate updates you make to your DNS records to its worldwide network of authoritative DNS servers within 60 seconds under normal conditions.</li> <li>You can use the Amazon Route 53 console to associate more VPCs with a private hosted zone if you created the hosted zone and the VPCs by using the same AWS account. Additionally, you can associate a VPC from one account with a private hosted zone in a different account. Using the account that created the hosted zone, authorize the association of the VPC with the private hosted zone. After associating the VPC is recommended to delete the authorization.</li> <li>A hosted zone is a container for records, and records contain information about how you want to route traffic for a specific domain and its subdomains. A hosted zone and the corresponding domain have the same name.</li> <li>Public hosted zones contain records that specify how you want to route traffic on the internet.</li> <li>Private hosted zones contain records that specify how you want to route traffic in an Amazon VPC. Your VPC has attributes that determine whether your EC2 instance receives public DNS hostnames (<code>enableDnsHostnames</code>), and whether DNS resolution through the Amazon DNS server is supported (<code>enableDnsSupport</code>).</li> <li>You can use Route 53 health checks to configure active-active and active-passive failover configurations. You configure active-active failover using any routing policy (or combination of routing policies) other than failover, and you configure active-passive failover using the failover routing policy.</li> <li>In Route 53, an alias record is a Route 53 extension to DNS. It's similar to a CNAME record, but you can create an alias record both for the root domain, such as example.com, and for subdomains, such as www.example.com. (You can create CNAME records only for subdomains). For EC2 instances, always use a Type A Record without an Alias. For ELB, Cloudfront, and S3, always use a Type A Record with an Alias, and finally, for RDS, always use the CNAME Record with no Alias.</li> <li>Use an A record to point to an IPv4, AAAA to point to an IPv6 and an Alias record to point to another domain name.</li> <li>Route 53 policies:</li> <li>Simple routing policy \u2013 Use for a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website.</li> <li>Failover routing policy \u2013 Use when you want to configure active-passive failover.</li> <li>Geolocation routing policy \u2013 Use when you want to route traffic based on the location of your users.</li> <li>Geoproximity routing policy \u2013 Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.</li> <li>Latency routing policy \u2013 Use when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the best latency.</li> <li>Multivalue answer routing policy \u2013 Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.</li> <li>Weighted routing policy \u2013 Use to route traffic to multiple resources in proportions that you specify.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-transit-gateway_1","title":"AWS Transit Gateway","text":"<ul> <li>Regional service.</li> <li>A transit gateway enables you to attach VPCs and VPN connections in the same Region and route traffic between them. A transit gateway works across AWS accounts, and you can use AWS RAM to share your transit gateway with other accounts. After you share a transit gateway with another AWS account, the account owner can attach their VPCs to your transit gateway. A user from either account can delete the attachment at any time.</li> <li>You can enable multicast on a transit gateway, and then create a transit gateway multicast domain that allows multicast traffic to be sent from your multicast source to multicast group members over VPC attachments that you associate with the domain.</li> <li>Each VPC or VPN attachment is associated with a single route table. That route table decides the next hop for the traffic coming from that resource attachment. A route table inside the transit gateway allows for both IPv4 or IPv6 CIDRs and targets. The targets are VPCs and VPN connections. When you attach a VPC or create a VPN connection on a transit gateway, the attachment is associated with the default route table of the transit gateway.</li> <li>You can create additional route tables inside the transit gateway, and change the VPC or VPN association to these route tables. This enables you to segment your network. For example, you can associate development VPCs with one route table and production VPCs with a different route table. This enables you to create isolated networks inside a transit gateway similar to virtual routing and forwarding (VRFs) in traditional networks.</li> <li>AWS Transit Gateway Network Manager is used to monitor network activity into a single dashboard.</li> <li>When you create attachments to a transit gateway, you can configure automatic acceptance for shared attachments, which means that the attachments are automatically accepted and the connection is established.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-virtual-private-cloud-amazon-vpc","title":"Amazon Virtual Private Cloud (Amazon VPC)","text":"<ul> <li>Regional service. Subnets are Single-AZ. Security groups are regional. Elastic IP address are regional and ENI are Single-AZ.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-vpn","title":"AWS VPN","text":"<ul> <li>Regional service, can be Single-AZ and Multi-AZ (recommended).</li> <li>Secure Sockets Layer (SSL) VPN is an emerging technology that provides remote-access VPN capability, using the SSL function that is already built into a modern web browser. SSL VPN allows users from any Internet-enabled location to launch a web browser to establish remote-access VPN connections, thus promising productivity enhancements and improved availability, as well as further IT cost reduction for VPN client software and support.</li> </ul>"},{"location":"cloud/aws/cert-sap/#security-identity-and-compliance","title":"Security, Identity, and Compliance","text":""},{"location":"cloud/aws/cert-sap/#aws-artifact","title":"AWS Artifact","text":"<ul> <li>Global service.</li> <li>Self-service portal that provides access to on-demand AWS compliance documentation. This service offers a comprehensive repository of AWS's security and compliance reports, including certifications, attestations, and agreements. These documents are essential for customers in highly regulated industries, such as healthcare, finance, and biotechnology, as they prove AWS's adherence to industry standards and regulatory requirements. AWS Artifact helps organizations ensure that their data hosted on AWS complies with frameworks like HIPAA, GDPR, and PCI DSS.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-audit-manager","title":"AWS Audit Manager","text":"<ul> <li>Regional service.</li> <li>AWS Audit Manager is used to map compliance requirements to AWS usage data with prebuilt and custom frameworks and automated evidence collection.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-certificate-manager-acm","title":"AWS Certificate Manager (ACM)","text":"<ul> <li>Global service for CloudFront, Regional service for other services.</li> <li>Used to store certificates.</li> <li>With AWS Certificate Manager, you can generate public or private SSL/TLS certificates that you can use to secure your site.</li> <li>Public SSL/TLS certificates provisioned through AWS Certificate Manager are free. You pay only for the AWS resources that you create to run your application.</li> <li>Public certificates generated from ACM can be used on Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway but not directly on EC2 instances, unlike private certificates.</li> <li>For private certificates, the ACM Private Certificate Authority (CA) is priced along two dimensions: (1) You pay a monthly fee for the operation of each private CA until you delete it and (2) you pay for the private certificates you issue each month.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-cloudhsm","title":"AWS CloudHSM","text":"<ul> <li>Regional service. Supports single-AZ and multi-AZ (recommended).</li> <li>AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.</li> <li>You can use AWS CloudHSM to offload SSL/TLS processing for your web servers. Using CloudHSM for this processing reduces the burden on your web server and provides extra security by storing your web server\u2019s private key in CloudHSM. Secure Sockets Layer (SSL) and Transport Layer Security (TLS) are used to confirm the identity of web servers and establish secure HTTPS connections over the Internet.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-cognito","title":"Amazon Cognito","text":"<ul> <li>Regional service.</li> <li>Idp.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-detective","title":"Amazon Detective","text":"<ul> <li>Regional service.</li> <li>Amazon Detective automatically collects log data from your AWS resources and uses machine learning (ML), statistical analysis, and graph theory to build a dataset that you can use to conduct more efficient security investigations.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-directory-service","title":"AWS Directory Service","text":"<ul> <li>Regional service.</li> <li>AWS Directory Service helps you to set up and run a standalone AWS Managed Microsoft AD directory hosted in the AWS Cloud. You can also use AWS Directory Service to connect your AWS resources with an existing on-premises Microsoft Active Directory. To configure AWS Directory Service to work with your on-premises Active Directory, you must first set up trust relationships to extend authentication from on-premises to the cloud.</li> <li>AWS Directory Service supports several directory types:</li> <li>Simple AD allows you to easily launch a new Active Directory forest in the AWS cloud.</li> <li>Microsoft AD allows you to run an existing on-premises AD in the AWS cloud.</li> <li>AD Connector allows you to connect your AWS resources to an existing on-premises AD.</li> <li>Amazon Cognito is an AWS service that helps you manage user sign-up, authentication, and account management for your mobile and web applications.</li> <li>Using AWS Directory Service in conjunction with AD Connector allows for seamless single sign-on for users.</li> <li>The AWS Client VPN endpoint creates a secure IPsec VPN tunnel between the on-premises network and the VPC. Enabling Multi-Factor Authentication (MFA) on the Active Directory (AD) connector ensures that only authorized and authenticated users can access resources within the Virtual Private Cloud (VPC).</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-firewall-manager_1","title":"AWS Firewall Manager","text":"<ul> <li>Regional service.</li> <li>AWS Firewall Manager is primarily used to manage your Firewall across multiple AWS accounts under your AWS Organizations.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-guardduty_1","title":"Amazon GuardDuty","text":"<ul> <li>Regional service.</li> <li>Amazon GuardDuty is a security monitoring service that analyzes and processes certain types of AWS logs, such as AWS CloudTrail data event logs for Amazon S3 and CloudTrail management event logs.</li> <li>It uses threat intelligence feeds, such as lists of malicious IP addresses and domains, and machine learning to identify unexpected and potentially unauthorized and malicious activity within your AWS environment.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-iam-identity-center","title":"AWS IAM Identity Center","text":"<ul> <li>Global service.</li> <li>AWS IAM Identity Center (successor to AWS SSO) expands the capabilities of AWS Identity and Access Management (IAM) to provide a central place that brings together administration of users and their access to AWS accounts and cloud applications. Has integration with Microsoft AD through the AWS Directory Service.</li> <li>AWS IAM Identity Center supports only SAML 2.0\u2013based applications</li> <li>IAM Identity Center supports automatic provisioning (synchronization) of user and group information from your identity provider (IdP) into IAM Identity Center using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. When you configure SCIM synchronization, you create a mapping of your identity provider (IdP) user attributes to the named attributes in IAM Identity Center. This causes the expected attributes to match between IAM Identity Center and your IdP.</li> <li>AWS IAM Identity  Center supports single sign-on to business applications through web browsers only.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-identity-and-access-management-iam","title":"AWS Identity and Access Management (IAM)","text":"<ul> <li>Global service.</li> <li>A trust policy is attached to a role and defines who can assume the role.</li> <li>A permissions policy is attached to a role and defines what the role can do.</li> <li>A permissions boundary is attached to a role/user and limits the permissions, even if policies allow more.</li> <li>Supports federated identity with SAML 2.0-compatible IdP.</li> <li>For other scenarios like LDAP you need to create an identiy broker that authenticates to LDAP and then calls STS to assume a role.</li> <li>For administrators you can use the <code>AdministratorAccess</code> policiy.</li> <li>For developer power users you can use the <code>PoweruserAccess</code> policiy.</li> <li>IAM Access Analyzer can analyze your CloudTrail events to identify actions and services used by an IAM entity (user or role).</li> <li>You can store SSL certificates in IAM but is preffered to use AWS Certificate Manager (ACM).</li> <li>IAM trust policies for IAM roles specify which entities, like users, roles, or services, can assume a role and request temporary credentials, whether across multiple accounts or within a single one.</li> <li>If you need to interact with AWS from GithubActions, instead of using hardcoded credentials, you can configure an IAM OpenID Connect (OIDC) Identity Provider (IdP) in AWS IAM, associated with GitHub. Create an IAM role with a trust policy for the sts:AssumeRoleWithWebIdentity AWS STS API calls from the GitHub OIDC IdP. Modify the GitHub Actions CI/CD pipeline to use this IAM role for its deployment processes. This uses <code>aws-actions/configure-aws-credentials@v1</code> with <code>role-to-assume</code>, <code>role-session-name</code> and <code>aws-region</code>. You can define a role with the condition to only allow the sub to match the repo name to avoid that other CIs assume the same role.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-inspector","title":"Amazon Inspector","text":"<ul> <li>Regional service.</li> <li>An automated security assessment service that helps improve the security and compliance of applications deployed on AWS.</li> <li>Amazon Inspector is used as an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities.</li> <li>You can use Amazon Inspector to conduct a detailed scan for CVE in your fleet of EC2 instances.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-key-management-service-aws-kms","title":"AWS Key Management Service (AWS KMS)","text":"<ul> <li>Regional service.</li> <li>Key management service to be used by other services and applications.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-macie","title":"Amazon Macie","text":"<ul> <li>Regional service.</li> <li>Macie automates the discovery of sensitive data, such as personally identifiable information (PII), personal health information (PHI) and financial data, to provide you with a better understanding of the types of data in Amazon S3.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-network-firewall_1","title":"AWS Network Firewall","text":"<ul> <li>Regional service.</li> <li>Deploys network security access for your VPC.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-resource-access-manager-aws-ram","title":"AWS Resource Access Manager (AWS RAM)","text":"<ul> <li>Enables you to share specified AWS resources that you own with other AWS accounts.</li> <li>Used to share AWS resources such as transit gateways, subnets, AWS License Manager license configurations, and Amazon Route 53 Resolver rules.</li> <li>To enable trusted access with AWS Organizations, from the AWS RAM CLI, use the <code>enable-sharing-with-aws-organizations</code> command.</li> <li>AWS AWS RAM does not support direct sharing of Lambda functions between accounts.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>Regional service. - This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.</p>"},{"location":"cloud/aws/cert-sap/#aws-security-hub","title":"AWS Security Hub","text":"<ul> <li>Regional service.</li> <li>AWS Organizations allows central management of multiple AWS accounts. It supports service delegation, enabling specific accounts to act as delegated administrators for various AWS services, including Security Hub. This helps streamline security operations by consolidating security management into a single account. A delegated administrator account is a member account within an AWS Organization that has been assigned the authority to manage specific services on behalf of the management account. For AWS Security Hub, the delegated administrator can:</li> <li>Enable Security Hub across all member accounts.</li> <li>Aggregate findings from all accounts into the delegated administrator account.</li> <li>Provide a unified security view, reducing the need for individual account management and aligning with AWS best practices.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-security-token-service-aws-sts","title":"AWS Security Token Service (AWS STS)","text":"<ul> <li>At a high level, the external ID is a piece of data that can be passed to the AssumeRole API of the Security Token Service (STS). You can then use the external ID in the condition element in a role\u2019s trust policy, allowing the role to be assumed only when a certain value is present in the external ID. This prevents transitive asume role from another account that can asume roles from the allowed account.</li> <li>A cross-account role should be created in the destination accounts, not origin account.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-shield_1","title":"AWS Shield","text":"<ul> <li>AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. Shield Advanced provides expanded DDoS attack protection.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-waf_1","title":"AWS WAF","text":"<ul> <li>AWS WAF is a web application firewall that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. With AWS Config, you can track changes to WAF web access control lists (web ACLs).</li> <li>It is easy to deploy and protect applications deployed on either Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts all your origin servers (but not a Network Load Balancer), or Amazon Rest API Gateway for your APIs, also AppSync GraphQL API, Cognito user pool, App Runner service, Verified Access instance and Amplify.</li> <li>Customers who have the technical expertise to manage their own monitoring and mitigation of application layer attacks can use AWS Shield together with AWS WAF rules to create a comprehensive DDoS attack mitigation strategy.</li> </ul>"},{"location":"cloud/aws/cert-sap/#storage_1","title":"Storage","text":""},{"location":"cloud/aws/cert-sap/#aws-backup_1","title":"AWS Backup","text":"<ul> <li>Regional service.</li> <li>A fully-managed service that makes it easy to centralize and automate data protection across AWS services, in the cloud and on-premises. Using this service, you can configure backup policies and monitor activity for your AWS resources in one place. It allows you to automate and consolidate backup tasks that were previously performed service-by-service and remove the need to create custom scripts and manual processes.</li> <li>Supports continuous backups and point-in-time recovery (PITR) in addition to snapshot backups. With continuous backups, you can restore your AWS Backup-supported resource by rewinding it back to a specific time that you choose within 1 second of precision (going back a maximum of 35 days). Continuous backup works by first creating a full backup of your resource and then constantly backing up your resource\u2019s transaction logs. PITR restore works by accessing your full backup and replaying the transaction log to the time that you tell AWS Backup to recover.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-elastic-block-store-amazon-ebs","title":"Amazon Elastic Block Store (Amazon EBS)","text":"<ul> <li>Tied to one AZ. The EBS snapshots are regional.</li> <li>Block storage.</li> <li>Can attach multiple volumes to one instance.</li> <li>We can de-attach from one instance and attach to another.</li> <li>Depending on the instance type and EBS volume, you can have the same volume attached to multiple instances (EBS Multi-Attach).</li> <li>Amazon Data Lifecycle Manager (DLM) for EBS Snapshots provides a simple, automated way to back up data stored on Amazon EBS volumes. You can define backup and retention schedules for EBS snapshots by creating lifecycle policies based on tags. With this feature, you no longer have to rely on custom scripts to create and manage your backups.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-elastic-disaster-recovery","title":"AWS Elastic Disaster Recovery","text":"<ul> <li>Recovery time objective (RTO) is the time it takes after a disruption to restore a business process to its service level, as defined by the operational level agreement (OLA).</li> <li>Recovery point objective (RPO) is the acceptable amount of data loss measured in time.</li> <li>Backup and restore (RPO in hours, RTO in 24 hours or less): Back up your data and applications using point-in-time backups into the DR Region. Restore this data when necessary to recover from a disaster.</li> <li>Pilot light (RPO in minutes, RTO in hours): Replicate your data from one region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup such as databases and object storage are always on. Other elements such as application servers are loaded with application code and configurations, but are switched off and are only used during testing or when Disaster Recovery failover is invoked.</li> <li>Warm standby (RPO in seconds, RTO in minutes): Maintain a scaled-down but fully functional version of your workload always running in the DR Region. Business-critical systems are fully duplicated and are always on, but with a scaled down fleet. When the time comes for recovery, the system is scaled up quickly to handle the production load.</li> <li>Multi-region (multi-site) active-active (RPO near zero, RTO potentially zero): Your workload is deployed to, and actively serving traffic from, multiple AWS Regions. This strategy requires you to synchronize data across Regions.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-elastic-file-system-amazon-efs","title":"Amazon Elastic File System (Amazon EFS)","text":"<ul> <li>Regional service.</li> <li>File storage.</li> <li>Operates as a Regional service.</li> <li>Automatically grows and shrinks as you add and remove files.</li> <li>Can grow to petabyte scale.</li> <li>You can connect tens, hundreds, and even thousands of compute instances to an Amazon EFS file system at the same time.</li> <li>Supports elastic, bursting and provisioned throughput.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-fsx-for-all-types","title":"Amazon FSx (for all types)","text":"<ul> <li>Regional service with Multi-AZ (except Lustre).</li> </ul>"},{"location":"cloud/aws/cert-sap/#fsx-for-lustre_1","title":"FSx for Lustre","text":"<ul> <li>For high performance computing (HPC).</li> </ul>"},{"location":"cloud/aws/cert-sap/#fsx-for-netapp-ontap_1","title":"FSx for NetApp ONTAP","text":"<ul> <li>Provides rich data management features and flexible shared file storage that are broadly accessible from Linux, Windows, and macOS compute instances running in AWS or on premises.</li> <li>Sub-millisecond latencies.</li> <li>These block services use NetApp's application programming interface (API) calls and management interface.</li> </ul>"},{"location":"cloud/aws/cert-sap/#fsx-for-openzfs_1","title":"FSx for OpenZFS","text":"<ul> <li>Implementation of the Open Zettabyte File System (ZFS).</li> <li>Supports NFS and SMB protocols for a wide range of application implementations. </li> <li>Delivers leading performance for latency-sensitive and small-file workloads with popular NAS data management capabilities (snapshots, and cloning), at a lower price than commercially licensed alternatives.</li> </ul>"},{"location":"cloud/aws/cert-sap/#fsx-for-windows-file-server_1","title":"FSx for Windows File Server","text":"<ul> <li>Provides file storage that is accessible over the Service Message Block (SMB) protocol and has the ability to serve as a drop-in replacement for existing Windows file server deployments.</li> <li>Built on Windows Server.</li> <li>Don't support dynamic file size.</li> <li>Using the <code>update-file-system</code> command, you can use AWS SDK or CLI to programmatically increase the size of the FSx file system.</li> <li>Can be Single-AZ or Multi-AZ.</li> <li>You can use Use AWS DataSync to copy data to a new Amazon FSx file system that uses a Multi-AZ deployment type to migrate from a Single-AZ. After copying, point the applications to use the new Amazon FSx share. It is not possible to update the deployment type of the FSx file system once it is created.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-simple-storage-service-amazon-s3","title":"Amazon Simple Storage Service (Amazon S3)","text":"<ul> <li>Global service but data is stored regional.</li> <li>Amazon S3 Cross-Region Replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions.</li> <li>A bucket owner can configure a bucket to be a Requester Pays bucket. With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. The bucket owner always pays the cost of storing data.</li> <li>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront\u2019s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</li> <li>When you enable versioning the existing object have the version value to null. -SSE-S3 uses strong multi-factor encryption. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Uses AES-256 to encrypt your data.</li> <li>With SSE-KMS, you can also enable S3 Bucket Keys to decrease request traffic from Amazon S3 to AWS KMS and reduce the cost of encryption.</li> <li>You can use <code>sync</code> comand to synchronize the data in your on-premises server and in AWS. Executing another <code>sync</code> only uploads the \"delta\" or in other words, the \"difference\" in the subset.</li> <li>Glacier doesn't have a built-in search function to help you retrieve the data, you can use another service like DynamoDB to associate archive ID with the search metadata.</li> <li>To grant access to an AWS KMS-encrypted bucket in Account A to a user in Account B, you must have these permissions in place:</li> <li>The bucket policy in Account A must grant access to Account B.</li> <li>The AWS KMS key policy in Account A must grant access to the user in Account B.</li> <li>The IAM policy in Account B must grant user access to the bucket and the AWS KMS key in Account A.</li> <li>Amazon S3 Transfer Acceleration (S3TA) can speed up content transfers to and from Amazon S3 by as much as 50% - 500% for long-distance transfer of larger objects.</li> <li>S3 Replication Time Control (S3 RTC) helps you meet compliance or business requirements for data replication and provides visibility into Amazon S3 replication times. S3 RTC replicates most objects that you upload to Amazon S3 in seconds and 99.99 percent of those objects within 15 minutes.</li> <li>Amazon S3 Storage Lens is a cloud-storage analytics feature that you can use to gain organization-wide visibility into object-storage usage and activity. You can use S3 Storage Lens metrics to generate summary insights, such as finding out how much storage you have across your entire organization or which are the fastest-growing buckets and prefixes.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-s3-glacier","title":"Amazon S3 Glacier","text":"<ul> <li>Global service but data is stored regional.</li> <li>Archival storage solution.</li> <li>You can use instant retrieval, flexible retriebal (retrieval time of 1-5 min expedited, or 5-12 h free) and deep archive (retrieval time of 12 h).</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-storage-gateway_1","title":"AWS Storage Gateway","text":"<ul> <li>Regional service.</li> </ul>"},{"location":"cloud/aws/cert-sap/#s3-file-gateway","title":"S3 File gateway","text":"<ul> <li>Amazon S3 File Gateway presents a file-based interface to Amazon S3, which appears as a network file share. It enables you to store and retrieve Amazon S3 objects through standard file storage protocols. File Gateway allows your existing file-based applications or devices to use secure and durable cloud storage without needing to be modified. With S3 File Gateway, your configured S3 buckets will be available as Network File System (NFS) mount points or Server Message Block (SMB) file shares. Your applications read and write files and directories over NFS or SMB, interfacing to the gateway as a file server. In turn, the gateway translates these file operations into object requests on your S3 buckets. Your most recently used data is cached on the gateway for low-latency access, and data transfer between your data center and AWS is fully managed and optimized by the gateway. Once in S3, you can access the objects directly or manage them using S3 features such as S3 Lifecycle Policies and S3 Cross-Region Replication (CRR). You can run S3 File Gateway on-premises or in EC2.</li> </ul>"},{"location":"cloud/aws/cert-sap/#fsx-file-gateway","title":"FSx File gateway","text":"<ul> <li>Amazon FSx File Gateway optimizes on-premises access to Windows file shares on Amazon FSx, making it easy for users to access FSx for Windows File Server data with low latency and conserving shared bandwidth. Users benefit from a local cache of frequently used data that they can access, enabling faster performance and reduced data transfer traffic. File system operations, such as reading and writing files, are all performed against the local cache, while Amazon FSx File Gateway synchronizes changed data to FSx for Windows File Server in the background. With these capabilities, you can consolidate all of your on-premises file share data in AWS on FSx for Windows File Server and benefit from protected, resilient, fully managed file systems.</li> </ul>"},{"location":"cloud/aws/cert-sap/#tape-gateway_1","title":"Tape gateway","text":"<ul> <li>Tape Gateway is a cloud-based Virtual Tape Library (VTL). It presents your backup application with a VTL interface, consisting of a media changer and tape drives. You can create virtual tapes in your virtual tape library using the AWS Management Console. Your backup application can read data from or write data to virtual tapes by mounting them to virtual tape drives using the virtual media changer. Virtual tapes are discovered by your backup application using its standard media inventory procedure. Virtual tapes are available for immediate access and are backed by Amazon S3. You can also archive tapes. Archived tapes are stored in Amazon S3 Glacier or Amazon S3 Glacier Deep Archive.</li> </ul>"},{"location":"cloud/aws/cert-sap/#volume-gateway_1","title":"Volume gateway","text":"<ul> <li>Volume Gateway provides an iSCSI target, which enables you to create block storage volumes and mount them as iSCSI devices from your on-premises or EC2 application servers. The Volume Gateway runs in either a cached or stored mode.</li> <li>In the cached mode, your primary data is written to S3, while retaining your frequently accessed data locally in a cache for low-latency access.</li> <li>In the stored mode, your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS.</li> <li>In either mode, you can take point-in-time snapshots of your volumes, which are stored as Amazon EBS Snapshots in AWS, enabling you to make space-efficient versioned copies of your volumes for data protection, recovery, migration and various other copy data needs.</li> </ul>"},{"location":"cloud/aws/cert-sap/#opscenter","title":"OpsCenter","text":"<ul> <li>OpsCenter is oriented towards incident management.</li> </ul>"},{"location":"cloud/aws/cert-sap/#data-pipeline","title":"Data Pipeline","text":"<ul> <li>AWS Data Pipeline is a web service that you can use to automate the movement and transformation of data. With AWS Data Pipeline, you can define data-driven workflows, so that tasks can be dependent on the successful completion of previous tasks. You define the parameters of your data transformations and AWS Data Pipeline enforces the logic that you've set up.</li> </ul>"},{"location":"cloud/aws/cert-sap/#design-patterns","title":"Design Patterns","text":"<ul> <li>Token Vending Machine: Obtain STS tokens from user credentials. These machines provide a mechanism for obtaining tokens while abstracting the complexity of how these tokens are generated. Developers can use a TVM without having detailed knowledge of how it produces tokens.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-connect","title":"Amazon Connect","text":"<ul> <li>Amazon Connect provides a seamless omnichannel experience through a single unified contact center for voice and chat. Contact center agents and managers don\u2019t have to learn multiple tools because Amazon Connect has the same contact routing, queuing, analytics, and management tools in a single UI across voice, web chat, and mobile chat.</li> <li>Contact flows define the experience your customers have when they interact with your contact center. These are similar in concept to Interactive Voice Response (IVR). Contact flows are comprised of blocks, with each block defining a step or interaction in your contact center. For example, there are blocks to play a prompt, get input from a customer, branch based on customer input, or invoke an AWS Lambda function or an Amazon Lex bot.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-ground-station","title":"AWS Ground Station","text":"<ul> <li>AWS Ground Station is a fully managed service that lets you control satellite communications, process data, and scale your operations without having to worry about building or managing your own ground station infrastructure.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-elemental-mediaconnect","title":"AWS Elemental MediaConnect","text":"<ul> <li>AWS Elemental MediaConnect is just a high-quality transport service for live video.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-elemental-mediaconvert","title":"AWS Elemental MediaConvert","text":"<ul> <li>AWS Elemental MediaConvert is a file-based video transcoding service with broadcast-grade features. It provides a comprehensive suite of advanced transcoding features with on-demand rates. It allows you to easily create video-on-demand (VOD) content for broadcast and multiscreen delivery at scale. The service combines advanced video and audio capabilities with a simple web services interface and pay-as-you-go pricing. With AWS Elemental MediaConvert, you can focus on delivering compelling media experiences without having to worry about the complexity of building and operating your own video processing infrastructure. MediaConvert is optimized to improve scalability, which allows you to process more files in parallel.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-identity-federation-with-saml-20","title":"AWS Identity Federation with SAML 2.0","text":"<ul> <li>Enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS API operations without having to create an IAM user for everyone in your organization. By using SAML, you can simplify the process of configuring federation with AWS because you can use the IdP's service instead of writing custom identity proxy code.</li> <li>Federated access allows a user or application in your organization to call AWS API operations. You use a SAML assertion (as part of the authentication response) that is generated in your organization to get temporary security credentials.</li> <li>Web-based single sign-on (SSO) to the AWS Management Console from your organization. Users can sign in to a portal in your organization hosted by a SAML 2.0\u2013compatible IdP, select an option to go to AWS, and be redirected to the console without having to provide additional sign-in information. You can use a third-party SAML IdP to establish SSO access to the console or you can create a custom IdP to enable console access for your external users.</li> <li>Before you can use SAML 2.0-based federation, you must configure your organization's IdP and your AWS account to trust each other. Inside your organization, you must have an IdP that supports SAML 2.0, like Microsoft Active Directory Federation Service (AD FS, part of Windows Server), Shibboleth, or another compatible SAML 2.0 provider. In your organization's IdP, you define assertions that map users or groups in your organization to the IAM roles. Note that different users and groups in your organization might map to different IAM roles. The exact steps for performing the mapping depend on what IdP you're using.</li> <li>The role or roles that you create in IAM define what federated users from your organization are allowed to do in AWS. When you create the trust policy for the role, you specify the SAML provider that you created earlier as the Principal. You can additionally scope the trust policy with a Condition element to allow only users that match certain SAML attributes to access the role.</li> </ul>"},{"location":"cloud/aws/cert-sap/#permissions_1","title":"Permissions","text":"<ul> <li>Cross-account access with a resource-based policy has some advantages over a role. With a resource that is accessed through a resource-based policy, the user still works in the trusted account and does not have to give up his or her user permissions in place of the role permissions. In other words, the user continues to have access to resources in the trusted account at the same time as he or she has access to the resource in the trusting account. This is useful for tasks such as copying information to or from the shared resource in the other account.</li> </ul>"},{"location":"cloud/aws/cert-sap/#amazon-workdocs","title":"Amazon WorkDocs","text":"<ul> <li>Amazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content, and because it\u2019s stored centrally on AWS, access it from anywhere on any device.</li> <li>Amazon WorkDocs lets you integrate with your existing systems, and offers a rich API so that you can develop your own content-rich applications.</li> <li>Amazon WorkDocs Content Manager is a high-level utility tool that uploads content or downloads it from an Amazon WorkDocs site. It can be used for both administrative and user applications. For user applications, a developer must construct the Amazon WorkDocs Content Manager with anonymous AWS credentials and an authentication token. For administrative applications, the Amazon WorkDocs client must be initialized with AWS Identity and Access Management (IAM) credentials. In addition, the authentication token must be omitted in subsequent API calls.</li> </ul>"},{"location":"cloud/aws/cert-sap/#vm-importexport","title":"VM Import/Export","text":"<ul> <li>VM Import/Export enables you to import virtual machine (VM) images from your existing virtualization environment to Amazon EC2 and then export them back. This enables you to migrate applications and workloads to Amazon EC2, copy your VM image catalog to Amazon EC2, or create a repository of VM images for backup and disaster recovery.</li> </ul>"},{"location":"cloud/aws/cert-sap/#migration-evaluator","title":"Migration Evaluator","text":"<ul> <li>The Migration Evaluator Agentless Collector is a tool provided by AWS that can be deployed on a Windows Amazon EC2 instance. This tool is designed to collect data from on-premises servers via the Simple Network Management Protocol (SNMP). The types of data collected by the Migration Evaluator Agentless Collector include server configuration, utilization, annual costs to operate, eligibility for bring-your-own-license, and hundreds of other parameters. This data is crucial for understanding the current state of the on-premises servers and planning for their migration to the AWS cloud.</li> <li>Once the data is collected, it is then analyzed by the Migration Evaluator to generate a comprehensive Total Cost of Ownership (TCO) analysis. The TCO analysis provides a clear baseline of what your organization is running today and projects AWS costs based on measured on-premises provisioning and utilization. This analysis is essential for understanding the financial impact of migrating the on-premises servers to the AWS cloud. The Migration Evaluator service analyzes an enterprise\u2019s compute footprint, including server configuration, utilization, annual costs to operate, eligibility for bring-your-own-license, and hundreds of other parameters. This allows the company to make informed decisions about using AWS.</li> <li>The Migration Evaluator Collector facilitates the gathering of data from on-premises environments. It collects detailed information on infrastructure usage and resource consumption, which can then be imported into the Migration Evaluator. Once the data is collected, the tool analyzes it. Produces a Quick Insights report, highlighting potential cost savings and providing a side-by-side comparison of the on-premises environment versus the proposed AWS environment. This analysis helps businesses make informed decisions about migration, budgeting, and resource planning, ensuring they understand the financial benefits and trade-offs of moving to the cloud.</li> <li>Also offers insights into optimization opportunities, assisting companies in refining their cloud strategy. It simplifies the complex process of evaluating cloud costs, enabling businesses to make more informed choices about their infrastructure and future cloud investments.</li> </ul>"},{"location":"cloud/aws/cert-sap/#aws-resource-groups","title":"AWS Resource Groups","text":"<ul> <li>AWS Resource Groups are used to manage and automate tasks on a large collection of AWS resources at once.</li> </ul>"},{"location":"cloud/aws/cert-sap/#networking_2","title":"Networking","text":"<ul> <li>Each 10 Mbsp is aprox 0.1 TB per day. </li> <li>If you want to filter by url, instead of using security groups or NACL, you use a web proxy. A forward proxy server acts as an intermediary for requests from internal users and servers, often caching content to speed up subsequent requests. Companies usually implement proxy solutions to provide URL and web content filtering, IDS/IPS, data loss prevention, monitoring, and advanced threat protection. AWS customers often use a VPN or AWS Direct Connect connection to leverage existing corporate proxy server infrastructure, or build a forward proxy farm on AWS using software such as Squid proxy servers with internal Elastic Load Balancing (ELB). You can limit outbound web connections from your VPC to the internet, using a web proxy (such as a squid server) with custom domain whitelists or DNS content filtering services. The solution is scalable, highly available, and deploys in a fully automated way.</li> </ul>"},{"location":"cloud/azure/azure/","title":"Azure","text":""},{"location":"cloud/azure/azure/#links","title":"Links","text":"<p>AZ-305 exam preparation</p>"},{"location":"databases/mongo/","title":"Upsert example","text":"<pre><code>{\n    \"_id\": 1,\n    \"sensor\": 5,\n    \"date\": Date(\"2020-10-10\"),\n    \"valcount\": 2,\n    \"total\": 80,\n    \"readings\": [\n        {\"v\": 50, \"t\": 23},\n        {\"v\": 30, \"t\": 24}\n    ]\n}\n</code></pre> <pre><code>db.iot.updateOne(\n    {\n        \"sensor\": 5,\n        \"date\": var.date,\n        \"valcount\": { $lt : 48 }\n    },\n    {\n        \"$push\": { \"readings\": {\"v\": var.v, \"t\", var.t}},\n        \"$inc\": {\"valcount\": 1, \"total\": var.v}\n    },\n    { upsert: true}\n)\n</code></pre>"},{"location":"databases/mongo/#workload-description","title":"Workload description","text":"<ul> <li>Query (Anomalies in inventory)</li> <li>Quantification (X/day Y/sec)</li> <li>Qualification (&lt;1s, slate data is fine, critical write)</li> </ul> <p>Details of critical operations</p> <ul> <li>Description</li> <li>Type (R/W)</li> <li>Frequency</li> <li>Size</li> <li>Consistency</li> <li>Latency</li> <li>Durability</li> <li>Life/Duration</li> <li>Security</li> </ul>"},{"location":"databases/mongo/#schema-design-patterns","title":"Schema design patterns","text":""},{"location":"databases/mongo/#computed-pattern","title":"Computed pattern","text":"<p>Sum all views of all screenings of movie into movie document</p>"},{"location":"databases/mongo/#bucket-pattern","title":"Bucket pattern","text":"<p>Used in IoT</p> <p>One document per device per day/week</p>"},{"location":"databases/mongo/#extended-reference","title":"Extended reference","text":"<p>Copy required fields from one element to the other (example shipping address)</p>"},{"location":"databases/mongo/#change-streams","title":"Change streams","text":""},{"location":"databases/mongo/#indexes","title":"Indexes","text":"<p>Order fields by equally, sort, range</p>"},{"location":"databases/mongo/#patterns","title":"Patterns","text":""},{"location":"databases/mongo/#attribute-pattern-polymorphic-pattern","title":"Attribute pattern / Polymorphic pattern","text":"<p>Move all diferent attributes but similar and want to search on multiple to a spec array of key-value objects, then create a index on that key-value pair</p>"},{"location":"databases/mongo/#extended-reference-pattern","title":"Extended reference pattern","text":"<p>Copy required fields from one element to the other (example shipping address)</p>"},{"location":"databases/mongo/#subset-pattern","title":"Subset pattern","text":"<p>Move some fields or part of that fields (list, keep top x) to other documents</p>"},{"location":"databases/mongo/#computed-pattern_1","title":"Computed pattern","text":"<p>Sum all views of all screenings of movie into movie document</p>"},{"location":"databases/mongo/#bucket-pattern_1","title":"Bucket pattern","text":"<p>One document per device per day/hour with all readings (upser use case)</p>"},{"location":"databases/mongo/#schema-version-pattern","title":"Schema version pattern","text":"<p>Add version to documents and modify application with multiple handlers while migrations run in batch on background task</p>"},{"location":"databases/mongo/#tree-pattern","title":"Tree pattern","text":"<ul> <li>Parent references</li> <li>Ancestors of X is COMPLEX</li> <li>Reports to Y is SIMPLE</li> <li>All nodes under Z is COMPLEX</li> <li>Change all under N to under P is SIMPLE</li> <li>Child references</li> <li>Ancestors of X is COMPLEX</li> <li>Reports to Y is COMPLEX</li> <li>All nodes under Z is SIMPLE</li> <li>Change all under N to under P is COMPLEX</li> <li>Array of ancestors</li> <li>Ancestors of X is SIMPLE</li> <li>Reports to Y is SIMPLE</li> <li>All nodes under Z is COMPLEX</li> <li> <p>Change all under N to under P is COMPLEX</p> </li> <li> <p>Materialized paths</p> </li> <li>Ancestors of X is SIMPLE</li> <li>Reports to Y is COMPLEX</li> <li>All nodes under Z is COMPLEX</li> <li>Change all under N to under P is COMPLEX</li> </ul> <p>Recomended: Array of ancestors + Parent</p>"},{"location":"databases/mongo/#polymorphic-pattern","title":"Polymorphic pattern","text":"<p>Used in single view cases (join data from different sources). Has a type field.</p>"},{"location":"databases/mongo/#approximation-pattern","title":"Approximation pattern","text":"<p>Does not matter if data is not precise. (page counter, counters with tolerance to imprecisions)</p>"},{"location":"databases/mongo/#outlier-pattern","title":"Outlier pattern","text":"<p>Handle documents in different maner if they are outliers</p> <p></p>"},{"location":"databases/mongo/#investigate","title":"Investigate","text":"<p>https://www.youtube.com/watch?v=bxw1AkH2aM4</p>"},{"location":"databases/neo4j/","title":"Neo4j","text":""},{"location":"databases/neo4j/#learn","title":"Learn","text":"<p>https://neo4j.com/sandbox/?utm_source=twitter&amp;utm_medium=ppc&amp;utm_campaign=sandbox-twitter-ads&amp;utm_content=usecase_image&amp;utm_term=graph-data-science</p>"},{"location":"databases/redis/","title":"Redis","text":""},{"location":"databases/redis/#links","title":"Links","text":"<p>Redis University</p>"},{"location":"databases/redis/#use-cases","title":"Use cases","text":"<ul> <li>String: Session, Cache, Distributed lock</li> <li>Int: Counter, Rate limit, Global ID</li> <li>Hash: Shopping cart</li> <li>Bitmap: User retention</li> <li>List: Message queue</li> <li>ZSet: Rank/Leaderboard</li> </ul>"},{"location":"databases/sql/","title":"SQL","text":""},{"location":"databases/sql/#procedures","title":"Procedures","text":""},{"location":"databases/sql/#detect-consecutive-dates-ranges-using-sql","title":"Detect consecutive dates ranges using SQL","text":"<pre><code>WITH t AS (\n  SELECT &lt;DATE_COLUMN&gt; d,ROW_NUMBER() OVER(ORDER BY &lt;DATE_COLUMN&gt;) i\n  FROM &lt;TABLE&gt;\n  GROUP BY &lt;DATE_COLUMN&gt;\n)\nSELECT MIN(d),MAX(d)\nFROM t\nGROUP BY DATEDIFF(day,i,d)\n</code></pre>"},{"location":"databases/sql/#count-comma-separated-items","title":"Count comma separated items","text":"<pre><code>LENGTH(fooCommaDelimColumn) - LENGTH(REPLACE(fooCommaDelimColumn, ',', ''))\n</code></pre>"},{"location":"databases/sql/#build-time-scale-table-in-mysql","title":"Build time scale table in MySQL","text":"<pre><code>SET @d0 = \"2020-01-02\";\nSET @d1 = \"2030-12-31\";\n\nSET @date = date_sub(@d0, interval 1 day);\n\nINSERT INTO &lt;TIME_TABLE&gt;(&lt;DATE_COL&gt;, &lt;YEAR_COL&gt;, &lt;MONTH_COL&gt;)\nSELECT @date := date_add(@date, interval 1 day) as &lt;DATE_COL&gt;,\nyear(@date) as &lt;YEAR_COL&gt;,\nmonth(@date) as &lt;MONTH_COL&gt;\nFROM &lt;BIG_TABLE&gt; -- biger than scale table\nWHERE date_add(@date, interval 1 day) &lt;= @d1\nORDER BY &lt;DATE_COL&gt;;\n\n</code></pre>"},{"location":"databases/sql/#greatest-n-per-group","title":"Greatest N per group","text":"<pre><code>SELECT c.*, p1.* \nFROM customer c \nJOIN purchase p1 ON (c.id = p1.customer_id) \nLEFT OUTER JOIN purchase p2 ON (c.id = p2.customer_id AND (p1.date &lt; p2.date OR p1.date = p2.date AND p1.id &lt; p2.id)) \nWHERE p2.id IS NULL;\n</code></pre>"},{"location":"databases/sql/#documentation","title":"Documentation","text":"<p>Analytic functions</p>"},{"location":"devops/devops/","title":"DevOps","text":""},{"location":"devops/devops/#links","title":"Links","text":"<p>Exercises</p>"},{"location":"devops/devops/#learn","title":"Learn","text":"<p>https://www.youtube.com/c/DevOpsToolkit/videos</p>"},{"location":"devops/app-definition/helm/","title":"Helm","text":""},{"location":"devops/app-definition/helm/#documentation","title":"Documentation","text":"<p>https://helm.sh/</p>"},{"location":"devops/automation/ansible/","title":"Ansible","text":""},{"location":"devops/automation/ansible/#installation","title":"Installation","text":"<p>https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html</p>"},{"location":"devops/automation/ansible/#commands","title":"Commands","text":"<pre><code>ansible\nansible-playbook\nansible-galaxy\nansible-inventory\nansible-vault\n\nsudo apt install python-pip\n\n</code></pre>"},{"location":"devops/automation/ansible/#use-cases","title":"Use cases","text":""},{"location":"devops/automation/ansible/#provisioning","title":"Provisioning","text":""},{"location":"devops/automation/ansible/#cloud","title":"Cloud","text":"<p>For integration testing.</p>"},{"location":"devops/automation/ansible/#virtual-machine","title":"Virtual machine","text":"<p>Comonly used by Vagrant</p>"},{"location":"devops/automation/ansible/#task-automating","title":"Task automating","text":""},{"location":"devops/automation/ansible/#system-reboot","title":"System reboot","text":""},{"location":"devops/automation/ansible/#disaster-recovery","title":"Disaster recovery","text":""},{"location":"devops/automation/ansible/#configuration-management","title":"Configuration management","text":""},{"location":"devops/automation/ansible/#patching","title":"Patching","text":"<p>Automating all OS updates.</p>"},{"location":"devops/automation/ansible/#kubernetes","title":"Kubernetes","text":"<p>Instead of helm.</p>"},{"location":"devops/automation/ansible/#switch","title":"Switch","text":"<p>Mass switch configuration changes.</p>"},{"location":"devops/automation/ansible/#application-deployment","title":"Application deployment","text":""},{"location":"devops/automation/ansible/#continuous-delivery","title":"Continuous delivery","text":""},{"location":"devops/automation/ansible/#cicd-pipeline","title":"CI/CD pipeline","text":"<p>In your CI/CD tool, use a template where first step just calls out your Ansible playbook for the actual build and deployment.</p>"},{"location":"devops/automation/ansible/#security-automation","title":"Security automation","text":""},{"location":"devops/automation/ansible/#orchestration","title":"Orchestration","text":""},{"location":"devops/automation/ansible/#dynamic-documentation","title":"Dynamic documentation","text":""},{"location":"devops/automation/ansible/#structure","title":"Structure","text":"<pre><code>inventories/\n   production/\n      hosts               # inventory file for production servers\n      group_vars/\n         group1           # here we assign variables to particular groups\n         group2           # \"\"\n      host_vars/\n         hostname1        # if systems need specific variables, put them here\n         hostname2        # \"\"\n\n   staging/\n      hosts               # inventory file for staging environment\n      group_vars/\n         group1           # here we assign variables to particular groups\n         group2           # \"\"\n      host_vars/\n         stagehost1       # if systems need specific variables, put them here\n         stagehost2       # \"\"\n\nlibrary/                  # if any custom modules, put them here (optional)\nmodule_utils/             # if any custom module_utils to support modules, put them here (optional)\nfilter_plugins/           # if any custom filter plugins, put them here (optional)\n\nsite.yml                  # master playbook\nwebservers.yml            # playbook for webserver tier\ndbservers.yml             # playbook for dbserver tier\n\nroles/\n    common/               # this hierarchy represents a \"role\"\n        tasks/            #\n            main.yml      #  &lt;-- tasks file can include smaller files if warranted\n        handlers/         #\n            main.yml      #  &lt;-- handlers file\n        templates/        #  &lt;-- files for use with the template resource\n            ntp.conf.j2   #  &lt;------- templates end in .j2\n        files/            #\n            bar.txt       #  &lt;-- files for use with the copy resource\n            foo.sh        #  &lt;-- script files for use with the script resource\n        vars/             #\n            main.yml      #  &lt;-- variables associated with this role\n        defaults/         #\n            main.yml      #  &lt;-- default lower priority variables for this role\n        meta/             #\n            main.yml      #  &lt;-- role dependencies\n        library/          # roles can also include custom modules\n        module_utils/     # roles can also include custom module_utils\n        lookup_plugins/   # or other types of plugins, like lookup in this case\n\n    webtier/              # same kind of structure as \"common\" was above, done for the webtier role\n    monitoring/           # \"\"\n    fooapp/               # \"\"\n</code></pre>"},{"location":"devops/automation/ansible/#components","title":"Components","text":""},{"location":"devops/automation/ansible/#inventory","title":"Inventory","text":""},{"location":"devops/automation/ansible/#static","title":"Static","text":"<p>Can be a INI or a yaml file. Uses IPs (125.125.125.125) and FQDNs (domain.com).</p> <pre><code>[group-1]\nhost1\nhost2\n\n[group-2]\nhost2\nhost3\n\n[group-3]\nhost2\nhost3\n\n[group-1_2:children]\ngroup-1\ngroup-2\n\n[group-all:children]\ngroup-1_2\ngroup-3\n</code></pre>"},{"location":"devops/automation/ansible/#variables","title":"Variables","text":"<p>Is a yaml file.</p>"},{"location":"devops/automation/ansible/#role-variables","title":"Role variables","text":"<p>Variables related to a specific role, for example packager required in a webserver.</p> <pre><code>webserver_packages:\n  - nginx\n  - git\n</code></pre>"},{"location":"devops/automation/ansible/#group-variables","title":"Group variables","text":"<p>Variables related to a group of hosts, for example the ssh key used for all servers in zone.</p> <pre><code>ansible_ssh_private_key_file: ~/.ssh/zone-1.pem\n</code></pre>"},{"location":"devops/automation/ansible/#host-variables","title":"Host variables","text":"<p>Variables related to a specific host, for example ssh related data.</p> <pre><code>ansible_connection: ssh\nansible_user: user\nansible_password: password\nansible_host: 123.123.123.123\nansible_port: 33\n</code></pre>"},{"location":"devops/automation/ansible/#roles","title":"Roles","text":"<p>Represents a sarting point comon in a system. Has all the required data to create a non customized installation of something.</p>"},{"location":"devops/automation/ansible/#tasks","title":"Tasks","text":"<p>Order to perform something.</p>"},{"location":"devops/automation/ansible/#good-practices","title":"Good practices","text":"<p>Keep your playbooks, roles, inventory, and variables files in git or another version control system.</p> <p>With cloud providers and other systems that maintain canonical lists of your infrastructure, use dynamic inventory to retrieve those lists instead of manually updating static inventory files.</p> <p>If you create groups named for the function of the nodes in the group, for example webservers or dbservers, your playbooks can target machines based on function.</p> <p>You can keep your production environment separate from development, test, and staging environments by using separate inventory files or directories for each environment. This way you pick with -i what you are targeting.</p> <p>You should encrypt sensitive or secret variables with Ansible Vault. However, encrypting the variable names as well as the variable values makes it hard to find the source of the values. You can keep the names of your variables accessible (by <code>grep</code>, for example) without exposing any secrets by adding a layer of indirection:</p> <ol> <li>Create a <code>group_vars/</code> subdirectory named after the group.</li> <li>Inside this subdirectory, create two files named <code>vars</code> and <code>vault</code>.</li> <li>In the <code>vars</code> file, define all of the variables needed, including any sensitive ones.</li> <li>Copy all of the sensitive variables over to the <code>vault</code> file and prefix these variables with <code>vault_</code>.</li> <li>Adjust the variables in the <code>vars</code> file to point to the matching <code>vault_</code> variables using jinja2 syntax: <code>db_password: {{ vault_db_password }}</code>.</li> <li>Encrypt the <code>vault</code> file to protect its contents.</li> <li>Use the variable name from the <code>vars</code> file in your playbooks.</li> </ol> <p>When running a playbook, Ansible finds the variables in the unencrypted file, which pulls the sensitive variable values from the encrypted file. There is no limit to the number of variable and vault files or their names.</p>"},{"location":"devops/automation/ansible/#ansible-config","title":"ansible-config","text":"<p>Used to detect changes between default config and project config.</p> <pre><code>ansible-config dump --only-changed\n</code></pre> <p>Example of modified config in <code>ansible.cfg</code>:</p> <pre><code>[defaults]\nroles_path = roles\nlog_path = ./ansible.log\n</code></pre>"},{"location":"devops/automation/ansible/#ansible-galaxy","title":"ansible-galaxy","text":"<p>Used to create new roles and import external existing roles from central repository. By default we can import any role from https://galaxy.ansible.com</p> <pre><code>ansible-galaxy install robertdebock.nginx\nansible-galaxy list\ncd roles\nansible-galaxy init demo\ncd ..\nansible-galaxy list\n</code></pre>"},{"location":"devops/automation/ansible/#ansible-inventory","title":"ansible-inventory","text":"<p>Used to manage the inventory.</p> <pre><code>ansible-inventory --list -i inventories/dev/hosts\n</code></pre>"},{"location":"devops/automation/ansible/#ansible-vault","title":"ansible-vault","text":"<p>Used to create and manage secure variable files (protected by password).</p> <pre><code>ansible-vault encrypt vault.yaml\nansible-playbook main.yaml -i inventories/dev/hosts --ask-vault-pass\nansible-vault view vault.yaml\nansible-vault edit vault.yaml\nenv EDITOR=nano ansible-vault edit vault.yaml\n</code></pre>"},{"location":"devops/automation/ansible/#ansible-playbook","title":"ansible-playbook","text":"<pre><code>ansible-playbook main.yaml -i inventories/dev/hosts\n</code></pre>"},{"location":"devops/automation/terraform/","title":"Terraform","text":""},{"location":"devops/automation/terraform/#links","title":"Links","text":"<p>Tutorial AWS Provider Course</p>"},{"location":"devops/automation/terraform/#install","title":"Install","text":""},{"location":"devops/automation/terraform/#ubuntu","title":"Ubuntu","text":"<pre><code>curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\nsudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"\nsudo apt-get update &amp;&amp; sudo apt-get install terraform\nterraform -help\n</code></pre>"},{"location":"devops/automation/terraform/#commands","title":"Commands","text":"<pre><code>terraform init\nterraform apply -auto-approve\nterraform destroy -auto-approve\nterraform plan\n</code></pre> <p>Format</p> <pre><code>terraform fmt -diff\n</code></pre>"},{"location":"devops/automation/terraform/#files","title":"Files","text":""},{"location":"devops/automation/terraform/#tfvars-files","title":"<code>*.tfvars</code> files","text":"<p>Used to define values for variables.</p> <p>Terraform will automatically load the variable values from the variable definition file if it is named <code>terraform.tfvars</code> or ends in <code>.auto.tvfars</code> and placed in the same directory as the other configuration files. Other can be load using the flag <code>-var-file=&lt;FILE&gt;</code> .</p> <p>Example:</p> <p><code>test-env.tfvars</code></p> <pre><code>project        = \"generated-area-284017\"\nregion         = \"europe-west1-b\"\ncluster_name   = \"test-cluster\"\nnode_pool_name = \"test-node-pool\"\nnum_nodes      = 0\n</code></pre>"},{"location":"devops/automation/terraform/#variablestf","title":"<code>variables.tf</code>","text":"<p>Used to define variables and default values.</p> <pre><code>variable \"project\" {\n  type        = string\n  description = \"Google project id\"\n}\n\nvariable \"region\" {\n  type        = string\n  description = \"Google location\"\n}\n\nvariable \"cluster_name\" {\n  type        = string\n  description = \"Cluester name\"\n}\n\nvariable \"node_pool_name\" {\n  type        = string\n  description = \"Node pool name\"\n}\n\nvariable \"num_nodes\" {\n  type        = number\n  description = \"Node pool name\"\n}\n</code></pre>"},{"location":"devops/automation/terraform/#versionstf","title":"<code>versions.tf</code>","text":"<p>Defines the version</p> <pre><code>terraform {\n  required_version = \"&gt;= 0.12.26\"\n}\n</code></pre>"},{"location":"devops/automation/terraform/#tf-files","title":"<code>*.tf</code> files","text":"<p>Contains code to execute</p>"},{"location":"devops/automation/terraform/#file-layout-example","title":"File layout example","text":"<pre><code>stage\n  + vpc\n  + services\n      + frontend-app\n      + backend-app\n  + data-storage\n      + mysql\n      + redis\nprod\n  + vpc\n  + services\n      + frontend-app\n      + backend-app\n  + data-storage\n      + mysql\n      + redis\nmgmt\n  + vpc\n  + services\n      + bastion-host\n      + jenkins\nglobal\n  + iam\n  + s3\n</code></pre>"},{"location":"devops/automation/terraform/#reusable-definitions","title":"Reusable definitions","text":"<p><code>modules/services/webserver-cluster/main.tf</code></p> <pre><code>resource \"aws_security_group\" \"elb\" {\n  name = \"${var.cluster_name}-elb\"\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n</code></pre> <p><code>modules/services/webserver-cluster/variables.tf</code></p> <pre><code>variable \"cluster_name\" {\n  description = \"The name to use for all the cluster resources\"\n  type        = string\n}\n</code></pre> <p><code>modules/services/webserver-cluster/outputs.tf</code></p> <pre><code>output \"asg_name\" {\n  value       = aws_autoscaling_group.example.name\n  description = \"The name of the Auto Scaling Group\"\n}\n</code></pre> <p><code>prod/services/webserver-cluster/main.tf</code></p> <pre><code>provider \"aws\" {\n  region = \"us-east-2\"\n}\nmodule \"webserver_cluster\" {\n  source = \"../../../modules/services/webserver-cluster\"\n\n  cluster_name = \"webservers-prod\"\n}\n\nresource \"aws_autoscaling_schedule\" \"scale_out_business_hours\" {\n  autoscaling_group_name = module.webserver_cluster.asg_name\n}\n</code></pre>"},{"location":"devops/automation/terraform/#version-control","title":"Version control","text":"<p>Is recomended to have 2 repositories, one with the live configuration and other with the reusable modules</p> <pre><code>module \"webserver_cluster\" {\n  source = \"github.com/foo/modules/services/webserver-cluster?ref=v0.0.1\"\n  cluster_name  = \"webservers-stage\"\n  instance_type = \"t2.micro\"\n  min_size      = 2\n  max_size      = 2\n}\n</code></pre>"},{"location":"devops/ci-cd/argocd/","title":"ArgoCD","text":""},{"location":"devops/ci-cd/argocd/#commands","title":"Commands","text":"<p>Install</p> <pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\nkubectl port-forward svc/argocd-server -n argocd 8080:443\n\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre>"},{"location":"devops/ci-cd/github-actions/","title":"Github actions","text":"<p>https://medium.com/holisticon-consultants/foss-ci-cd-with-github-actions-c65c37236c19</p>"},{"location":"devops/ci-cd/github-actions/#links","title":"Links","text":"<p>Github learning paths</p>"},{"location":"devops/ci-cd/gitlabci/","title":"GitlabCI","text":""},{"location":"devops/ci-cd/gitlabci/#explore","title":"Explore","text":"<p>https://www.notion.so/100DaysOfKubernetes-0c1e0de84d654e59bdf89242195abb22</p>"},{"location":"devops/ci-cd/gitlabci/#file-structure","title":"File structure","text":"<p>File should be named <code>.gitlab-ci.yml</code> and be in the root folder.</p> <pre><code>image: &lt;BASE_IMAGE&gt;\n\nservices:\n  - &lt;BASE_SERVICE_1&gt;\n\nstages:\n  - &lt;STAGE_1&gt;\n\nbefore_script:\n  - &lt;SCRIPT_COMMAND&gt;\n\nafter_script:\n  - &lt;SCRIPT_COMMAND&gt;\n\ninclude:\n  - &lt;TEMPLATE_TO_INCLUDE&gt;\n\nvariables:\n  &lt;VARIABLE_1&gt;: &lt;VALUE_1&gt;\n\ncache:\n  paths:\n    - &lt;PATH_1&gt;\n  key: &lt;CACHE_KEY&gt;\n\n&lt;JOB_NAME_1&gt;:\n  image: &lt;JOB_IMAGE&gt;\n  stage: &lt;JOB_STAGE&gt;\n  services:\n    - &lt;SERVICE_1&gt;\n  script:\n    - &lt;SCRIPT_COMMAND&gt;\n  artifacts:\n    paths:\n      - &lt;ARTIFACT_PATH_1&gt;\n    expire_in: &lt;TIME_EXPRESION&gt;\n</code></pre>"},{"location":"devops/ci-cd/gitlabci/#common-schemes","title":"Common schemes","text":""},{"location":"devops/ci-cd/gitlabci/#merge-request-validation","title":"Merge request validation","text":"<p>Validate the source branch of a merge request and block merge if don't meets the criteria.</p> <pre><code>variables:\n  MAVEN_OPTS: \"-Dhttps.protocols=TLSv1.2 -Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN -Dorg.slf4j.simpleLogger.showDateTime=true -Djava.awt.headless=true\"\n  SONAR_USER_HOME: \"${CI_PROJECT_DIR}/.sonar\"\n\nstages:\n  - test\n  - qa\n\nmaven_test:\n  stage: test\n  image: maven:latest\n  script:\n    - mvn $MAVEN_CLI_OPTS clean verify\n  cache:\n    paths:\n      - .m2/repository\n  artifacts:\n    paths:\n      - target/*\n    expire_in: 1 hour\n  only:\n    - merge_requests\n\nsonarqube_check:\n  stage: qa\n  image:\n    name: sonarsource/sonar-scanner-cli:latest\n    entrypoint: [\"\"]\n  cache:\n    paths:\n      - .sonar/cache\n  needs:\n    - job: maven_test\n      artifacts: true\n  script:\n    - sonar-scanner\n      -Dsonar.qualitygate.wait=true\n      -Dsonar.projectName=${CI_PROJECT_NAME}\n      -Dsonar.projectKey=${CI_PROJECT_NAME}\n      -Dsonar.java.binaries=target/classes\n      -Dsonar.links.ci=${CI_PROJECT_URL}/pipelines\n      -Dsonar.links.homepage=${CI_PROJECT_URL}\n      -Dsonar.sourceEncoding=UTF-8\n  only:\n    - merge_requests\n\ngitleaks:\n  stage: qa\n  image:\n    name: dxa4481/trufflehog:latest\n    entrypoint: [\"\"]\n  script:\n    - &gt;\n      trufflehog\n      --branch=${CI_COMMIT_BRANCH}\n      --since_commit=${CI_COMMIT_BEFORE_SHA}\n      --regex\n      ${CI_REPOSITORY_URL}\n  only:\n    - merge_requests\n</code></pre>"},{"location":"devops/ci-cd/gitlabci/#ci-vars","title":"CI Vars","text":"<pre><code>SONAR_HOST_URL\nSONAR_TOKEN\n</code></pre>"},{"location":"devops/ci-cd/gitlabci/#gitlab-config","title":"Gitlab config","text":"<p>In <code>Settings &gt; General &gt; Merge Requests &gt; Merge checks</code> select <code>Pipeline must succeed</code></p>"},{"location":"devops/ci-cd/gitlabci/#spring-boot-docker-kubernetes","title":"Spring Boot + Docker + Kubernetes","text":"<p>In <code>.gitlab-ci.yaml</code></p> <pre><code>variables:\n  MAVEN_OPTS: \"-Dhttps.protocols=TLSv1.2 -Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN -Dorg.slf4j.simpleLogger.showDateTime=true -Djava.awt.headless=true\"\n\nstages:\n  - test\n  - build\n  - containerize\n  - deploy\n\nmaven test:\n  stage: test\n  image: maven\n  script: mvn $MAVEN_CLI_OPTS clean test\n  cache:\n    paths:\n      - .m2/repository\n\nmaven package:\n  stage: build\n  image: maven\n  script: mvn $MAVEN_CLI_OPTS clean package -Dmaven.test.skip=true\n  cache:\n    paths:\n      - .m2/repository\n  artifacts:\n    paths:\n      - target/*.jar\n    expire_in: 1 hour\n\ndocker containerize:\n  stage: containerize\n  image:\n    name: gcr.io/kaniko-project/executor:debug\n    entrypoint: [\"\"]\n  script:\n    - mkdir -p /kaniko/.docker\n    - echo \"{\\\"auths\\\":{\\\"$CI_REGISTRY\\\":{\\\"username\\\":\\\"$CI_REGISTRY_USER\\\",\\\"password\\\":\\\"$CI_REGISTRY_PASSWORD\\\"}}}\" &gt; /kaniko/.docker/config.json\n    - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $CI_REGISTRY_IMAGE:latest\n\n# $CI_COMMIT_TAG\n\nkubernetes deploy:\n  stage: deploy\n  script: echo 'Deploy'\n  when: manual\n</code></pre> <p>In <code>Dockerfile</code></p> <pre><code>FROM adoptopenjdk/openjdk11:jre-11.0.9_11.1-alpine\nRUN addgroup -S spring &amp;&amp; adduser -S spring -G spring\nUSER spring:spring\nARG JAR_FILE=target/*.jar\nCOPY ${JAR_FILE} app.jar\nENTRYPOINT [\"java\", \"-Djava.security.egd=file:/dev/./urandom\", \"-jar\", \"/app.jar\"]\n</code></pre> <p>Download image</p> <pre><code>docker login registry.gitlab.com/group -u TOKEN_USER -p TOKEN\ndocker pull registry.gitlab.com/group/project:latest\ndocker run -p 8080:8080 registry.gitlab.com/group/project:latest\n</code></pre>"},{"location":"devops/ci-cd/gitlabci/#trigger-other-pipeline","title":"Trigger other pipeline","text":"<pre><code>trigger_job:\n    stage: project2_deploy\n    script:\n        - \u201ccurl -X POST -F token=${OTHER_REPO_TOKEN} -F ref=${REF_NAME} https://gitlab.com/api/v4/projects/2/trigger/pipeline\"\n    when: on_success\n</code></pre>"},{"location":"devops/ci-cd/gitlabci/#other-sources","title":"Other sources","text":"<p>https://gitlab.com/gitlab-org/gitlab-ci-yml/tree/master</p>"},{"location":"devops/containers/docker/","title":"Docker","text":""},{"location":"devops/containers/docker/#commands","title":"Commands","text":"<p>Add credentials</p> <pre><code>docker login -u &lt;username&gt; -p &lt;deploy_token&gt; registry.example.com\n</code></pre> <pre><code>\\\\wsl$\\docker-desktop-data\\version-pack-data\\community\\docker\\volumes\\\n</code></pre>"},{"location":"devops/containers/docker/#info","title":"Info","text":"<p>Prefer COPY over ADD, use ADD to download files or to extract archives</p> <pre><code># download external file\nADD https://source.com/file /destination/path\n\n# copy and extract content\nADD source.file.tar.gz /destination/path\n</code></pre>"},{"location":"devops/containers/docker/#optimize-images","title":"Optimize images","text":"<p>TODO</p> <p>https://jpetazzo.github.io/2020/02/01/quest-minimal-docker-images-part-1/</p> <p>https://jpetazzo.github.io/2020/03/01/quest-minimal-docker-images-part-2/</p> <p>https://jpetazzo.github.io/2020/04/01/quest-minimal-docker-images-part-3/</p>"},{"location":"devops/containers/kubernetes/","title":"Kubernetes","text":""},{"location":"devops/containers/kubernetes/#links","title":"Links","text":"<p>Should i use K8s? Patterns</p>"},{"location":"devops/containers/kubernetes/#install-kubectl","title":"Install <code>kubectl</code>","text":""},{"location":"devops/containers/kubernetes/#debian","title":"Debian","text":"<pre><code>sudo apt-get install gnupg2\nsudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\necho \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre>"},{"location":"devops/containers/kubernetes/#ubuntu","title":"Ubuntu","text":"<pre><code>sudo apt-get install kubectl\n</code></pre>"},{"location":"devops/containers/kubernetes/#commands","title":"Commands","text":"<p>List contexts</p> <pre><code>kubectl config get-contexts\n</code></pre> <p>Switch context</p> <pre><code>kubectl config use-context &lt;CONTEXT_NAME&gt;\n</code></pre> <p>Rename context</p> <pre><code>kubectl config rename-context &lt;OLD_NAME&gt; &lt;NEW_NAME&gt;\n</code></pre> <p>Create manifest (Imperative)</p> <pre><code>kubectl create -f manifest.yaml\n</code></pre> <p>Apply manifest (Declarative)</p> <pre><code>kubectl apply -f manifest.yaml\n</code></pre> <p>Show resources</p> <pre><code>kubectl get nodes\nkubectl get pods\nkubectl get rs\nkubectl get deployments\nkubectl get svc\nkubectl get ingress\nkubectl get serviceaccount\nkubectl get netpol\n</code></pre> <p>Delete manifest</p> <pre><code>kubectl delete manifes.yaml\n</code></pre> <p>Show namespaces with labels</p> <pre><code>kubectl get namespaces --show-labels\n</code></pre> <p>Add lable to namespace</p> <pre><code> kubectl label namespace default istio-injection=enabled\n</code></pre> <p>Rolling updates</p> <pre><code>kubectl apply -f file.yaml --record\nkubectl rollout history deployment/name\nkubectl rollout history deployment/name --revision=1\nkubectl rollout undo deployment/name\nkubectl rollout undo deployment/name --to-revision=1\n</code></pre> <p>See logs</p> <pre><code>kubectl logs -f POD_NAME\n</code></pre> <p>Execute shell</p> <pre><code>kubectl exec -it nginx -- /bin/bash\n</code></pre> <p>Port forward</p> <pre><code>kubectl port-forward deployments/deployment local_port:image_port\nkubectl port-forward pod local_port:image_port\n</code></pre> <p>Proxy</p> <pre><code>kubectl proxy\nhttp://localhost:8001/api/v1/proxy/namespaces/&lt;NAMESPACE&gt;/services/&lt;SERVICE_NAME&gt;/proxy/\n</code></pre>"},{"location":"devops/containers/kubernetes/#pods","title":"Pods","text":""},{"location":"devops/containers/kubernetes/#envs-from-spec","title":"Envs from spec","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-envars-fieldref\nspec:\n  containers:\n    - name: test-container\n      image: k8s.gcr.io/busybox\n      env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n</code></pre>"},{"location":"devops/containers/kubernetes/#resources","title":"Resources","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: image\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n</code></pre>"},{"location":"devops/containers/kubernetes/#probes","title":"Probes","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: goproxy\n  labels:\n    app: goproxy\nspec:\n  containers:\n  - name: goproxy\n    image: k8s.gcr.io/goproxy:0.1\n    ports:\n    - containerPort: 8080\n    readinessProbe:\n      tcpSocket:\n        port: 8080\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 8080\n      initialDelaySeconds: 15\n      periodSeconds: 20\n</code></pre>"},{"location":"devops/containers/kubernetes/#multi-container","title":"Multi container","text":"<p>Only recommended for specific cases. In this example is required to extract the logs from the main container.</p> <pre><code>apiVersion: v1                                 \nkind: Pod                                      \nmetadata:                                      \n  name: exim                                   \nspec:                                          \n  containers:                                  \n    - name: exim                               \n      image: dodemoorg/exim:1.3                \n      env:                                     \n        - name: domain                         \n          value: example.local                 \n        - name: root_alias                     \n          value: youraddress@example.local     \n        - name: queue_time                     \n          value: 1m                            \n      volumeMounts:                            \n        - mountPath: /var/log/exim4            \n          name: logs                           \n    - name: filebeats                          \n      image: dodemoorg/eximbeats:1.3           \n      volumeMounts:                            \n        - mountPath: /var/log/exim4            \n          name: logs                           \n  volumes:                                     \n    - name: logs                               \n      emptyDir: {}                             \n</code></pre>"},{"location":"devops/containers/kubernetes/#initcontainer","title":"InitContainer","text":"<pre><code>apiVersion: v1                                                                                      \nkind: Pod                                                                                           \nmetadata:                                                                                           \n  name: octopress                                                                                   \nspec:                                                                                               \n  initContainers:                                                                                   \n    - name: octopress                                                                               \n      image: registry.gitlab.com/perlstalker/octopress-deploy                                       \n      volumeMounts:                                                                                 \n        - mountPath: /public                                                                        \n          name: docroot                                                                             \n  containers:                                                                                       \n    - name: web                                                                                     \n      image: nginx:1.13.6                                                                           \n      ports:                                                                                        \n        - containerPort: 80                                                                         \n      volumeMounts:                                                                                 \n        - mountPath: /usr/share/nginx/html                                                          \n          name: docroot                                                                             \n  volumes:                                                                                          \n    - name: docroot                                                                                 \n      emptyDir: {}                                                                                  \n</code></pre>"},{"location":"devops/containers/kubernetes/#job","title":"Job","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello\nspec:\n  template:\n    metadata:\n      name: hello\n    spec:\n      containers:\n        - name: hello\n          image: debian:9\n          command: [\"echo\", \"Hello world!\"]\n      restartPolicy: OnFailure\n</code></pre>"},{"location":"devops/containers/kubernetes/#cronjob","title":"CronJob","text":"<pre><code>apiVersion: batch/v1beta1                          \nkind: CronJob                                      \nmetadata:                                          \n  name: hello                                      \nspec:                                              \n  schedule: \"*/1 * * * *\"                          \n  jobTemplate:                                     \n    metadata:                                      \n      name: hello                                  \n    spec:                                          \n      template:                                    \n        spec:                                      \n          containers:                              \n            - name: hello                          \n              image: debian:9                      \n              command: [\"echo\", \"Hello world!\"]    \n          restartPolicy: Never                \n</code></pre>"},{"location":"devops/containers/kubernetes/#secrets","title":"Secrets","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: passwords\ndata:\n  dbpass: cGFzc3dvcmQ= #b64\n  rootpass: ZGZqbCQ0OTRxbGtsJCUlXl4=\n</code></pre> <p>Create secret from console:</p> <pre><code>kubectl create secret generic test-secret --from-literal=username='my-app' --from-literal=password='39528$vdg7Jb'\n</code></pre> <p>Using a value of <code>Secret</code> in <code>env</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: secretenv\nspec:\n  containers:\n    - name: busybox\n      image: busybox:1.27.2\n      env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: passwords\n              key: dbpass\n        - name: ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: passwords\n              key: rootpass\n</code></pre> <p>Using all values of <code>Secret</code> in <code>env</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: envfrom-secret\nspec:\n  containers:\n  - name: envars-test-container\n    image: nginx\n    envFrom:\n    - secretRef:\n        name: passwords\n</code></pre> <p>Using a value of <code>Secret</code> in <code>volume</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: secretvol\nspec:\n  volumes:\n    - name: password-volume\n      secret:\n        secretName: passwords\n  containers:\n    - name: busybox\n      image: busybox:1.27.2\n      volumeMounts:\n        - name: password-volume\n          mountPath: /etc/passwords\n</code></pre>"},{"location":"devops/containers/kubernetes/#configmaps","title":"ConfigMaps","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: testconf\n  namespace: default\ndata:\n  test: value\n</code></pre> <p>Using a value of <code>ConfigMap</code> in <code>env</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: configenv\nspec:\n  containers:\n    - name: busybox\n      image: busybox:1.27.2\n      env:\n        - name: TEST_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: testconf\n              key: test\n</code></pre> <p>Using all values of <code>ConfigMap</code> in <code>env</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: allconfigenv\nspec:\n  containers:\n    - name: busybox\n      image: busybox:1.27.2\n      envFrom:\n      - configMapRef:\n          name: testconf\n</code></pre> <p>Using value of <code>ConfigMap</code> in <code>env</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: configvol\nspec:\n  volumes:\n    - name: testconf-volume\n      configMap:\n        name: testconf\n  containers:\n    - name: busybox\n      image: busybox:1.27.2\n      volumeMounts:\n        - name: testconf-volume\n          mountPath: /etc/testconf\n</code></pre>"},{"location":"devops/containers/kubernetes/#volumes","title":"Volumes","text":""},{"location":"devops/containers/kubernetes/#volume","title":"Volume","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: redis\nspec:\n  containers:\n  - name: redis\n    image: redis\n    volumeMounts:\n    - name: redis-storage\n      mountPath: /data/redis\n  volumes:\n  - name: redis-storage\n    emptyDir: {}\n</code></pre>"},{"location":"devops/containers/kubernetes/#persistentvolume","title":"PersistentVolume","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n  - image: k8s.gcr.io/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - name: test-volume\n    gcePersistentDisk:\n      pdName: my-data-disk\n      fsType: ext4\n</code></pre>"},{"location":"devops/containers/kubernetes/#deployments","title":"Deployments","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"devops/containers/kubernetes/#services","title":"Services","text":""},{"location":"devops/containers/kubernetes/#clusterip","title":"ClusterIP","text":"<p>Only accesible inside cluster. Is the default value.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:  \n  name: my-internal-nginx-service\nspec:\n  selector:    \n    app: nginx\n  type: ClusterIP\n  ports:  \n  - name: http\n    port: 80\n    targetPort: 80\n    protocol: TCP\n</code></pre>"},{"location":"devops/containers/kubernetes/#nodeport","title":"NodePort","text":"<p>Used to configure <code>Ingress</code>.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:  \n  name: nginx-node-service\nspec:\n  selector:    \n    app: nginx\n  type: NodePort\n  ports:  \n  - name: http\n    port: 80\n    targetPort: 80\n    protocol: TCP\n</code></pre>"},{"location":"devops/containers/kubernetes/#load-balancer","title":"Load Balancer","text":"<p>The big downside is that each service you expose with a LoadBalancer will get its own IP address, and you have to pay for a LoadBalancer per exposed service, which can get expensive!</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx\n  ports:\n    - port: 80\n      targetPort: 80\n  type: LoadBalancer\n</code></pre>"},{"location":"devops/containers/kubernetes/#externalname","title":"ExternalName","text":"<p>Used to configure external services that have a URI.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  namespace: prod\nspec:\n  type: ExternalName\n  externalName: my.database.example.com\n</code></pre>"},{"location":"devops/containers/kubernetes/#external-service-with-ip","title":"External service with IP","text":"<pre><code>kind: Service\napiVersion: v1\nmetadata:\n name: mongo\nSpec:\n type: ClusterIP\n ports:\n - port: 27017\n   targetPort: 27017 # can be another port\n</code></pre> <pre><code>kind: Endpoints\napiVersion: v1\nmetadata:\n name: mongo\nsubsets:\n - addresses:\n     - ip: 10.240.0.4\n   ports:\n     - port: 27017 # can be another port\n</code></pre>"},{"location":"devops/containers/kubernetes/#serviceaccount","title":"ServiceAccount","text":"<p>Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example, default).</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: myapp-sa\n  labels:\n    account: myapp\n</code></pre>"},{"location":"devops/containers/kubernetes/#ingress","title":"Ingress","text":"<p>It sits in front of multiple services and act as a \"smart router\" or entrypoint into your cluster.</p>"},{"location":"devops/containers/kubernetes/#single-service","title":"Single service","text":"<pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /\nspec:\n  backend:\n    serviceName: nginx-node-service\n    servicePort: 80\n</code></pre>"},{"location":"devops/containers/kubernetes/#simple-fanout","title":"Simple fanout","text":"<pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: simple-fanout-example\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - path: /foo\n        backend:\n          serviceName: service1\n          servicePort: 4200\n      - path: /bar\n        backend:\n          serviceName: service2\n          servicePort: 8080\n</code></pre>"},{"location":"devops/containers/kubernetes/#name-based-virtual-hosting","title":"Name based virtual hosting","text":"<pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: name-virtual-host-ingress\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - backend:\n          serviceName: service1\n          servicePort: 80\n  - host: bar.foo.com\n    http:\n      paths:\n      - backend:\n          serviceName: service2\n          servicePort: 80\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: name-virtual-host-ingress\nspec:\n  rules:\n  - host: first.bar.com\n    http:\n      paths:\n      - backend:\n          serviceName: service1\n          servicePort: 80\n  - host: second.foo.com\n    http:\n      paths:\n      - backend:\n          serviceName: service2\n          servicePort: 80\n  - http:\n      paths:\n      - backend:\n          serviceName: service3\n          servicePort: 80\n</code></pre>"},{"location":"devops/containers/kubernetes/#networkpolicy","title":"NetworkPolicy","text":"<pre><code>apiVersion: networking.k8s.io/v1                            \nkind: NetworkPolicy                                         \nmetadata:                                                   \n  name: demo-np                                             \n  namespace: default                                        \nspec:                                                       \n  podSelector:                                              \n    matchLabels:                                            \n     app: foo                                               \n  policyTypes:                                              \n    - Ingress                                               \n  ingress:                                                  \n    - from:                                                 \n        - ipBlock:                                          \n            cidr: 192.168.101.0/24                          \n        - podSelector:                                      \n            matchLabels:                                    \n              app: foo                                      \n      ports:                                                \n        - protocol: TCP                                     \n          port: 80                                          \n</code></pre>"},{"location":"devops/containers/kubernetes/#learn","title":"Learn","text":"<p>https://k8s.af/</p> <p>https://devopswithkubernetes.com/</p>"},{"location":"devops/containers/kubernetes/#tools","title":"Tools","text":"<p>https://k8slens.dev/</p>"},{"location":"devops/containers/kubernetes/#configure-credentials","title":"Configure credentials","text":"<pre><code>echo -n \"{REGISTRY_USERNAME}:{REGISTRY_PASSWORD}\" | base64\n</code></pre> <pre><code>{\n    \"auths\": {\n        \"https://registry.gitlab.com\":{\n            \"auth\":\"USER_PASS_B64\"\n        }\n    }\n}\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: registry-credentials\n  namespace: dev\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: BASE_64_ENCODED_DOCKER_FILE\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: default\n  namespace: dev\nimagePullSecrets:\n- name: registry-credentials\n</code></pre>"},{"location":"devops/containers/kubernetes/#top-command","title":"Top command","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\nkubectl top pods\nAdd - --kubelet-insecure-tls to metrics-server image\n</code></pre>"},{"location":"devops/containers/kubernetes/#old","title":"Old","text":"<p>Share information from pod to container</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: kubernetes-downwardapi-volume-example\n  labels:\n    zone: us-est-coast\n    cluster: test-cluster1\n    rack: rack-22\n  annotations:\n    build: two\n    builder: john-doe\nspec:\n  containers:\n    - name: client-container\n      image: k8s.gcr.io/busybox\n      command: [\"sh\", \"-c\"]\n      args:\n      - while true; do\n          if [[ -e /etc/podinfo/labels ]]; then\n            echo -en '\\n\\n'; cat /etc/podinfo/labels; fi;\n          if [[ -e /etc/podinfo/annotations ]]; then\n            echo -en '\\n\\n'; cat /etc/podinfo/annotations; fi;\n          sleep 5;\n        done;\n      volumeMounts:\n        - name: podinfo\n          mountPath: /etc/podinfo\n          readOnly: false\n  volumes:\n    - name: podinfo\n      downwardAPI:\n        items:\n          - path: \"labels\"\n            fieldRef:\n              fieldPath: metadata.labels\n          - path: \"annotations\"\n            fieldRef:\n              fieldPath: metadata.annotations\n</code></pre> <p>Lifecycle events</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: lifecycle-demo\nspec:\n  containers:\n  - name: lifecycle-demo-container\n    image: nginx\n    lifecycle:\n      postStart:\n        exec:\n          command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler &gt; /usr/share/message\"]\n      preStop:\n        exec:\n          command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"]\n</code></pre> <p>Select node to deploy pod</p> <pre><code>kubectl get nodes \nkubectl label nodes &lt;nodename&gt; disktype=ssd\nkubectl get nodes --show-labels\nnano dev-pod.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  nodeSelector:\n    disktype: ssd\n</code></pre> <p>Taints</p> <pre><code>kubectl get nodes\nkubectl taint nodes (nodename) env=dev:NoSchedule\nkubectl run nginx \u2014image=nginx\nkubectl label deployments/nginx env=dev\nkubectl scale deployments/nginx \u2014replicas=5\nkubectl get pods -o wide\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 7\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n      tolerations:\n      - key: \u201cdev\u201d\n        operator: \u201cEqual\u201d\n        value: \u201cenv\u201d\n        effect: \u201cNoSchedule\u201d\n</code></pre> <pre><code>A node selector affects a single pod template, asking the scheduler to place it on a set of nodes. A NoSchedule taint affects all pods asking the scheduler to block all pods from being scheduled there.\n\nA node selector is useful when the pod needs something from the node. For example, requesting a node that has a GPU. A node taint is useful when the node needs to be reserved for special workloads. For example, a node that should only be running pods that will use the GPU (so the GPU node isn't filled with pods that aren't using it).\n\nSometimes they are useful together as in the example above, too. You want the node to only have pods that use the GPU, and you want the pod that needs a GPU to be scheduled to a GPU node. In that case you may want to taint the node with dedicated=gpu:NoSchedule and add both a taint toleration and node selector to the pod template.\n</code></pre>"},{"location":"devops/containers/kubernetes/#_1","title":"Kubernetes","text":""},{"location":"devops/public-clouds/gcloud/","title":"gcloud","text":""},{"location":"devops/public-clouds/gcloud/#install","title":"Install","text":"<ul> <li>Download zip</li> <li>Unzip and move to folder</li> <li>Add bin to PATH</li> </ul>"},{"location":"devops/public-clouds/gcloud/#debian-ubuntu","title":"Debian / Ubuntu","text":"<pre><code>echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\nsudo apt-get update &amp;&amp; sudo apt-get install google-cloud-sdk\n</code></pre>"},{"location":"devops/public-clouds/gcloud/#commands","title":"Commands","text":"<p>Install kubectl</p> <pre><code>gcloud components install kubectl\n</code></pre> <p>Authenticate</p> <pre><code>gcloud auth login\n</code></pre> <p>Authenticate applications</p> <pre><code>gcloud auth application-default login\n</code></pre> <p>Configure docker</p> <pre><code>gcloud auth configure-docker\n</code></pre> <p>Show current user</p> <pre><code>gcloud auth list\n</code></pre> <p>Configure basic data</p> <pre><code>gcloud config set compute/zone us-central1-a\ngcloud config set compute/region us-central1\ngcloud config set project generated-area-284017\n</code></pre> <p>Show config</p> <pre><code>gcloud config list\n</code></pre> <p>Create cluster</p> <pre><code>gcloud container clusters create test-cluster --num-nodes 1\n</code></pre> <p>Resize cluster</p> <pre><code>gcloud container clusters resize test-cluster --num-nodes=0\n</code></pre> <p>Delete cluster</p> <pre><code>gcloud container clusters delete test-cluster\n</code></pre> <p>Upgrade cluster</p> <pre><code>gcloud container get-server-config\ngcloud container clusters upgrade cluster-name --master --cluster-version cluster-version\n</code></pre> <p>Create disk</p> <pre><code>gcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk\n</code></pre> <p>Show resources</p> <pre><code>gcloud compute instances list\ngcloud container clusters list\ngcloud compute disks list\n</code></pre> <p>Upload docker image</p> <pre><code>docker tag [SOURCE_IMAGE] [HOSTNAME]/[PROJECT-ID]/[IMAGE]:[TAG]\ndocker push [HOSTNAME]/[PROJECT-ID]/[IMAGE]:[TAG]\n</code></pre> <p>List docker iamges</p> <pre><code>gcloud container images list\ngcloud container images list --repository eu.gcr.io/generated-area-284017\n</code></pre> <p>Delete docker image</p> <pre><code>gcloud container images delete &lt;IMAGE_NAME&gt; --force-delete-tags  --quiet\n</code></pre>"},{"location":"devops/service-mesh/istio/","title":"Istio","text":""},{"location":"devops/service-mesh/istio/#install","title":"Install","text":"<ul> <li>Download zip</li> <li>Unzip and move to folder</li> <li>Add bin to PATH</li> </ul>"},{"location":"devops/service-mesh/istio/#commands","title":"Commands","text":"<p>Install</p> <pre><code>istioctl install --set profile=demo\n</code></pre> <p>Show pods</p> <pre><code>kubectl get pods -n istio-system\n</code></pre> <p>Uninstall </p> <pre><code>istioctl x uninstall --purge\nkubectl delete namespace istio-system\n</code></pre> <p>Enable injection</p> <pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre>"},{"location":"devops/service-mesh/istio/#sample-app-components","title":"Sample app components","text":""},{"location":"devops/service-mesh/istio/#service","title":"Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\n  labels:\n    app: myapp\n    service: myapp\nspec:\n  ports:\n  - port: 9080\n    name: http\n  selector:\n    app: myapp\n</code></pre> <p>Remember that default type is <code>ClusterIP</code></p>"},{"location":"devops/service-mesh/istio/#serviceaccount","title":"ServiceAccount","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: myapp-sa\n  labels:\n    account: myapp\n</code></pre>"},{"location":"devops/service-mesh/istio/#deployment","title":"Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-v1\n  labels:\n    app: myapp\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myapp\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: myapp\n        version: v1\n    spec:\n      serviceAccountName: myapp-sa\n      containers:\n      - name: myapp\n        image: myapp-image-url\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n</code></pre>"},{"location":"devops/service-mesh/istio/#networking-components","title":"Networking components","text":""},{"location":"devops/service-mesh/istio/#gateway","title":"Gateway","text":"<p>A <code>Gateway</code> configures a load balancer for HTTP/TCP traffic, regardless of where it will be running. Any number of gateways can exist within the mesh and multiple different gateway implementations can co-exist.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: my-gateway\nspec:\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n</code></pre> <p><code>hosts</code>can be a list of allowed hosts (for example <code>myhost.com</code>) or <code>*</code> for any.</p>"},{"location":"devops/service-mesh/istio/#virtualservice","title":"VirtualService","text":"<p>A <code>VirtualService</code> describes the mapping between one or more user-addressable destinations to the actual destination workloads inside the mesh.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: my-service\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - my-gateway\n  http:\n  - match:\n    - uri:\n        exact: /test\n    - uri:\n        prefix: /myapp/v1\n    route:\n      - destination:\n        host: myapp\n        subset: v1\n        port:\n          number: 9080\n  - match:\n    - uri:\n      prefix: /myapp/v2\n    route:\n      - destination:\n        host: myapp\n        subset: v2\n</code></pre> <p><code>hosts</code> : The destination hosts to which traffic is being sent.</p> <p>We can configure weigths</p> <pre><code>spec:\n  hosts:\n  - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n      weight: 75\n    - destination:\n        host: reviews\n        subset: v2\n      weight: 25\n</code></pre>"},{"location":"devops/service-mesh/istio/#destinationrule","title":"DestinationRule","text":"<p>If we use <code>subset</code>in the <code>VirtualService</code> we need to define the <code>DestinationRule</code></p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: my-destination-rule\nspec:\n  host: myapp # myapp.default.svc.cluster.local\n  trafficPolicy:\n    loadBalancer:\n      simple: RANDOM\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n    trafficPolicy:\n      loadBalancer:\n        simple: ROUND_ROBIN\n  - name: v3\n    labels:\n      version: v3\n</code></pre>"},{"location":"devops/service-mesh/istio/#sidecar","title":"Sidecar","text":"<p>By default, Istio configures every Envoy proxy to accept traffic on all the ports of its associated workload, and to reach every workload in the mesh when forwarding traffic. You can use a sidecar configuration to do the following:</p> <ul> <li>Fine-tune the set of ports and protocols that an Envoy proxy accepts.</li> <li>Limit the set of services that the Envoy proxy can reach.</li> </ul>"},{"location":"devops/service-mesh/kuma/","title":"Kuma","text":""},{"location":"devops/service-mesh/kuma/#install","title":"Install","text":"<pre><code>curl -L https://kuma.io/installer.sh | sh -\nPATH=$PATH:~/kuma-0.7.1/bin\n</code></pre>"},{"location":"devops/service-mesh/kuma/#comands","title":"Comands","text":"<p>Install contrlo plane</p> <pre><code>kumactl install control-plane | kubectl apply -f -\n</code></pre> <p>Open dashboard</p> <pre><code>kubectl port-forward svc/kuma-control-plane -n kuma-system 5681:5681\n</code></pre> <p>http://127.0.0.1:5681/gui</p> <p>Install metrics</p> <pre><code>kumactl install metrics | kubectl apply -f -\n</code></pre> <p>Open metrics</p> <pre><code>kubectl port-forward svc/grafana -n kuma-metrics 3000:80\n</code></pre> <p>http://127.0.0.1:3000/</p> <pre><code>echo \"apiVersion: kuma.io/v1alpha1\nkind: Mesh\nmetadata:\n  name: default\nspec:\n  mtls:\n    enabledBackend: ca-1\n    backends:\n    - name: ca-1\n      type: builtin\" | kubectl apply -f -\n</code></pre> <pre><code>echo \"apiVersion: kuma.io/v1alpha1\nkind: TrafficPermission\nmesh: default\nmetadata:\n  namespace: default\n  name: all-traffic-allowed\nspec:\n  sources:\n    - match:\n        kuma.io/service: '*'\n  destinations:\n    - match:\n        kuma.io/service: '*'\" | kubectl apply -f -\n</code></pre> <pre><code>echo \"apiVersion: kuma.io/v1alpha1\nkind: Mesh\nmetadata:\n  name: default\nspec:\n  mtls:\n    enabledBackend: ca-1\n    backends:\n    - name: ca-1\n      type: builtin\n  metrics:\n    enabledBackend: prometheus-1\n    backends:\n    - name: prometheus-1\n      type: prometheus\" | kubectl apply -f -\n</code></pre>"},{"location":"devops/service-mesh/linkerd/","title":"Linkerd","text":""},{"location":"devops/service-mesh/linkerd/#install-client","title":"Install client","text":"<pre><code>curl -sL https://run.linkerd.io/install | sh\nexport PATH=$PATH:$HOME/.linkerd2/bin\nlinkerd version\n</code></pre>"},{"location":"devops/service-mesh/linkerd/#install-in-cluster","title":"Install in cluster","text":"<pre><code>linkerd check --pre\nlinkerd install | kubectl apply -f -\nlinkerd install --proxy-auto-inject | kubectl apply -f -\nlinkerd check\n</code></pre>"},{"location":"devops/service-mesh/linkerd/#access-dashboards","title":"Access dashboards","text":"<pre><code>linkerd dashboard &amp;\n</code></pre>"},{"location":"devops/service-mesh/linkerd/#adding-service-to-linkerd","title":"Adding service to Linkerd","text":"<p>In order for your services to take advantage of Linkerd, they need to be \"added to the mesh\" by having Linkerd's data plane proxy injected into their pods. This is typically done by annotating the namespace, deployment, or pod with the <code>linkerd.io/inject: enabled</code> Kubernetes annotation. This annotation triggers automatic proxy injection when the resources are created.</p> <pre><code>kubectl label namespace default linkerd.io/inject=enabled\n</code></pre> <pre><code>cat deployment.yml | linkerd inject - | kubectl apply -f -\n</code></pre>"},{"location":"devops/service-mesh/linkerd/#todo","title":"TODO","text":"<p>https://linkerd.io/2/tasks/exposing-dashboard/</p> <p>https://linkerd.io/2/tasks/distributed-tracing/</p> <p>Namespace Injection BUG</p> <p>Production install</p> <p>https://blog.intenseye.com/service-mesh-with-linkerd2-part-1/</p> <p>https://blog.intenseye.com/service-mesh-with-linkerd2-part-2/</p> <p>https://blog.polymatic.systems/service-mesh-wars-goodbye-istio-b047d9e533c7</p>"},{"location":"languages/english/","title":"English","text":""},{"location":"languages/english/#links","title":"Links","text":"<p>Vite listenings</p>"},{"location":"optimization/optimization/","title":"Optimization","text":""},{"location":"optimization/optimization/#links","title":"Links","text":"<p>HikariCP Pool Sizing</p>"},{"location":"other/algorithms/","title":"Algorithms","text":""},{"location":"other/algorithms/#investigate","title":"Investigate","text":"<p>https://www.baeldung.com/cs/category/algorithms/page/7</p> <p>https://llimllib.github.io/bloomfilter-tutorial/</p>"},{"location":"other/commands/","title":"Commands","text":"<p>Generate ssh keys</p> <pre><code>ssh-keygen -C \"name\" -t rsa\n</code></pre> <p>Copy public key to server</p> <pre><code>~/.ssh/authorized_keys\n</code></pre>"},{"location":"other/developer-rules/","title":"Developer rules","text":"<ul> <li>Avoid keep unfinished branches</li> <li>Early test with real data</li> <li>Automate repetitive tasks</li> <li>Persist intermediate data</li> <li>Group data in embedded sub-entities</li> <li>Use TODO and FIXME</li> </ul>"},{"location":"other/developer-rules/#common-bugs","title":"Common bugs","text":"<ul> <li>Caching all values: Solved by using conditions about what values are valid to be cached</li> <li>Forget to configure timezone: Solved by explicitly configuring the timezone</li> <li>Avoid to validate in local because is complex: Solved by having a friendly local app config</li> <li>Mistake variables with similar names: Solved by reviewing code before commit and merge</li> <li>Not having automated CI tasks: Solved by defining CI jobs</li> </ul>"},{"location":"other/explore/","title":"Explore","text":""},{"location":"other/explore/#tech","title":"Tech","text":"<p>https://nats.io/</p> <p>https://dzone.com/articles/spring-tips-apache-rocketmq</p> <p>https://www.openfaas.com/blog/lambda-to-openfaas/</p> <p>https://landscape.cncf.io/</p>"},{"location":"other/git/","title":"Git","text":""},{"location":"other/git/#messages","title":"Messages","text":"<ul> <li>Use present verb</li> <li>Avoid dots</li> <li>Max 50 characters</li> <li>Use body (git commit -m \"Add summary of commit\" -m \"This is a message to add more context.\")</li> <li>Use semantic commits (type: description)</li> </ul>"},{"location":"other/git/#ssh","title":"SSH","text":"<p>Generate key</p> <pre><code>ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_name\nssh-keygen -t rsa -b 4096 -P \"pass\" -f ~/.ssh/id_name\neval \"$(ssh-agent)\" ssh-add ~/.ssh/id_name.pub\n</code></pre> <p>Configure key In <code>~/.ssh/config</code></p> <pre><code>Host gitlab.com\n Hostname gitlab.com\n User user\n IdentityFile ~/.ssh/id_name\n\nHost gitlab.com\n Hostname gitlab.com\n User user2\n IdentityFile ~/.ssh/id_name2\n</code></pre>"},{"location":"other/git/#sourcetree","title":"Sourcetree","text":"<ul> <li>In <code>Tools</code> &gt; <code>Options</code> &gt; <code>General</code> configuration select OpenSSH as SSH client</li> <li>In <code>Tools</code> &gt; <code>Add SSH</code> Key we add the private key</li> </ul>"},{"location":"other/git/#repository-management","title":"Repository management","text":"<ul> <li>Keep one branch for each minor version with the version as name like <code>2.1.x</code></li> <li>Keep in the <code>main</code>  all the changes</li> <li>Tag each version with vX.X.X or vX.X.X-RCX (Release candidate) or vX.X.X-MX (Milestone)</li> <li>Only tag in release branches and then remove they</li> </ul>"},{"location":"other/git/#example","title":"Example:","text":""},{"location":"other/git/#first-version","title":"First version","text":"<ol> <li>Start in <code>main</code> branch with the version <code>1.0.0-SNAPSHOT</code></li> <li>Add changes to branch <code>main</code> that now has the version <code>1.0.0-SNAPSHOT</code></li> <li>Create the branch <code>release/1.0.0</code> from the branch <code>main</code></li> <li>Update the branch <code>release/1.0.0</code> to have the version <code>1.0.0</code></li> <li>Tag the branch <code>release/1.0.0</code> with the tag <code>v1.0.0</code> and remove this branch</li> <li>Update the branch <code>main</code> with the version <code>1.0.1-SNAPSHOT</code> and commit message <code>Next development version (v1.0.1-SNAPSHOT)</code></li> </ol>"},{"location":"other/git/#multiple-versions","title":"Multiple versions","text":"<ol> <li>Create the branch <code>1.0.x</code> from the branch <code>main</code></li> <li>Update the <code>main</code> branch with the version <code>1.1.0-SNAPSHOT</code></li> </ol>"},{"location":"other/git/#procedures","title":"Procedures","text":""},{"location":"other/git/#new-hotfix-release","title":"New hotfix release","text":"<p>Create new commit only removing the SNAPSHOT from the branch and tag it Update the branch to the next hotfix version</p> <p>Merge branch 2.1.x into 2.2.x Merge branch 2.2.x</p>"},{"location":"other/git/#common-operations","title":"Common operations","text":""},{"location":"other/git/#partial-merge","title":"Partial merge","text":"<pre><code>git checkout branch_where_changes_applied\ngit merge --no--ff --no-commit branche_to_merge\n</code></pre>"},{"location":"other/git/#partial-merge-squash","title":"Partial merge squash","text":"<pre><code>git checkout branch_where_changes_applied\ngit merge --squash --no-commit branche_to_merge\n</code></pre>"},{"location":"other/git/#squash-all-commits-in-one-branch","title":"Squash all commits in one branch","text":"<pre><code>git checkout feature_branch\ngit reset --soft parent_branch\ngit add .\ngit commit -m \"Message\"\n</code></pre>"},{"location":"other/git/#squash-last-n-commits","title":"Squash last N commits","text":"<pre><code>git checkout feature_branch\ngit reset --soft HEAD~N\ngit add .\ngit commit -m \"Message\"\n</code></pre>"},{"location":"other/git/#force-push","title":"Force push","text":"<pre><code>git push --force origin branch_name\n</code></pre>"},{"location":"other/git/#rebase","title":"Rebase","text":"<pre><code>git rebase &lt;base&gt;\n</code></pre>"},{"location":"other/git/#documentation","title":"Documentation","text":"<p>Gitflow</p> <p>Flight rules</p> <p>Changelog</p>"},{"location":"other/github/","title":"Github","text":""},{"location":"other/github/#procedures","title":"Procedures","text":""},{"location":"other/github/#pull-request","title":"Pull request","text":"<ol> <li>Fork the repository</li> <li>Pull</li> <li>Go to correct branch</li> <li>Commit and push</li> <li>Go to web and do pull request</li> </ol>"},{"location":"other/hexagonal-architecture/","title":"Hexagonal architecture","text":""},{"location":"other/hexagonal-architecture/#introduction","title":"Introduction","text":""},{"location":"other/hexagonal-architecture/#kotlin","title":"Kotlin","text":"<pre><code>.\n+-- root.package\n|   +-- App.kt\n|   +-- commons\n|   +-- config\n|   +-- app\n|       +-- inbound\n|       +-- outbound\n|       +-- domain\n|           +-- models\n|           +-- services\n|           +-- providers\n|           +-- repositories\n</code></pre> <p>Mapping functions are declared with extension functions</p> <pre><code>fun Domain.toResource() = Resource(\n    resourceValue = this.domainValue,\n)\n</code></pre> <p>Use <code>val</code> in all attributes and the method <code>copy()</code> to perform mutations</p> <pre><code>data class Counter(val value: Int)\nval newCounter = counter.copy(value = 0)\n</code></pre> <p>https://www.baeldung.com/hexagonal-architecture-ddd-spring</p>"},{"location":"other/publish-maven-central/","title":"Publish maven central","text":"<ul> <li> <p>Create a user in Sonartype.org (https://issues.sonatype.org/secure/Signup!default.jspa)</p> </li> <li> <p>Request ownership of group</p> </li> <li> <p>Open a issue (https://issues.sonatype.org/secure/CreateIssue.jspa?issuetype=21&amp;pid=10134)</p> <p><code>Summary: Various java projects under \"&lt;GROUP:ID&gt;\" group id Description: Add links to project GroupId: &lt;GROUP_ID&gt; Project URL: &lt;REPO_URL&gt; SCM URL: &lt;REPO_URL&gt;</code></p> </li> <li> <p>Meet requirments (https://central.sonatype.org/publish/requirements/)</p> </li> </ul> <p>```xml   </p> <p> 4.0.0 <pre><code>  &lt;groupId&gt;io.github.devdevx&lt;/groupId&gt;\n  &lt;artifactId&gt;nothing&lt;/artifactId&gt;\n  &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;\n  &lt;packaging&gt;jar&lt;/packaging&gt;\n\n  &lt;name&gt;${project.groupId}:${project.artifactId}&lt;/name&gt;\n  &lt;description&gt;A demo for deployment to the Central Repository via OSSRH&lt;/description&gt;\n  &lt;url&gt;https://github.com/devdevx/nothing&lt;/url&gt;\n\n  &lt;licenses&gt;\n      &lt;license&gt;\n          &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt;\n          &lt;url&gt;https://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt;\n      &lt;/license&gt;\n  &lt;/licenses&gt;\n\n  &lt;developers&gt;\n      &lt;developer&gt;\n          &lt;id&gt;author&lt;/id&gt;\n          &lt;name&gt;Adrian Palanques&lt;/name&gt;\n          &lt;email&gt;devdevx@mail.com&lt;/email&gt;\n      &lt;/developer&gt;\n  &lt;/developers&gt;\n\n  &lt;scm&gt;\n      &lt;connection&gt;scm:git:git://github.com/devdevx/nothing.git&lt;/connection&gt;\n      &lt;developerConnection&gt;scm:git:ssh://github.com:devdevx/nothing.git&lt;/developerConnection&gt;\n      &lt;url&gt;https://github.com/devdevx/nothing/tree/master&lt;/url&gt;\n  &lt;/scm&gt;\n\n  &lt;distributionManagement&gt;\n      &lt;snapshotRepository&gt;\n          &lt;id&gt;ossrh&lt;/id&gt;\n          &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;\n      &lt;/snapshotRepository&gt;\n      &lt;repository&gt;\n          &lt;id&gt;ossrh&lt;/id&gt;\n          &lt;url&gt;https://oss.sonatype.org/service/local/staging/deploy/maven2/&lt;/url&gt;\n      &lt;/repository&gt;\n  &lt;/distributionManagement&gt;\n\n  &lt;properties&gt;\n      &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;\n      &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;\n\n      &lt;maven-source-plugin.version&gt;3.2.1&lt;/maven-source-plugin.version&gt;\n      &lt;maven-javadoc-plugin.version&gt;3.2.0&lt;/maven-javadoc-plugin.version&gt;\n      &lt;maven-gpg-plugin.version&gt;1.6&lt;/maven-gpg-plugin.version&gt;\n      &lt;maven-enforcer-plugin.version&gt;3.0.0-M3&lt;/maven-enforcer-plugin.version&gt;\n  &lt;/properties&gt;\n\n  &lt;prerequisites&gt;\n      &lt;maven&gt;3.6.0&lt;/maven&gt;\n  &lt;/prerequisites&gt;\n\n  &lt;build&gt;\n      &lt;plugins&gt;\n          &lt;plugin&gt;\n              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n              &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt;\n              &lt;version&gt;${maven-source-plugin.version}&lt;/version&gt;\n              &lt;executions&gt;\n                  &lt;execution&gt;\n                      &lt;id&gt;attach-sources&lt;/id&gt;\n                      &lt;goals&gt;\n                          &lt;goal&gt;jar-no-fork&lt;/goal&gt;\n                      &lt;/goals&gt;\n                  &lt;/execution&gt;\n              &lt;/executions&gt;\n          &lt;/plugin&gt;\n          &lt;plugin&gt;\n              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n              &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt;\n              &lt;version&gt;${maven-javadoc-plugin.version}&lt;/version&gt;\n              &lt;executions&gt;\n                  &lt;execution&gt;\n                      &lt;id&gt;attach-javadocs&lt;/id&gt;\n                      &lt;goals&gt;\n                          &lt;goal&gt;jar&lt;/goal&gt;\n                      &lt;/goals&gt;\n                  &lt;/execution&gt;\n              &lt;/executions&gt;\n          &lt;/plugin&gt;\n          &lt;plugin&gt;\n              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n              &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt;\n              &lt;version&gt;${maven-gpg-plugin.version}&lt;/version&gt;\n              &lt;executions&gt;\n                  &lt;execution&gt;\n                      &lt;id&gt;sign-artifacts&lt;/id&gt;\n                      &lt;phase&gt;verify&lt;/phase&gt;\n                      &lt;goals&gt;\n                          &lt;goal&gt;sign&lt;/goal&gt;\n                      &lt;/goals&gt;\n                  &lt;/execution&gt;\n              &lt;/executions&gt;\n          &lt;/plugin&gt;\n          &lt;plugin&gt;\n              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n              &lt;artifactId&gt;maven-enforcer-plugin&lt;/artifactId&gt;\n              &lt;version&gt;${maven-enforcer-plugin.version}&lt;/version&gt;\n              &lt;executions&gt;\n                  &lt;execution&gt;\n                      &lt;id&gt;enforce-maven&lt;/id&gt;\n                      &lt;goals&gt;\n                          &lt;goal&gt;enforce&lt;/goal&gt;\n                      &lt;/goals&gt;\n                      &lt;configuration&gt;\n                          &lt;rules&gt;\n                              &lt;requireMavenVersion&gt;\n                                  &lt;version&gt;3.6.0&lt;/version&gt;\n                              &lt;/requireMavenVersion&gt;\n                          &lt;/rules&gt;\n                          &lt;fail&gt;true&lt;/fail&gt;\n                      &lt;/configuration&gt;\n                  &lt;/execution&gt;\n              &lt;/executions&gt;\n          &lt;/plugin&gt;\n      &lt;/plugins&gt;\n  &lt;/build&gt;\n</code></pre> <p>   ```</p> <ul> <li> <p>Generate GPD key (https://central.sonatype.org/publish/requirements/gpg/)</p> </li> <li> <p>Windows</p> <ul> <li> <p>Download from https://www.gnupg.org/download/</p> </li> <li> <p>Install only GnuPG</p> </li> <li> <p>Commands</p> </li> </ul> <p><code>gpg --list-keys   gpg --list-secret-keys   gpg --generate-key   gpg --delete-secret-key &lt;ID&gt;   gpg --delete-key &lt;ID&gt;   gpg -a --export &lt;ID&gt; &gt; key.pub   gpg --import key.pub   gpg --export-secret-keys &lt;ID&gt; &gt; key.priv   gpg --gen-revoke &lt;ID&gt; &gt; key.revoc   gpg --import key.revoc   gpg --keyserver keys.openpgp.org --send-keys &lt;ID&gt;</code></p> </li> <li> <p>Package</p> </li> </ul> <p><code>mvn package -Dgpg.passphrase=XXX</code></p> <ul> <li>Configure credentials</li> </ul> <p><code>&lt;servers&gt;        &lt;server&gt;           &lt;id&gt;ossrh&lt;/id&gt;           &lt;username&gt;user&lt;/username&gt;           &lt;password&gt;pass&lt;/password&gt;       &lt;/server&gt;   &lt;/servers&gt;</code></p> <ul> <li>Deploy</li> </ul> <p><code>mvn deploy -Dgpg.passphrase=XXX</code></p> <ul> <li> <p>Relese the artifacts</p> </li> <li> <p>Login in https://s01.oss.sonatype.org/</p> </li> <li>Go to Staging repositories</li> <li>Close repository</li> <li> <p>Release repository</p> </li> <li> <p>Validate in https://repo1.maven.org/maven2/</p> </li> </ul>"},{"location":"other/samples/","title":"Samples","text":""},{"location":"other/samples/#code","title":"Code","text":"<p><code>py title=\"bubble_sort.py\" def bubble_sort(items):     for i in range(len(items)):         for j in range(len(items) - 1 - i): # (1)             if items[j] &gt; items[j + 1]:                 items[j], items[j + 1] = items[j + 1], items[j]</code></p> <ol> <li>I'm a code annotation! I can contain <code>code</code>, formatted    text, images, ... basically anything that can be written in Markdown.</li> </ol> <p>Other thing.</p>"},{"location":"other/services/","title":"Online services","text":""},{"location":"other/services/#elasticsearch-kibana","title":"Elasticsearch + Kibana","text":"<ul> <li>Bonsai.io</li> </ul>"},{"location":"other/tooling/","title":"Tooling","text":""},{"location":"other/tooling/#windows","title":"Windows","text":""},{"location":"other/tooling/#wsl-2","title":"WSL 2","text":""},{"location":"other/tooling/#installation","title":"Installation","text":"<pre><code>dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\ndism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\nshutdown \u2013r\nwsl --set-default-version 2\n</code></pre>"},{"location":"other/tooling/#install-linux-distro-subsystem","title":"Install linux distro subsystem","text":"<p>All distros are here: https://docs.microsoft.com/en-us/windows/wsl/install-manual</p> <pre><code>curl -o ubuntu.appx https://aka.ms/wslubuntu2004\nAdd-AppxPackage .\\ubuntu.appx\n</code></pre> <p>Go to Start menu, search for the distro an click to set user and password.</p>"},{"location":"other/tooling/#commands","title":"Commands","text":"<p>List all distros</p> <pre><code>wsl -l -v\n</code></pre> <p>Set default distro</p> <pre><code>wsl -s Ubuntu-20.04\n</code></pre> <p>Run default distro in current directory</p> <pre><code>wsl\n</code></pre>"},{"location":"other/tooling/#docker","title":"Docker","text":""},{"location":"other/tooling/#installation_1","title":"Installation","text":"<p>Install Docker Desktop for Windows.</p>"},{"location":"other/tooling/#configuration","title":"Configuration","text":""},{"location":"other/tooling/#general","title":"General","text":"<p>Enable \"Use the WSL 2 base engine\"</p>"},{"location":"other/tooling/#resources-wsl-integration","title":"Resources &gt; WSL INTEGRATION","text":"<p>Enable \"Enable Integration with mt default WSL distro\"</p>"},{"location":"other/tooling/#kubernetes","title":"Kubernetes","text":"<p>Disable \"Enable Kubernetes\"</p>"},{"location":"other/tooling/#useful-info","title":"Useful info","text":"<p>When using WSL integration, docker create two distros.</p> <pre><code>\\\\wsl$\\docker-desktop\n\\\\wsl$\\docker-desktop-data\n</code></pre>"},{"location":"other/tooling/#instellij","title":"InstelliJ","text":""},{"location":"other/tooling/#plugins","title":"Plugins","text":"<ul> <li>Indent Rainbow</li> <li>Lombok</li> <li>Maven Helper</li> <li>Rainbow Brackets</li> <li>SonarLint</li> <li>VisualVM Launcher</li> <li>Go</li> <li>Rust</li> <li>Python</li> <li>Key Promoter X</li> <li>Kotlin</li> <li>Kotlin Fill Class</li> <li>Native Debugging Support</li> <li>Request Mapper</li> <li>RoboPOJOGenerator</li> <li>Toml</li> <li>OpenAPI (Swagger) Editor</li> <li>Detekt</li> </ul>"},{"location":"other/tooling/#sdkman","title":"SDKMAN!","text":""},{"location":"other/tooling/#validate-prerequisites","title":"Validate prerequisites","text":"<pre><code>which zip\nwhich unzip\nwhich curl\nwhich tar\nwhich gzip\n</code></pre>"},{"location":"other/tooling/#install-zip","title":"Install <code>zip</code>","text":"<ol> <li>Enter here</li> <li>Download <code>zip-3.0-bin.zip</code> and <code>bzip2-1.0.5-bin.zip</code></li> <li>Extract <code>zip.exe</code> and <code>bzip2.dll</code> from the bin`folders</li> <li>Copy the files to <code>C:\\Program Files\\Git\\mingw64\\bin</code></li> </ol>"},{"location":"other/tooling/#disable-curl-ssl","title":"Disable <code>curl</code> SSL","text":"<pre><code>echo \"insecure\" &gt; ~/.curlrc\n</code></pre>"},{"location":"other/tooling/#install","title":"Install","text":"<pre><code>curl -s \"https://get.sdkman.io\" | bash\n</code></pre>"},{"location":"other/tooling/#common-commands","title":"Common commands","text":"<p>https://sdkman.io/usage</p>"},{"location":"other/tooling/#dbeaver","title":"DBeaver","text":""},{"location":"other/tooling/#sourcetree","title":"SourceTree","text":""},{"location":"other/tooling/#compass","title":"Compass","text":""},{"location":"other/tooling/#insomnia","title":"Insomnia","text":""},{"location":"other/tooling/#postman","title":"Postman","text":""},{"location":"other/tooling/#dive","title":"Dive","text":""},{"location":"programming/programming/","title":"Programming","text":""},{"location":"programming/programming/#links","title":"Links","text":"<p>https://codesignal.com/developers/ https://github.com/ByteByteGoHq/system-design-101 https://uicolors.app/create (ULID Optimization)[https://www.youtube.com/watch?v=f53-Iw_5ucA&amp;t=5s]</p>"},{"location":"programming/golang/golang/","title":"Golang","text":""},{"location":"programming/golang/golang/#links","title":"Links","text":"<p>Tutorial Hexagonal Architecture API MongoDB and Chi API Rest APIs Clean architecture</p>"},{"location":"programming/golang/golang/#commands","title":"Commands","text":"<pre><code>go mod init &lt;project_name&gt;\n</code></pre> <pre><code>go run &lt;file&gt;\n</code></pre>"},{"location":"programming/golang/golang/#dependencies","title":"Dependencies","text":"<p>Gin</p> <p>Kin-Openapi</p> <p>https://github.com/golang/mock</p> <p>https://gorm.io/docs/</p> <p>https://github.com/ulule/deepcopier</p> <p>https://medium.com/@matryer/api-responses-in-go-1ef8f7b74997</p>"},{"location":"programming/golang/golang/#learn","title":"Learn","text":"<p>https://gobyexample.com/</p> <p>https://awesome-go.com/</p> <p>https://learning-cloud-native-go.github.io/docs/a5.adding_chi_router/</p> <p>https://astaxie.gitbooks.io/build-web-application-with-golang/content/en/11.1.html</p> <p>https://tour.golang.org/basics/1</p> <p>https://rakyll.org/style-packages/</p> <p>https://medium.com/@jfeng45</p> <p>https://www.gobeyond.dev/tag/application-design/</p> <p>https://github.com/spy16/droplets</p> <p>https://www.wolfe.id.au/2020/03/10/how-do-i-structure-my-go-project/</p> <p>https://medium.com/full-stack-tips/dependency-injection-in-go-99b09e2cc480</p> <p>https://blog.drewolson.org/dependency-injection-in-go</p> <p>https://blog.drewolson.org/go-dependency-injection-with-wire</p> <p>https://medium.com/scum-gazeta/golang-production-ready-solution-e82fad132dd9</p> <p>https://eminetto.medium.com/clean-architecture-using-golang-b63587aa5e3f</p> <p>https://medium.com/@jfeng45/go-microservice-with-clean-architecture-transaction-support-61eb0f886a36</p>"},{"location":"programming/golang/golang/#strategies","title":"Strategies","text":"<p>https://opencredo.com/blogs/why-i-dont-like-error-handling-in-go/</p> <p>https://adrianwit.medium.com/abstract-class-reinvented-with-go-4a7326525034</p> <p>https://mohewedy.medium.com/echo-framework-middleware-and-audited-gorm-integration-667fd0987c48</p> <p>https://medium.com/@ozdemir.zynl/rest-api-error-handling-in-go-behavioral-type-assertion-509d93636afd</p>"},{"location":"programming/java/java/","title":"Java","text":""},{"location":"programming/java/java/#procedures","title":"Procedures","text":""},{"location":"programming/java/java/#configure-multiple-versions","title":"Configure multiple versions","text":"<ol> <li>Install all the versions that you want</li> <li>Set the <code>JAVA_HOME</code> env var to the most recent and stable version</li> <li>Remove from the <code>Path</code> env var all the paths related to java</li> <li>Add the <code>%JAVA_HOME%\\bin</code> to <code>Path</code> env var</li> <li>Create a script for each configurable version ```` title=\"java17.bat\" @echo off set JAVA_HOME=C:\\Program Files\\Java\\jdk-17 set Path=%JAVA_HOME%\\bin;%Path% echo Java 17 activated.</li> </ol> <pre><code>6. Add the script folder to `Path` env var \n7. Execute the file name of the script each time you want to change the version\n\n### Detect memory leak\n\n#### VisualVM\n\n##### Remote Java process\n\nStart program with:\n\n````bash\njava\n-Dcom.sun.management.jmxremote\n-Dcom.sun.management.jmxremote.port=9010\n-Dcom.sun.management.jmxremote.rmi.port=9010\n-Dcom.sun.management.jmxremote.local.only=false\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n-jar app.jar\n</code></pre>"},{"location":"programming/java/java/#libraries","title":"Libraries","text":"<p>Openhtmltopdf: HTML to PDF (Repo)</p> <p>Moneta: Manage money (Repo)</p> <p>Apache POI: Manage EXCEL files</p> <p>Spring Boot: Framework (Repo)</p> <p>Liquibase: SQL Changelog (Repo)</p> <p>MongoCK: MongoDB Changelog (Repo)</p> <p>JSONassert (Repo)</p>"},{"location":"programming/java/java/#containerize","title":"Containerize","text":"<p>Good practices</p>"},{"location":"programming/java/java/#patterns","title":"Patterns","text":"<p>List</p>"},{"location":"programming/java/java/#releases","title":"Releases","text":""},{"location":"programming/java/java/#java-25-lts","title":"Java 25 (LTS)","text":""},{"location":"programming/java/java/#java-24","title":"Java 24","text":""},{"location":"programming/java/java/#java-23","title":"Java 23","text":""},{"location":"programming/java/java/#pattern-matching-for-primitive-types","title":"Pattern Matching for Primitive Types","text":"<pre><code>int x = 55;\nswitch (x) {\n    case 200 -&gt; System.out.println(\"OK\");\n    case 404 -&gt; System.out.println(\"Resource Not Found\");\n    case 500 -&gt; System.out.println(\"Internal Server Error\");\n    case int k -&gt; System.out.println(\"Unknown status: \" + k);\n}\n</code></pre>"},{"location":"programming/java/java/#java-22","title":"Java 22","text":""},{"location":"programming/java/java/#structured-concurrency","title":"Structured Concurrency","text":"<pre><code>import java.util.concurrent.*;\n\npublic void downloadFiles(List&lt;String&gt; fileUrls) throws InterruptedException, ExecutionException {\n    try (var executor = Executors.newVirtualThreadPerTaskExecutor()) {\n        List&lt;Future&lt;?&gt;&gt; futures = new ArrayList&lt;&gt;();\n        for (String url : fileUrls) {\n            futures.add(executor.submit(() -&gt; downloadFile(url)));\n        }\n        for (Future&lt;?&gt; future : futures) {\n            future.get();\n        }\n    }\n}\n</code></pre>"},{"location":"programming/java/java/#statements-before-super","title":"Statements Before Super","text":"<pre><code>public class Rectangle extends Shape {\n    private int width;\n    private int height;\n\n    public Rectangle(int width, int height, String color) {\n        this.width = width;\n        this.height = height;\n        super(color);\n  }\n}\n</code></pre>"},{"location":"programming/java/java/#unnamed-variables","title":"Unnamed Variables","text":"<pre><code>String result = switch (expression) {\n    case Integer _ -&gt; \"Integer\";\n    case String _ -&gt; \"String\";\n    default -&gt; \"Unknown\";\n};\n</code></pre>"},{"location":"programming/java/java/#string-literal-preview","title":"String Literal (Preview)","text":"<pre><code>String name = \"Bob\";\nString welcomeText = STR.\"Welcome \\{name}\";\n</code></pre> <pre><code>double price1 = 10.5;\ndouble price2 = 20.75;\nString result = FMT.\"$%.2f\\{price1} + $%.2f\\{price2} = $%.2f\\{price1 + price2}\";\n</code></pre>"},{"location":"programming/java/java/#java-21-lts","title":"Java 21 (LTS)","text":""},{"location":"programming/java/java/#record-patterns","title":"Record Patterns","text":"<pre><code>record Point(int x, int y) {}\n\npublic static int recordPattern(Object obj) {\n    if(obj instanceof Point(int x, int y)) {\n        return x+y;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"programming/java/java/#pattern-matching-for-switch","title":"Pattern Matching for switch","text":"<pre><code>double result = 0;\nswitch (account) {\n    case null -&gt; throw new RuntimeException(\"No account\");\n    case SavingsAccount sa -&gt; result = sa.getSavings();\n    case TermAccount ta -&gt; result = ta.getTermAccount();\n    case CurrentAccount ca -&gt; result = ca.getCurrentAccount();\n    default -&gt; result = account.getBalance();\n};\n</code></pre> <pre><code>String output = null;\nswitch(input) {\n    case null -&gt; output = \"NULL\";\n    case String s when \"Yes\".equalsIgnoreCase(s) -&gt; output = \"Yes\";\n    case String s when \"No\".equalsIgnoreCase(s) -&gt; output = \"No\";\n    case String s -&gt; output = \"Try Again\";\n}\nreturn output;\n</code></pre>"},{"location":"programming/java/java/#virtual-threads","title":"Virtual Threads","text":"<p>Virtual threads are lightweight threads with the purpose of reducing the effort of developing high-concurrent applications. Many virtual threads can share OS threads to run their code.</p> <pre><code>try(var executor = Executors.newVirtualThreadPerTaskExecutor()) {\n    IntStream.rangeClosed(1, 10_000).forEach(i -&gt; {\n        executor.submit(() -&gt; {\n            System.out.println(i);\n            try {\n                Thread.sleep(Duration.ofSeconds(1));\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n        });\n    });\n}\n</code></pre>"},{"location":"programming/java/java/#sequenced-collections","title":"Sequenced Collections","text":"<pre><code>interface SequencedCollection&lt;E&gt; extends Collection&lt;E&gt; {\n    SequencedCollection&lt;E&gt; reversed();\n    void addFirst(E);\n    void addLast(E);\n    E getFirst();\n    E getLast();\n    E removeFirst();\n    E removeLast();\n}\n</code></pre> <pre><code>interface SequencedSet&lt;E&gt; extends Set&lt;E&gt;, SequencedCollection&lt;E&gt; {\n    SequencedSet&lt;E&gt; reversed();\n}\n</code></pre> <pre><code>interface SequencedMap&lt;K, V&gt; extends Map&lt;K, V&gt; {\n    SequencedMap&lt;K, V&gt; reversed();\n    SequencedSet&lt;K&gt; sequencedKeySet();\n    SequencedCollection&lt;V&gt; sequencedValues();\n    SequencedSet&lt;Entry&lt;K, V&gt;&gt; sequencedEntrySet();\n    V putFirst(K, V);\n    V putLast(K, V);\n    Entry&lt;K, V&gt; firstEntry();\n    Entry&lt;K, V&gt; lastEntry();\n    Entry&lt;K, V&gt; pollFirstEntry();\n    Entry&lt;K, V&gt; pollLastEntry();\n}\n</code></pre>"},{"location":"programming/java/java/#java-17-lts","title":"Java 17 (LTS)","text":""},{"location":"programming/java/java/#pattern-matching","title":"Pattern matching","text":"<pre><code>if (obj instanceof String s) {\n    System.out.println(s.length());\n}\n</code></pre>"},{"location":"programming/java/java/#switch-expression","title":"Switch expression","text":"<pre><code>int day = 3;\nString result = switch (day) {\n    case 1 -&gt; \"Monday\";\n    case 2 -&gt; \"Tuesday\";\n    case 3 -&gt; \"Wednesday\";\n    default -&gt; \"Invalid day\";\n};\n</code></pre>"},{"location":"programming/java/java/#java-11-lts","title":"Java 11 (LTS)","text":""},{"location":"programming/java/java/#var","title":"<code>var</code>","text":"<pre><code>var value = \"the value\";\n</code></pre>"},{"location":"programming/java/java/#string-methods","title":"<code>String</code> methods","text":"<pre><code>String str = \" Java 11 \";\nstr.isBlank();\nstr.strip();\nstr.lines();\n</code></pre>"},{"location":"programming/java/java/#files-methods","title":"<code>Files</code> methods","text":"<pre><code>Files.writeString(Paths.get(\"example.txt\"), \"Hello, Java 11\");\n</code></pre>"},{"location":"programming/java/java/#http-client-api","title":"HTTP Client API","text":"<pre><code>HttpClient client = HttpClient.newHttpClient();\nHttpRequest request = HttpRequest.newBuilder()\n    .uri(URI.create(\"https://example.com\"))\n    .build();\nHttpResponse&lt;String&gt; response = client.send(request, HttpResponse.BodyHandlers.ofString());\n</code></pre>"},{"location":"programming/java/maven/","title":"Maven","text":""},{"location":"programming/java/maven/#dependencies","title":"Dependencies","text":"<p>Show dependency tree</p> <pre><code>mvn dependency:tree\n</code></pre> <p>Detect outdated dependencies</p> <p>Update dependencies versions</p> <pre><code>mvn versions:update-properties\nmvn versions:update-parent\n</code></pre>"},{"location":"programming/java/maven/#version","title":"Version","text":"<pre><code>mvn help:evaluate -Dexpression=project.version -q -DforceStdout\n</code></pre>"},{"location":"programming/java/maven/#publish-single-module-project","title":"Publish single module project","text":"<pre><code>    &lt;distributionManagement&gt;\n        &lt;snapshotRepository&gt;\n            &lt;id&gt;RESPOSITORY_ID&lt;/id&gt;\n            &lt;url&gt;REPOSITORY_URL&lt;/url&gt;\n        &lt;/snapshotRepository&gt;\n        &lt;repository&gt;\n            &lt;id&gt;RESPOSITORY_ID&lt;/id&gt;\n            &lt;url&gt;REPOSITORY_URL&lt;/url&gt;\n        &lt;/repository&gt;\n    &lt;/distributionManagement&gt;\n</code></pre> <pre><code>mvn clean deploy\n</code></pre>"},{"location":"programming/java/maven/#add-repository","title":"Add repository","text":"<pre><code>    &lt;repositories&gt;\n        &lt;repository&gt;\n            &lt;id&gt;RESPOSITORY_ID&lt;/id&gt;\n            &lt;url&gt;REPOSITORY_URL&lt;/url&gt;\n        &lt;/repository&gt;\n    &lt;/repositories&gt;\n</code></pre>"},{"location":"programming/java/maven/#release","title":"Release","text":"<pre><code>mvn release:prepare\n\nask for current v\nnew v\nand tag\n\nmvn release:perform\n</code></pre> <p>TODO: Try and document</p>"},{"location":"programming/java/maven/#sonar","title":"Sonar","text":"<pre><code>mvn verify sonar:sonar -Dsonar.host.url=XXX -Dsonar.login=TOKEN\n</code></pre>"},{"location":"programming/java/maven/#jacoco","title":"Jacoco","text":"<pre><code>&lt;plugin&gt;\n    &lt;groupId&gt;org.jacoco&lt;/groupId&gt;\n    &lt;artifactId&gt;jacoco-maven-plugin&lt;/artifactId&gt;\n    &lt;version&gt;0.8.7&lt;/version&gt;\n    &lt;executions&gt;\n        &lt;execution&gt;\n            &lt;id&gt;prepare-agent&lt;/id&gt;\n            &lt;goals&gt;\n                &lt;goal&gt;prepare-agent&lt;/goal&gt;\n            &lt;/goals&gt;\n        &lt;/execution&gt;\n        &lt;execution&gt;\n            &lt;id&gt;report&lt;/id&gt;\n            &lt;phase&gt;prepare-package&lt;/phase&gt;\n            &lt;goals&gt;\n                &lt;goal&gt;report&lt;/goal&gt;\n            &lt;/goals&gt;\n        &lt;/execution&gt;\n    &lt;/executions&gt;\n&lt;/plugin&gt;\n</code></pre> <pre><code>mvn verify\n</code></pre>"},{"location":"programming/java/maven/#dependency-check","title":"Dependency check","text":"<pre><code>        &lt;sonar.dependencyCheck.htmlReportPath&gt;target/dependency-check-report.html&lt;/sonar.dependencyCheck.htmlReportPath&gt;\n\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.owasp&lt;/groupId&gt;\n                &lt;artifactId&gt;dependency-check-maven&lt;/artifactId&gt;\n                &lt;version&gt;6.1.5&lt;/version&gt;\n                &lt;configuration&gt;\n                    &lt;failBuildOnCVSS&gt;6&lt;/failBuildOnCVSS&gt;\n                &lt;/configuration&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;check&lt;/goal&gt;\n                        &lt;/goals&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n</code></pre>"},{"location":"programming/java/maven/#settings-file","title":"Settings file","text":"<p>In <code>$HOME/.m2/settings.xml</code></p> <p>Configure repository credentials</p> <pre><code>&lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt;\n  &lt;servers&gt;\n    &lt;server&gt;\n      &lt;id&gt;REPOSITORY_ID&lt;/id&gt;\n      &lt;username&gt;USERNAME&lt;/username&gt;\n      &lt;password&gt;PASSWORD&lt;/password&gt;\n    &lt;/server&gt;\n  &lt;/servers&gt;\n&lt;/settings&gt;\n</code></pre>"},{"location":"programming/java/spring-boot/","title":"Spring Boot","text":""},{"location":"programming/java/spring-boot/#links","title":"Links","text":"<ul> <li>Spring Boot Academy</li> <li>SpringDeveloper YT channel</li> <li>Clean architecture</li> </ul>"},{"location":"programming/java/spring-boot/#recipes","title":"Recipes","text":""},{"location":"programming/java/spring-boot/#show-build-info-in-log-and-actuator-info-endpoint","title":"Show build info in log and actuator info endpoint","text":"<p>NOTE: Some values are not available until is executed from jar</p> <ol> <li>In <code>application.properties</code> configure</li> </ol> <pre><code>spring.config.import=optional:classpath:/git.properties\n\nspring.application.name=@project.name@\nspring.application.version=@project.version@\n\ninfo.app.name=@project.name@\ninfo.app.description=@project.description@\ninfo.app.version=@project.version@\n\nmanagement.info.java.enabled=true\nmanagement.info.build.enabled=true\nmanagement.info.env.enabled=true\nmanagement.info.defaults.enabled=true\nmanagement.info.git.enabled=true\nmanagement.info.os.enabled=true\n\nmanagement.endpoint.info.enabled=true\n\nmanagement.endpoint.health.probes.enabled=true\nmanagement.endpoint.health.show-details=always\n\nmanagement.endpoints.web.exposure.include=health,info\n</code></pre> <ol> <li>In <code>banner.txt</code> define</li> </ol> <pre><code>Spring Boot: ${spring-boot.version}\nApplication: ${spring.application.name} ${spring.application.version}\nGit: ${git.branch} ${git.commit.id.abbrev} ${git.tags} ${git.commit.time}\n</code></pre> <ol> <li>In <code>pom.xml</code> declare</li> </ol> <pre><code>...\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n&lt;/dependency&gt;\n...\n&lt;plugin&gt;\n    &lt;groupId&gt;io.github.git-commit-id&lt;/groupId&gt;\n    &lt;artifactId&gt;git-commit-id-maven-plugin&lt;/artifactId&gt;\n&lt;/plugin&gt;\n...\n</code></pre> <p>Note: In Spring Boot 2 use this for the git plugin.</p> <pre><code>...\n&lt;plugin&gt;\n    &lt;groupId&gt;pl.project13.maven&lt;/groupId&gt;\n    &lt;artifactId&gt;git-commit-id-plugin&lt;/artifactId&gt;\n&lt;/plugin&gt;\n...\n</code></pre>"},{"location":"programming/java/spring-boot/#integrate-sentry","title":"Integrate Sentry","text":"<ol> <li>In <code>pom.xml</code> declare</li> </ol> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.sentry&lt;/groupId&gt;\n    &lt;artifactId&gt;sentry-spring-boot-starter-jakarta&lt;/artifactId&gt;\n    &lt;version&gt;7.3.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Note: In Spring Boot 2 use this for the git plugin.</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.sentry&lt;/groupId&gt;\n    &lt;artifactId&gt;sentry-spring-boot-starter&lt;/artifactId&gt;\n    &lt;version&gt;7.3.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <ol> <li> <p>Configure the <code>SENTRY_AUTH_TOKEN</code> and <code>SENTRY_DSN</code> env vars.</p> </li> <li> <p>Optionally user context data (In web filters, event pre processors, etc).</p> </li> </ol> <pre><code>import io.sentry.Sentry;\nimport io.sentry.protocol.User;\n\n...\n\nUser user = new User();\nuser.setEmail(email);\nuser.setName(name);\nuser.setUsername(username);\nuser.setData(Map.of(\"prop\", prop));\nSentry.setUser(user);\n</code></pre> <ol> <li>Configure the <code>@ControllerAdvice</code> to send unexpected web originated exceptions.</li> </ol> <pre><code>@ExceptionHandler(Exception.class)\npublic ResponseEntity&lt;...&gt; unexpectedException(Exception e) {\n    Sentry.captureException(e);\n    return ResponseEntity.internalServerError().body(...);\n}\n</code></pre> <ol> <li>Capture the exceptions originated from scheduled tasks.</li> </ol> <pre><code>@Component\npublic class SentryThreadPoolTaskSchedulerCustomizer implements ThreadPoolTaskSchedulerCustomizer {\n    @Override\n    public void customize(ThreadPoolTaskScheduler taskScheduler) {\n        taskScheduler.setErrorHandler(Sentry::captureException);\n    }\n}\n</code></pre> <p>Note:: In Spring Boot 2 use this instead.</p> <pre><code>@Component\npublic class SentryTaskSchedulerCustomizer implements TaskSchedulerCustomizer {\n    @Override\n    public void customize(ThreadPoolTaskScheduler taskScheduler) {\n        taskScheduler.setErrorHandler(Sentry::captureException);\n    }\n}\n</code></pre> <ol> <li>Configure the async executor to handle sentry data between threads and the exception handler to manage exceptions from <code>@Async</code>.</li> </ol> <pre><code>public class AsyncHandler implements AsyncUncaughtExceptionHandler {\n\n    @Override\n    public void handleUncaughtException(Throwable ex, Method method, Object... params) {\n        Sentry.captureException(ex);\n    }\n}\n</code></pre> <pre><code>@Configuration\npublic class SentryAsyncConfigurer implements AsyncConfigurer {\n\n    @Override\n    public Executor getAsyncExecutor() {\n        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n        executor.setTaskDecorator(new SentryTaskDecorator());\n        executor.initialize();\n        return executor;\n    }\n\n    @Override\n    public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() {\n        return new AsyncHandler();\n    }\n}\n</code></pre> <p>Note:: In Spring Boot 2 use this instead.</p> <pre><code>@Configuration\npublic class SentryAsyncConfigurerSupport extends AsyncConfigurerSupport {\n\n    @Override\n    public Executor getAsyncExecutor() {\n        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n        executor.setTaskDecorator(new SentryTaskDecorator());\n        executor.initialize();\n        return executor;\n    }\n\n    @Override\n    public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() {\n        return new AsyncHandler();\n    }\n}\n</code></pre>"},{"location":"programming/java/spring-boot/#controller-annotation-interceptor","title":"Controller annotation interceptor","text":""},{"location":"programming/java/spring-boot/#learn","title":"Learn","text":"<p>https://tuhrig.de/self-made-event-dispatcher/</p> <p>https://github.com/ityouknow/spring-boot-examples</p> <p>https://github.com/xkcoding/spring-boot-demo</p>"},{"location":"programming/java/spring-boot/#learn-security","title":"Learn security","text":"<p>https://kevcodez.de/posts/2020-03-26-secure-spring-boot-app-with-oauth2-aws-cognito/</p> <p>https://www.baeldung.com/spring-security-method-security</p> <p>https://www.baeldung.com/spring-security-create-new-custom-security-expression</p> <p>https://www.baeldung.com/spring-security-role-filter-json</p> <p>https://dreamix.eu/blog/java/implementing-custom-authorization-function-for-springs-pre-and-post-annotations</p> <p>http://code-addict.pl/permission-evaluator-boot2/</p> <p>https://insource.io/blog/articles/custom-authorization-with-spring-boot.html</p> <p>https://dzone.com/articles/simple-attribute-based-access-control-with-spring</p>"},{"location":"programming/java/spring-boot/#learn-distributed-transactions","title":"Learn distributed transactions","text":"<p>https://piotrminkowski.com/2020/06/19/distributed-transactions-in-microservices-with-spring-boot/</p>"},{"location":"programming/java/spring-boot/#learn-health-check","title":"Learn health check","text":"<p>https://github.com/spring-projects/spring-boot/blob/main/spring-boot-project/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/system/DiskSpaceHealthIndicator.java</p>"},{"location":"programming/java/spring-boot/#learn-saga","title":"Learn SAGA","text":"<p>https://progressivecoder.com/wp-content/cache/all/saga-pattern-implementation-with-axon-and-spring-boot-part-1/index.html</p> <p>https://tuhrig.de/saga-pattern-with-spring-boot-and-activemq/</p>"},{"location":"programming/java/spring-boot/#learn-websockets","title":"Learn websockets","text":"<p>https://dzone.com/articles/build-a-secure-app-using-spring-boot-and-websocket</p>"},{"location":"programming/java/spring-boot/#learn-admin-server","title":"Learn Admin Server","text":"<p>https://codecentric.github.io/spring-boot-admin/</p>"},{"location":"programming/javascript/nestjs/","title":"NestJS","text":""},{"location":"programming/javascript/nestjs/#commands","title":"Commands","text":"<p>Init</p> <pre><code>npm i -g @nestjs/cli\nnest new project-name\n</code></pre> <p>Generate</p> <pre><code>nest g controller users\nnest g service users\nnest g module users\nnest g resource users\n</code></pre> <p>Validations</p> <pre><code>npm i --save class-validator class-transformer\n</code></pre> <p>Config</p> <pre><code>npm install @nestjs/config\nnpm install --save joi\n</code></pre> <p>Database</p> <pre><code>npm install --save @nestjs/typeorm typeorm pg\n</code></pre> <p>Openapi</p> <pre><code>npm install --save @nestjs/swagger swagger-ui-express\n</code></pre> <p><code>nest-cli.json.js</code></p> <p><code>{   \"collection\": \"@nestjs/schematics\",   \"sourceRoot\": \"src\",   \"compilerOptions\": {     \"plugins\": [\"@nestjs/swagger\"]   } }</code></p> <p>Migrations</p> <p>TODO: Validate</p> <pre><code>    migrations: [\"dist/migrations/*{.ts,.js}\"],\n    migrationsTableName: \"migrations_typeorm\",\n    migrationsRun: true\n</code></pre> <pre><code>npx typeorm migration:generate -n RenameColInArticleTable -d src/migrations\n</code></pre>"},{"location":"programming/javascript/nestjs/#main-components","title":"Main components","text":"<ul> <li>Controllers</li> <li>Providers</li> <li>Modules</li> <li>Middleware</li> <li>Exception filters</li> <li>Pipes</li> <li>Guards</li> <li>Incerceptors</li> <li>Custom decorators</li> </ul>"},{"location":"programming/javascript/nestjs/#testing","title":"Testing","text":"<pre><code>npm i --save-dev @nestjs/testing\n</code></pre> <p>TODO: Add more info</p>"},{"location":"programming/javascript/nestjs/#techniques","title":"Techniques","text":"<ul> <li>Persistence: Offers abstract layer and separated schema definition with TypeORM. Allows migration definition.</li> <li>Mapped Types: Allows to reuse DTOs for different operations.</li> <li>Built-in security with passport libs. CASL support is perfect for complex permission systems.</li> <li>Helmet helps secure common http mistakes.</li> <li>Has a plugin to generate OpenAPI documentation.</li> <li>Support GraphQL federation.</li> </ul>"},{"location":"programming/javascript/nestjs/#documentation","title":"Documentation","text":"<p>Official documentation</p> <p>Complete guide</p> <p>Examples</p>"},{"location":"programming/kotlin/kotlin/","title":"Kotlin","text":""},{"location":"programming/kotlin/kotlin/#libraries","title":"Libraries","text":"<p>Arrow Core: Functional style</p>"},{"location":"programming/kotlin/quarkus/","title":"Quarkus","text":""},{"location":"programming/kotlin/quarkus/#main-classes-and-annotations","title":"Main classes and annotations","text":""},{"location":"programming/kotlin/quarkus/#web","title":"Web","text":"<ul> <li><code>@Path</code> </li> <li><code>@GET</code> </li> <li><code>@Produces</code> </li> <li><code>@PathParam</code></li> </ul>"},{"location":"programming/kotlin/quarkus/#reactive","title":"Reactive","text":"<ul> <li><code>@RestPath</code></li> </ul>"},{"location":"programming/kotlin/quarkus/#di","title":"DI","text":"<ul> <li><code>@Inject</code> (field with package access) </li> <li><code>@ApplicationScoped</code> </li> <li><code>@Superior</code> </li> <li><code>@Dependent</code> </li> <li><code>@Produces</code> </li> <li><code>@PostConstruct</code> </li> <li><code>@PreDestroy</code> </li> <li><code>@IfBuildProfile(\"prod\")</code> </li> <li><code>@UnlessBuildProfile(\"test\")</code> </li> <li><code>@IfBuildProperty(name = \"some.tracer.enabled\", stringValue = \"true\")</code> </li> <li><code>@UnlessBuildProperty(name = \"some.tracer.enabled\", stringValue = \"false\")</code> </li> </ul>"},{"location":"programming/kotlin/quarkus/#configuration","title":"Configuration","text":"<pre><code>@ConfigProperty(name = \"greeting.suffix\", defaultValue=\"!\")\n</code></pre> <pre><code>String databaseName = ConfigProvider.getConfig().getValue(\"database.name\", String.class);\nOptional&lt;String&gt; maybeDatabaseName = ConfigProvider.getConfig().getOptionalValue(\"database.name\", String.class);\n</code></pre> <pre><code>@StaticInitSafe\n@ConfigMapping(prefix = \"server\")\ninterface Server {\n    String host();\n    int port();\n}\n</code></pre>"},{"location":"programming/kotlin/quarkus/#testing","title":"Testing","text":"<ul> <li><code>@QuarkusTest</code> </li> <li><code>@NativeImageTest</code></li> </ul>"},{"location":"programming/kotlin/quarkus/#reactive_1","title":"Reactive","text":"<ul> <li><code>Uni</code> </li> <li><code>Panache</code> </li> <li><code>PanacheEntity</code></li> </ul>"},{"location":"programming/kotlin/quarkus/#useful","title":"Useful","text":"<p><code>quarkus:dev</code> runs Quarkus in development mode. This enables hot deployment.</p> <p>Dev UI in http://localhost:8080/q/dev/.</p> <p>Use https://code.quarkus.io/ to generate projects.</p> <p>Can define env vars in <code>.env</code> file in the root folder.</p>"},{"location":"programming/kotlin/quarkus/#comands","title":"Comands","text":""},{"location":"programming/kotlin/quarkus/#maven","title":"Maven","text":"<p>Build native executable</p> <pre><code>./mvnw package -Pnative -Dquarkus.native.container-build=true\n./mvnw package -Pnative --define quarkus.native.container-build=true\n</code></pre> <p>Build native executable and a container</p> <pre><code>./mvnw quarkus:add-extension -Dextensions=\"container-image-docker\"\n./mvnw package -Pnative -Dquarkus.native.container-build=true -Dquarkus.container-image.build=true\n./mvnw package -Pnative --define quarkus.native.container-build=true --define quarkus.container-image.build=true\n\n./mvnw package -Pnative -Dquarkus.native.container-build=true -Dquarkus.container-image.build=true -Dquarkus.container-image.image=registry.com/name\n./mvnw package -Pnative --define quarkus.native.container-build=true --define quarkus.container-image.build=true --define quarkus.container-image.image=registry.com/name\n</code></pre> <p>List and add extensions</p> <pre><code>./mvnw quarkus:list-extensions\n./mvnw quarkus:add-extensions -Dextensions=ex1,ex2\n</code></pre>"},{"location":"programming/kotlin/quarkus/#gradle","title":"Gradle","text":"<p>TODO: Build native executable and container</p> <p>List and add extensions</p> <pre><code>./gradlew listExtensions\n./gradlew addExtension --extensions=\"ex1,ex2\"\n</code></pre>"},{"location":"programming/kotlin/quarkus/#receipts","title":"Receipts","text":""},{"location":"programming/kotlin/quarkus/#reactive-rest-with-postgresql","title":"Reactive REST with PostgreSQL","text":"<ul> <li> <p>RESTEasy Reactive Jackson</p> </li> <li> <p>Hibernate Reactive with Panache</p> </li> <li> <p>Reactive PostgreSQL client</p> </li> </ul>"},{"location":"programming/kotlin/quarkus/#learn","title":"Learn","text":"<p>https://pattern-match.com/blog/quarkus-with-kotlin/</p>"},{"location":"programming/kotlin/quarkus/#todo","title":"TODO","text":"<p>https://quarkus.io/guides/getting-started-testing</p>"},{"location":"programming/kotlin/quarkus/#reorder","title":"Reorder","text":"<p>https://www.composerize.com/</p>"},{"location":"programming/kotlin/quarkus/#documentation","title":"Documentation","text":"<p>Official documentation</p>"},{"location":"programming/python/python/","title":"Python","text":""},{"location":"programming/python/python/#links","title":"Links","text":"<p>Tutorial: Face detection</p> <p>Tutorial: Home surveillance</p> <p>Facial expression recognition</p> <p>Face detection</p> <p>Podcast</p>"},{"location":"programming/python/python/#poetry","title":"Poetry","text":"<p>Basic commands</p> <pre><code>poetry new &lt;project_name&gt;\npoetry install\npoetry add &lt;pk1&gt; &lt;pk2&gt;\npoetry remove &lt;pk1&gt;\npoetry update\npoetry shell\n</code></pre> <p>After the 'shell', you can open an editor and place new files next to the 'init' folder.</p> <p>Manager development dependencies</p> <pre><code>poetry add -D &lt;dpk1&gt; &lt;dpk2&gt;\npoetry remove -D &lt;dpk1&gt;\n</code></pre>"},{"location":"programming/python/python/#pipenv","title":"Pipenv","text":""},{"location":"programming/python/python/#commands","title":"Commands","text":"<p>Install</p> <pre><code> sudo apt install pipenv\n</code></pre> <p>Configure to create env in project folder</p> <pre><code>export PIPENV_VENV_IN_PROJECT=\"enabled\"\n</code></pre> <p>Update <code>pip</code> inside <code>pipenv</code></p> <pre><code>pipenv run pip install --upgrade pip\n</code></pre> <p>Remove env</p> <pre><code>pipenv --rm\n</code></pre> <p>Install package</p> <pre><code>pipenv install &lt;PK&gt;\n</code></pre> <p>Remove package</p> <pre><code>pipenv uninstall &lt;PK&gt; \n</code></pre> <p>Sync packages from Pipfile</p> <pre><code>pipenv sync\n</code></pre> <p>Run code</p> <pre><code>pipenv run python &lt;file.py&gt;\n</code></pre>"},{"location":"programming/python/python/#learn","title":"Learn","text":"<p>https://medium.com/@yvanscher/playing-with-perlin-noise-generating-realistic-archipelagos-b59f004d8401</p> <p>https://medium.com/@yvanscher/cellular-automata-how-to-create-realistic-worlds-for-your-game-2a9ec35f5ba9</p> <p>https://github.com/priya-dwivedi/face_and_emotion_detection</p> <p>https://github.com/thevarunsharma/Animoji-Animate</p> <p>https://github.com/jtoy/awesome-tensorflow</p> <p>https://github.com/ageitgey/face_recognition</p> <p>https://www.techwithtim.net/tutorials/</p> <p>https://towardsdatascience.com/visualizing-beyond-3-dimensions-67531b431119</p> <p>https://github.com/spmallick/learnopencv</p> <p>https://www.youtube.com/watch?v=WFr2WgN9_xE</p> <p>https://www.youtube.com/watch?v=D5xqcGk6LEc</p> <p>https://www.youtube.com/watch?v=WQeoO7MI0Bs</p>"},{"location":"programming/rust/rust/","title":"Rust","text":""},{"location":"programming/rust/rust/#links","title":"Links","text":"<p>Comprehensive Rust Practice with Advent of Code Patterns Desktop app demo API demo Protobuf demo</p>"},{"location":"programming/rust/rust/#commands","title":"Commands","text":"<p>Install toolchain</p> <pre><code>rustup install stable-x86_64-pc-windows-gnu\nrustup install stable-x86_64-pc-windows-msvc\n\nrustup default stable-x86_64-pc-windows-msvc\n</code></pre> <p>Update toolchain</p> <pre><code>rustup update\n</code></pre> <p>List toolchains</p> <pre><code>rustup toolchain list\n</code></pre> <p>Install cargo watch</p> <pre><code>cargo install cargo-watch\n</code></pre> <p>Install cargo edit</p> <pre><code>cargo install cargo-edit\ncargo install cargo-edit --features vendored-openssl\n</code></pre> <p>Cargo edit commands</p> <pre><code>cargo add x\ncargo add x --features f1 f2\ncargo rm x\ncargo upgrade\ncargo upgrade x\n\ncargo set-version 1.0.0\ncargo set-version --bump major\ncargo set-version --bump minor\ncargo set-version --bump patch\n</code></pre> <p>Create new project</p> <pre><code>cargo new hello_world --bin\ncargo new hello_world --lib\n</code></pre> <p>Watch project</p> <pre><code>cargo watch -x 'run'\ncargo watch -x 'run' --why\n</code></pre>"},{"location":"programming/rust/rust/#libraries-to-explore","title":"Libraries to explore","text":"<p>Tokio, Hyper/Reqwest, Ring, and Neon</p> <p>warp</p> <p>tokio-tungstenite</p> <p>https://crates.io/crates/anyhow</p> <p>https://crates.io/crates/wasm-bindgen</p> <p>https://crates.io/crates/femme</p>"},{"location":"programming/rust/rust/#learn","title":"Learn","text":"<p>https://github.com/pingcap/talent-plan/tree/master/courses/rust</p> <p>https://github.com/rust-lang/rustlings</p> <p>https://docs.microsoft.com/en-us/learn/paths/rust-first-steps/?WT.mc_id=javascript-00000-wachegha</p> <p>https://betterprogramming.pub/structuring-rust-project-for-testability-18207b5d0243</p> <p><code>uninplemented!()</code></p> <p>https://www.youtube.com/c/RustVideos/videos</p> <p>https://www.youtube.com/watch?v=gesNaLkUJeA</p>"},{"location":"programming/rust/rust/#documentation","title":"Documentation","text":"<p>The Rust Programming Language</p> <p>The Rustonomicon</p>"},{"location":"programming/rust/rust/#ides","title":"IDEs","text":""},{"location":"programming/rust/rust/#visual-studio-code","title":"Visual Studio Code","text":""},{"location":"security/kali/","title":"Kali","text":"<p><code>sudo apt-get install aircrack-ng</code></p> <p><code>sudo airmon-ng</code></p>"},{"location":"security/security/","title":"Security","text":""},{"location":"security/security/#links","title":"Links","text":"<p>Tutorial: Web Lab Web hacking techniques HackTheBox Notes Async Vulnerabilities Labs</p>"},{"location":"security/security/#dorks","title":"Dorks","text":"<p>site:http://s3.amazonaws.com \"target[.]com\" site:http://blob.core.windows.net \"target[.]com\" site:http://googleapis.com \"target[.]com\" site:http://drive.google.com \"target[.]com\" site:http://blob.core.windows.net \"target[.]com\"</p> <p>site:http://s3.amazonaws.com | site:http://blob.core.windows.net | site:http://googleapis.com | site:http://drive.google.com | site:http://blob.core.windows.net \"target[.]com\"</p>"},{"location":"security/security/#learn","title":"Learn","text":"<p>https://portswigger.net/web-security/all-materials/detailed</p> <p>https://dzone.com/articles/api-security-weekly-issue-146</p> <p>https://github.com/jhaddix/tbhm</p> <p>https://dsopas.github.io/MindAPI/play/</p> <p>https://www.redeszone.net/tutoriales/seguridad/paginas-aprender-hacking-etico-internet/</p> <p>https://blog.thinkst.com/p/canarytokensorg-quick-free-detection.html</p> <p>https://github.com/blaCCkHatHacEEkr/PENTESTING-BIBLE</p> <p>https://github.com/vulhub/vulhub</p> <p>https://www.youtube.com/c/InsiderPhD/videos</p>"},{"location":"security/security/#learn-wifi","title":"Learn WIFI","text":"<p>https://infosecwriteups.com/how-i-hacked-into-my-neighbours-wifi-and-harvested-credentials-487fab106bfc</p>"},{"location":"services/services/","title":"Services","text":""},{"location":"services/services/#email","title":"Email","text":"<p>Email Sending</p>"},{"location":"services/services/#frontend","title":"Frontend","text":"<ul> <li>Vercel</li> <li>Render</li> <li>Github pages</li> <li>Firebase Hosting</li> <li>Netlify</li> </ul>"},{"location":"services/services/#backend","title":"Backend","text":"<ul> <li>AWS Lambda</li> <li>Google Cloud Functions</li> <li>Vercel</li> <li>Fly.io</li> <li>http://fl0.com</li> </ul>"},{"location":"services/services/#database","title":"Database","text":"<ul> <li>Vercel Postgres</li> <li>CockroachDB</li> <li>Firebase</li> <li>DynamoDB</li> <li>Upstash Redis</li> </ul>"},{"location":"specs/asyncapi/","title":"AsyncAPI","text":""},{"location":"specs/asyncapi/#spec","title":"Spec","text":"<p>https://www.asyncapi.com/</p>"},{"location":"specs/openapi/","title":"OpenAPI","text":""},{"location":"specs/openapi/#spec","title":"Spec","text":"<p>https://www.openapis.org/</p>"},{"location":"specs/openapi/#reusable-definition","title":"Reusable definition","text":"<pre><code>components:\n  schemas:\n    id:\n      type: string\n    resource_id:\n      type: object\n      properties:\n        id:\n          $ref: '#/components/schemas/id'\n    resource_properties:\n      type: object\n      properties:\n        required_prop:\n          type: string\n        optional_prop:\n          type: string\n    resource_required_properties:\n      type: object\n      required:\n        - required_prop\n    resource:\n      allOf:\n        - $ref: '#/components/schemas/resource_id'\n        - $ref: '#/components/schemas/resource_properties'\n        - $ref: '#/components/schemas/resource_required_properties'\n\n  requestBodies:\n    create_update_resource:\n      description: Create a new resource\n      content:\n        application/json:\n          schema:\n            allOf:\n              - $ref: '#/components/schemas/resource_properties'\n              - $ref: '#/components/schemas/resource_required_properties'\n    partial_update_resource:\n      content:\n        application/json:\n          schema:\n            allOf:\n              - $ref: '#/components/schemas/resource_properties'\n              - type: object\n                properties:\n                  optional_prop:\n                    nullable: true\n</code></pre> <p>Note: <code>partial_update_resource</code> definition follows JSON Merge Patch rules.</p>"}]}